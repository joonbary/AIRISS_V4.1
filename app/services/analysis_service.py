# app/services/analysis_service.py
"""
AIRISS v4.0 ë¶„ì„ ì„œë¹„ìŠ¤
- ë¶„ì„ ì‘ì—… ê´€ë¦¬ ë° ì²˜ë¦¬
- WebSocketê³¼ ì—°ë™
"""

import logging
from typing import Dict, Any, Optional
import asyncio
from datetime import datetime
import pandas as pd
import io
import json

logger = logging.getLogger(__name__)

class AnalysisService:
    """ë¶„ì„ ì„œë¹„ìŠ¤ - ë¶„ì„ ì‘ì—…ì˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    
    def __init__(self, websocket_manager=None):
        self.websocket_manager = websocket_manager
        self.active_jobs = {}
        self.uploaded_files = {}  # ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì €ì¥
        logger.info("âœ… AnalysisService ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def upload_file(self, file_contents: bytes, filename: str) -> Dict[str, Any]:
        """íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ë° ì»¬ëŸ¼ ë¶„ì„"""
        try:
            import uuid
            from pathlib import Path
            import pandas as pd
            import io
            
            # Generate file ID
            file_id = str(uuid.uuid4())
            
            # Save file
            upload_dir = Path("uploads")
            upload_dir.mkdir(exist_ok=True)
            
            file_path = upload_dir / f"{file_id}_{filename}"
            
            # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            import os
            logger.info(f"ğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
            logger.info(f"ğŸ“ uploads ë””ë ‰í† ë¦¬ ê²½ë¡œ: {upload_dir.absolute()}")
            logger.info(f"ğŸ“ íŒŒì¼ ì €ì¥ ê²½ë¡œ: {file_path.absolute()}")
            
            with open(file_path, "wb") as f:
                f.write(file_contents)
            
            logger.info(f"ğŸ“ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            logger.info(f"ğŸ“ íŒŒì¼ í¬ê¸°: {os.path.getsize(file_path)} bytes")
            logger.info(f"ğŸ“ íŒŒì¼ ì¡´ì¬ í™•ì¸: {os.path.exists(file_path)}")
            
            # Excel íŒŒì¼ ë¶„ì„
            df = None
            try:
                if filename.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(io.BytesIO(file_contents))
                    logger.info(f"ğŸ“Š Excel íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(df)} rows, {len(df.columns)} columns")
                elif filename.endswith('.csv'):
                    # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
                    encodings = ['utf-8', 'cp949', 'euc-kr', 'iso-8859-1']
                    for encoding in encodings:
                        try:
                            df = pd.read_csv(io.StringIO(file_contents.decode(encoding)))
                            logger.info(f"ğŸ“Š CSV íŒŒì¼ ì½ê¸° ì™„ë£Œ (ì¸ì½”ë”©: {encoding}): {len(df)} rows, {len(df.columns)} columns")
                            break
                        except:
                            continue
                    
                    if df is None:
                        raise ValueError("CSV íŒŒì¼ ì¸ì½”ë”©ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                else:
                    raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë˜ëŠ” Excel íŒŒì¼ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                
                logger.info(f"ğŸ“‹ íŒŒì¼ ì»¬ëŸ¼ëª…: {list(df.columns)}")
                logger.info(f"ğŸ“„ ì²« 3í–‰ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n{df.head(3)}")
                
            except Exception as e:
                logger.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                raise ValueError(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            
            # ë°ì´í„° í”„ë ˆì„ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if df.empty:
                logger.error(f"âŒ íŒŒì¼ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                raise ValueError("íŒŒì¼ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì‹­ì‹œì˜¤.")
            
            # ì»¬ëŸ¼ ë¶„ì„
            all_columns = list(df.columns)
            
            # UID ì»¬ëŸ¼ ê°ì§€
            uid_keywords = ['uid', 'id', 'ì•„ì´ë””', 'ì‚¬ë²ˆ', 'ì§ì›', 'user', 'emp', 'employee']
            uid_columns = [col for col in all_columns if any(keyword in col.lower() for keyword in uid_keywords)]
            
            # ì˜ê²¬ ì»¬ëŸ¼ ê°ì§€
            opinion_keywords = ['ì˜ê²¬', 'opinion', 'í‰ê°€', 'feedback', 'ë‚´ìš©', 'ì½”ë©˜íŠ¸', 'í”¼ë“œë°±', 'comment', 'review', 'ìë£Œ', 'í…ìŠ¤íŠ¸', 'text']
            opinion_columns = [col for col in all_columns if any(keyword in col.lower() for keyword in opinion_keywords)]
            
            # ì •ëŸ‰ë°ì´í„° ì»¬ëŸ¼ ê°ì§€
            quant_keywords = ['ì ìˆ˜', 'score', 'í‰ì ', 'rating', 'ë“±ê¸‰', 'grade', 'level', 'ë‹¬ì„±ë¥ ', 'ë¹„ìœ¨', 'rate', '%', 'percent']
            quantitative_columns = []
            
            for col in all_columns:
                col_lower = col.lower()
                if any(keyword in col_lower for keyword in quant_keywords):
                    # ì‹¤ì œ ë°ì´í„° í™•ì¸
                    sample_data = df[col].dropna().head(10)
                    if len(sample_data) > 0:
                        quantitative_columns.append(col)
            
            logger.info(f"ğŸ¯ ì»¬ëŸ¼ ë¶„ì„ ê²°ê³¼:")
            logger.info(f"   - ì „ì²´ ì»¬ëŸ¼: {len(all_columns)}ê°œ")
            logger.info(f"   - UID ì»¬ëŸ¼: {uid_columns}")
            logger.info(f"   - ì˜ê²¬ ì»¬ëŸ¼: {opinion_columns}")
            logger.info(f"   - ì •ëŸ‰ ì»¬ëŸ¼: {quantitative_columns}")
            
            # Store file info with analysis results
            file_info = {
                "file_id": file_id,
                "filename": filename,
                "path": str(file_path.absolute()),  # ì ˆëŒ€ ê²½ë¡œ ì €ì¥
                "size": len(file_contents),
                "total_records": len(df),
                "columns": all_columns,
                "uid_columns": uid_columns,
                "opinion_columns": opinion_columns,
                "quantitative_columns": quantitative_columns,
                "uploaded_at": datetime.now().isoformat()
            }
            
            self.uploaded_files[file_id] = file_info
            
            logger.info(f"âœ… íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„ ì™„ë£Œ: {filename} -> {file_id}")
            
            return {
                "file_id": file_id,
                "filename": filename,
                "total_records": len(df),
                "column_count": len(all_columns),
                "columns": all_columns,
                "uid_columns": uid_columns,
                "opinion_columns": opinion_columns,
                "quantitative_columns": quantitative_columns,
                "airiss_ready": len(uid_columns) > 0 and len(opinion_columns) > 0,
                "hybrid_ready": len(quantitative_columns) > 0,
                "data_quality": {
                    "non_empty_records": len(df.dropna()),
                    "completeness": round((len(df.dropna()) / len(df)) * 100, 1) if len(df) > 0 else 0,
                    "quantitative_completeness": round((len(quantitative_columns) / len(all_columns)) * 100, 1) if len(all_columns) > 0 else 0
                },
                "message": "File uploaded successfully"
            }
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
            raise
    
    async def start_analysis(self, 
                           file_id: str,
                           sample_size: int = 10,
                           analysis_mode: str = "hybrid",
                           enable_ai_feedback: bool = False,
                           openai_api_key: Optional[str] = None,
                           openai_model: str = "gpt-3.5-turbo",
                           max_tokens: int = 1200) -> str:
        """ë¶„ì„ ì‘ì—… ì‹œì‘"""
        try:
            logger.info(f"ğŸ¯ ë¶„ì„ ì‹œì‘ ìš”ì²­ - enable_ai_feedback: {enable_ai_feedback}")
            logger.info(f"ğŸ¯ OpenAI API í‚¤ ì „ë‹¬ ì—¬ë¶€: {'ìˆìŒ' if openai_api_key else 'ì—†ìŒ'}")
            
            # Generate job ID
            import uuid
            job_id = str(uuid.uuid4())
            
            # Store job data
            job_data = {
                'file_id': file_id,
                'sample_size': sample_size,
                'analysis_mode': analysis_mode,
                'enable_ai_feedback': enable_ai_feedback,
                'openai_api_key': openai_api_key,
                'openai_model': openai_model,
                'max_tokens': max_tokens
            }
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— Job ë ˆì½”ë“œ ìƒì„±
            await self._create_job_record(job_id, job_data)
            
            self.active_jobs[job_id] = {
                'status': 'processing',
                'start_time': datetime.now(),
                'data': job_data,
                'progress': 0
            }
            
            if self.websocket_manager:
                await self.websocket_manager.send_alert(
                    "info",
                    f"ë¶„ì„ ì‹œì‘: {job_id}",
                    {"job_id": job_id, "file_id": file_id}
                )
            
            logger.info(f"âœ… ë¶„ì„ ì‘ì—… ì‹œì‘: job_id={job_id}, file_id={file_id}")
            
            # ì‹¤ì œ ë¶„ì„ ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹œì‘
            asyncio.create_task(self._process_analysis(job_id, job_data))
            
            return job_id
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‹œì‘ ì˜¤ë¥˜: {e}")
            raise
    
    async def _create_job_record(self, job_id: str, job_data: Dict[str, Any]):
        """ë°ì´í„°ë² ì´ìŠ¤ì— Job ë ˆì½”ë“œ ìƒì„±"""
        try:
            from app.db.database import get_db
            from app.models.job import Job
            import json
            
            def create_job():
                db = next(get_db())
                try:
                    # íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    file_id = job_data['file_id']
                    file_info = self.uploaded_files.get(file_id, {})
                    
                    job = Job(
                        id=job_id,
                        file_id=file_id,
                        filename=file_info.get('filename'),
                        status='processing',
                        sample_size=job_data['sample_size'],
                        analysis_mode=job_data['analysis_mode'],
                        enable_ai_feedback=job_data['enable_ai_feedback'],
                        openai_model=job_data.get('openai_model'),
                        max_tokens=job_data.get('max_tokens'),
                        total_records=file_info.get('total_records', 0),
                        job_data=json.dumps(job_data)
                    )
                    db.add(job)
                    db.commit()
                    logger.info(f"âœ… Job ë ˆì½”ë“œ ìƒì„± ì™„ë£Œ: {job_id}")
                    
                except Exception as e:
                    db.rollback()
                    logger.error(f"âŒ Job ë ˆì½”ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
                    raise
                finally:
                    db.close()
            
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            import asyncio
            await asyncio.to_thread(create_job)
            
        except Exception as e:
            logger.error(f"âŒ Job ë ˆì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    async def update_progress(self, job_id: str, progress: float, details: Dict = None):
        """ë¶„ì„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            # ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
            if job_id in self.active_jobs:
                self.active_jobs[job_id]['progress'] = progress
                self.active_jobs[job_id]['last_update'] = datetime.now()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            from app.db.database import get_db
            from app.models.job import Job
            
            def update_job_progress():
                db = next(get_db())
                try:
                    job = db.query(Job).filter(Job.id == job_id).first()
                    if job:
                        job.progress = progress
                        job.updated_at = datetime.now()
                        db.commit()
                        logger.info(f"ğŸ“Š ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {job_id} - {progress}%")
                finally:
                    db.close()
            
            await asyncio.to_thread(update_job_progress)
            
            # WebSocket ì•Œë¦¼
            if self.websocket_manager:
                await self.websocket_manager.send_analysis_progress(job_id, {
                    "progress": progress,
                    "details": details or {}
                })
            
        except Exception as e:
            logger.error(f"âŒ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    async def complete_analysis(self, job_id: str, results: Dict[str, Any]):
        """ë¶„ì„ ì™„ë£Œ ì²˜ë¦¬"""
        try:
            if job_id in self.active_jobs:
                self.active_jobs[job_id]['status'] = 'completed'
                self.active_jobs[job_id]['end_time'] = datetime.now()
                self.active_jobs[job_id]['results'] = results
                
                # ë°ì´í„°ë² ì´ìŠ¤ Job ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
                await self._update_job_completion(job_id, results)
                
                if self.websocket_manager:
                    await self.websocket_manager.send_alert(
                        "success",
                        f"ë¶„ì„ ì™„ë£Œ: {job_id}",
                        {"job_id": job_id, "results_count": len(results.get('data', []))}
                    )
                
                logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {job_id}")
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì™„ë£Œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def _update_job_completion(self, job_id: str, results: Dict[str, Any]):
        """ë°ì´í„°ë² ì´ìŠ¤ Job ë ˆì½”ë“œ ì™„ë£Œ ì—…ë°ì´íŠ¸"""
        try:
            from app.db.database import get_db
            from app.models.job import Job
            import json
            
            def update_job():
                db = next(get_db())
                try:
                    job = db.query(Job).filter(Job.id == job_id).first()
                    if job:
                        job.status = 'completed'
                        job.end_time = datetime.now()
                        job.progress = 100.0
                        job.processed_records = len(results.get('analysis_results', results.get('data', [])))
                        job.average_score = results.get('summary', {}).get('average_score', 0.0)
                        job.results_data = json.dumps(results)
                        
                        db.commit()
                        logger.info(f"âœ… Job ì™„ë£Œ ì—…ë°ì´íŠ¸: {job_id}")
                    else:
                        logger.warning(f"âš ï¸ Job ë ˆì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {job_id}")
                        
                except Exception as e:
                    db.rollback()
                    logger.error(f"âŒ Job ì™„ë£Œ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
                    raise
                finally:
                    db.close()
            
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            import asyncio
            await asyncio.to_thread(update_job)
            
        except Exception as e:
            logger.error(f"âŒ Job ì™„ë£Œ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def fail_analysis(self, job_id: str, error: str):
        """ë¶„ì„ ì‹¤íŒ¨ ì²˜ë¦¬"""
        try:
            if job_id in self.active_jobs:
                self.active_jobs[job_id]['status'] = 'failed'
                self.active_jobs[job_id]['error'] = error
                self.active_jobs[job_id]['end_time'] = datetime.now()
                
                # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
                from app.db.database import get_db
                from app.models.job import Job
                
                def update_job_failure():
                    db = next(get_db())
                    try:
                        job = db.query(Job).filter(Job.id == job_id).first()
                        if job:
                            job.status = 'failed'
                            job.error_message = error[:500] if error else None  # ì—ëŸ¬ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
                            job.end_time = datetime.now()
                            db.commit()
                            logger.info(f"ğŸ”„ Job ì‹¤íŒ¨ ì—…ë°ì´íŠ¸: {job_id}")
                    except Exception as e:
                        db.rollback()
                        logger.error(f"âŒ Job ì‹¤íŒ¨ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
                    finally:
                        db.close()
                
                await asyncio.to_thread(update_job_failure)
                
                if self.websocket_manager:
                    await self.websocket_manager.send_alert(
                        "error",
                        f"ë¶„ì„ ì‹¤íŒ¨: {job_id}",
                        {"job_id": job_id, "error": error}
                    )
                
                logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {job_id} - {error}")
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"íŠ¸ë ˆì´ìŠ¤ë°±:\n{traceback.format_exc()}")
    
    async def _process_analysis(self, job_id: str, job_data: Dict[str, Any]):
        """ì‹¤ì œ ë¶„ì„ ì‘ì—… ì²˜ë¦¬ - HybridAnalyzer í†µí•©"""
        logger.info("="*60)
        logger.info(f"ğŸš€ _process_analysis ì‹œì‘")
        logger.info(f"ğŸ†” job_id: {job_id}")
        logger.info(f"ğŸ“ job_data: {job_data}")
        logger.info("="*60)
        
        try:
            file_id = job_data['file_id']
            logger.info(f"ë¶„ì„ ì²˜ë¦¬ ì‹œì‘: job_id={job_id}, file_id={file_id}")
            
            # Progress updates
            await self.update_progress(job_id, 10, {"status": "íŒŒì¼ ë¡œë“œ ì¤‘"})
            
            # 1. íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸° - ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™•ì¸
            file_info = None
            file_path = None
            filename = None
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íŒŒì¼ ì •ë³´ ì¡°íšŒ
            from app.db.database import get_db
            from app.models.file import File as FileModel
            import json
            
            db = next(get_db())
            try:
                file_record = db.query(FileModel).filter(FileModel.id == file_id).first()
                if file_record:
                    logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íŒŒì¼ ì •ë³´ ì°¾ìŒ: {file_id}")
                    file_path = file_record.file_path
                    filename = file_record.filename
                    
                    # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
                    self.uploaded_files[file_id] = {
                        'path': file_path,
                        'filename': filename,
                        'total_records': file_record.total_records,
                        'columns': json.loads(file_record.columns) if file_record.columns else []
                    }
                    file_info = self.uploaded_files[file_id]
                else:
                    # ë©”ëª¨ë¦¬ì—ì„œ í™•ì¸ (fallback)
                    if file_id in self.uploaded_files:
                        logger.info(f"ğŸ“¦ ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ íŒŒì¼ ì •ë³´ ì°¾ìŒ: {file_id}")
                        file_info = self.uploaded_files[file_id]
                        file_path = file_info['path']
                        filename = file_info['filename']
                    else:
                        logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
                        logger.error(f"ğŸ“ í˜„ì¬ ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡: {list(self.uploaded_files.keys())}")
                        raise ValueError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
            finally:
                db.close()
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            import os
            if not os.path.exists(file_path):
                logger.error(f"âŒ íŒŒì¼ì´ íŒŒì¼ì‹œìŠ¤í…œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                logger.error(f"ğŸ“ uploads ë””ë ‰í† ë¦¬ ë‚´ìš©: {os.listdir('uploads') if os.path.exists('uploads') else 'ë””ë ‰í† ë¦¬ ì—†ìŒ'}")
                raise ValueError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            
            # 2. íŒŒì¼ ì½ê¸° ë° ë°ì´í„° ë¡œë“œ
            import pandas as pd
            logger.info(f"íŒŒì¼ ì½ê¸° ì‹œì‘: {file_path}")
            
            try:
                # pickle íŒŒì¼ë¡œ ì €ì¥ëœ ê²½ìš°
                if file_path.endswith('.pkl'):
                    df = pd.read_pickle(file_path)
                    logger.info(f"ğŸ“Š Pickle íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)} rows, {len(df.columns)} columns")
                # Excel íŒŒì¼ì¸ ê²½ìš°
                elif file_path.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(file_path)
                    logger.info(f"ğŸ“Š Excel íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)} rows, {len(df.columns)} columns")
                # CSV íŒŒì¼ì¸ ê²½ìš°
                elif file_path.endswith('.csv'):
                    df = pd.read_csv(file_path)
                    logger.info(f"ğŸ“Š CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)} rows, {len(df.columns)} columns")
                else:
                    raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
                
                logger.info(f"ğŸ“‹ íŒŒì¼ ì»¬ëŸ¼ëª…: {list(df.columns)}")
                logger.info(f"ğŸ“„ ì²« 5í–‰ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\n{df.head()}")
            except Exception as e:
                logger.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                raise ValueError(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            
            # ë°ì´í„° í”„ë ˆì„ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if df.empty:
                logger.error(f"âŒ íŒŒì¼ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                raise ValueError("íŒŒì¼ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì‹­ì‹œì˜¤.")
            
            await self.update_progress(job_id, 20, {
                "status": "ë°ì´í„° ê²€ì¦ ì¤‘",
                "rows": len(df),
                "columns": len(df.columns)
            })
            
            # 3. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (ìœ ì—°í•œ ì»¬ëŸ¼ëª… ë§¤ì¹­)
            column_names = [col.lower().strip() for col in df.columns]
            logger.info(f"ğŸ” ì†Œë¬¸ì ë³€í™˜ëœ ì»¬ëŸ¼ëª…: {column_names}")
            
            # uid ì»¬ëŸ¼ ì°¾ê¸°
            uid_column = None
            for col in df.columns:
                if col.lower().strip() in ['uid', 'id', 'ì§ì›ë²ˆí˜¸', 'employee_id', 'ì‚¬ë²ˆ']:
                    uid_column = col
                    break
            
            # opinion ì»¬ëŸ¼ ì°¾ê¸°  
            opinion_column = None
            for col in df.columns:
                col_lower = col.lower().strip()
                if any(keyword in col_lower for keyword in ['opinion', 'ì˜ê²¬', 'í‰ê°€', 'comment', 'ì½”ë©˜íŠ¸', 'feedback', 'ë¦¬ë·°', 'review', 'ìë£Œ', 'ë‚´ìš©', 'content', 'í…ìŠ¤íŠ¸', 'text']):
                    opinion_column = col
                    break
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            if uid_column is None:
                logger.error(f"âŒ UID ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼ 'UID' ë˜ëŠ” 'ì§ì›ë²ˆí˜¸'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
            if opinion_column is None:
                logger.error(f"âŒ í‰ê°€ì˜ê²¬ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
                raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼ 'í‰ê°€ì˜ê²¬' ë˜ëŠ” 'Opinion'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì´ë¦„ ì»¬ëŸ¼ ì°¾ê¸°
            name_column = None
            for col in df.columns:
                col_lower = col.lower().strip()
                if any(keyword in col_lower for keyword in ['name', 'ì´ë¦„', 'ì„±ëª…', 'ì§ì›ëª…']):
                    name_column = col
                    break
            
            # ë¶€ì„œ ì»¬ëŸ¼ ì°¾ê¸°
            department_column = None
            for col in df.columns:
                col_lower = col.lower().strip()
                if any(keyword in col_lower for keyword in ['department', 'ë¶€ì„œ', 'dept', 'ì†Œì†', 'íŒ€']):
                    department_column = col
                    break
            
            # ì§ê¸‰ ì»¬ëŸ¼ ì°¾ê¸°
            position_column = None
            for col in df.columns:
                col_lower = col.lower().strip()
                if any(keyword in col_lower for keyword in ['position', 'ì§ê¸‰', 'ì§ìœ„', 'title', 'grade', 'ë“±ê¸‰']):
                    position_column = col
                    break
            
            logger.info(f"ğŸ“¦ ì»¬ëŸ¼ ë§¤í•‘ ê²°ê³¼:")
            logger.info(f"  - UID: {uid_column}")
            logger.info(f"  - Opinion: {opinion_column}")
            logger.info(f"  - Name: {name_column or 'ì—†ìŒ'}")
            logger.info(f"  - Department: {department_column or 'ì—†ìŒ'}")
            logger.info(f"  - Position: {position_column or 'ì—†ìŒ'}")
                
            if not opinion_column:
                logger.error(f"âŒ ì˜ê²¬ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
                raise ValueError("ì˜ê²¬ ì»¬ëŸ¼(opinion, ì˜ê²¬, í‰ê°€ ë“±)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"âœ… ì»¬ëŸ¼ ë§¤ì¹­ ì„±ê³µ: uid='{uid_column}', opinion='{opinion_column}'")
            if name_column:
                logger.info(f"âœ… ì´ë¦„ ì»¬ëŸ¼ ë°œê²¬: '{name_column}'")
            if department_column:
                logger.info(f"âœ… ë¶€ì„œ ì»¬ëŸ¼ ë°œê²¬: '{department_column}'")
            if position_column:
                logger.info(f"âœ… ì§ê¸‰ ì»¬ëŸ¼ ë°œê²¬: '{position_column}'")
            
            # 4. HybridAnalyzer ì´ˆê¸°í™”
            from app.services.hybrid_analyzer import AIRISSHybridAnalyzer
            analyzer = AIRISSHybridAnalyzer()
            
            # 5. ìƒ˜í”Œ í¬ê¸° ì œí•œ
            sample_size = min(job_data.get('sample_size', 10), len(df))
            df_sample = df.head(sample_size)
            logger.info(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: {sample_size}ëª… (ì „ì²´: {len(df)}ëª…)")
            logger.info(f"ğŸ“‹ ë¶„ì„í•  ì»¬ëŸ¼: uid={uid_column}, opinion={opinion_column}")
            
            await self.update_progress(job_id, 30, {
                "status": "AI ë¶„ì„ ì‹œì‘",
                "analyzing": sample_size
            })
            
            # 6. ê° í–‰ì— ëŒ€í•´ ë¶„ì„ ìˆ˜í–‰
            analysis_results = []
            logger.info(f"ğŸ”„ ë¶„ì„ ë£¨í”„ ì‹œì‘: {sample_size}ê°œ ë ˆì½”ë“œ")
            
            for idx, (index, row) in enumerate(df_sample.iterrows()):
                try:
                    logger.info(f"ğŸ“Š ë ˆì½”ë“œ {idx+1}/{sample_size} ë¶„ì„ ì‹œì‘")
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ - ë” ì„¸ë°€í•˜ê²Œ
                    progress = 30 + ((idx + 0.5) / sample_size) * 50  # 30-80% êµ¬ê°„
                    await self.update_progress(job_id, progress, {
                        "status": f"ë¶„ì„ ì¤‘: {idx+1}/{sample_size}",
                        "current_uid": row.get(uid_column, f'ROW_{idx+1}'),
                        "processed": idx + 1,
                        "total": sample_size
                    })
                    
                    # ë¶„ì„ ìˆ˜í–‰ (ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
                    uid = str(row.get(uid_column, f'EMP_{idx+1}'))  
                    opinion = str(row.get(opinion_column, ''))
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    name = str(row.get(name_column, '')) if name_column else ''
                    department = str(row.get(department_column, '')) if department_column else ''
                    position = str(row.get(position_column, '')) if position_column else ''
                    
                    # API í‚¤ ì²˜ë¦¬: í™˜ê²½ë³€ìˆ˜ ìš°ì„ , ê·¸ ë‹¤ìŒ í´ë¼ì´ì–¸íŠ¸ í‚¤
                    from app.core.config import settings
                    
                    logger.info(f"ğŸ”‘ API í‚¤ ì²˜ë¦¬ - enable_ai_feedback: {job_data.get('enable_ai_feedback')}")
                    
                    # 1. ë¨¼ì € í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ í™•ì¸
                    api_key = settings.OPENAI_API_KEY
                    if api_key and api_key.startswith('sk-'):
                        logger.info(f"âœ… í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ì‚¬ìš©: {api_key[:20]}...")
                    else:
                        # 2. í™˜ê²½ë³€ìˆ˜ì— ì—†ìœ¼ë©´ í´ë¼ì´ì–¸íŠ¸ ì œê³µ í‚¤ ì‚¬ìš©
                        client_api_key = job_data.get('openai_api_key')
                        if client_api_key and client_api_key.startswith('sk-') and len(client_api_key) > 20:
                            api_key = client_api_key
                            logger.info(f"ğŸ“± í´ë¼ì´ì–¸íŠ¸ ì œê³µ API í‚¤ ì‚¬ìš©: {api_key[:20]}...")
                        else:
                            api_key = None
                            logger.warning("âš ï¸ ìœ íš¨í•œ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. LLM ë¶„ì„ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                    
                    logger.info(f"ğŸ”‘ ìµœì¢… API í‚¤ ì‚¬ìš©: {'ìˆìŒ' if api_key else 'ì—†ìŒ'}")
                    
                    # HybridAnalyzerë¡œ ì¢…í•© ë¶„ì„
                    logger.info(f"ğŸ”¬ HybridAnalyzer í˜¸ì¶œ ì‹œì‘ - UID: {uid}")
                    result = await analyzer.comprehensive_analysis(
                        uid=uid,
                        opinion=opinion,
                        row_data=row,
                        save_to_storage=True,
                        file_id=file_id,
                        filename=filename,
                        enable_ai=job_data.get('enable_ai_feedback', False),
                        openai_api_key=api_key,
                        openai_model=job_data.get('openai_model', 'gpt-3.5-turbo'),
                        max_tokens=job_data.get('max_tokens', 1200)
                    )
                    
                    logger.info(f"âœ… HybridAnalyzer ë¶„ì„ ì™„ë£Œ - UID: {uid}")
                    
                    # ê²°ê³¼ ì •ë¦¬
                    ai_feedback_data = result.get('ai_feedback', {})
                    analysis_results.append({
                        "uid": uid,
                        "name": name,
                        "department": department,
                        "position": position,
                        "opinion": opinion[:200] + '...' if len(opinion) > 200 else opinion,
                        "score": result['hybrid_analysis']['overall_score'],
                        "grade": result['hybrid_analysis']['grade'],
                        "confidence": result['hybrid_analysis']['confidence'],
                        "text_score": result['text_analysis']['overall_score'],
                        "quantitative_score": result['quantitative_analysis']['quantitative_score'],
                        "dimension_scores": result['text_analysis']['dimension_scores'],
                        "explainability": result['explainability'],
                        "ai_feedback": {
                            'strengths': self._extract_strengths(ai_feedback_data),
                            'improvements': self._extract_improvements(ai_feedback_data),
                            'overall_comment': ai_feedback_data.get('ai_feedback', '')
                        }
                    })
                    
                except Exception as e:
                    logger.error(f"âŒ í–‰ {idx+1} ë¶„ì„ ì˜¤ë¥˜: {e}")
                    logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
                    logger.error(f"ë¶„ì„ íŒŒë¼ë¯¸í„°: enable_ai={job_data.get('enable_ai_feedback')}, api_key={'ìˆìŒ' if api_key else 'ì—†ìŒ'}")
                    import traceback
                    logger.error(f"íŠ¸ë ˆì´ìŠ¤ë°±:\n{traceback.format_exc()}")
                    
                    analysis_results.append({
                        "uid": row.get(uid_column, f'ROW_{idx+1}'),
                        "name": row.get('name', ''),
                        "score": 0,
                        "grade": "ERROR",
                        "error": str(e)
                    })
                    
                    # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ì§„í–‰ë¥ ì€ ê³„ì† ì—…ë°ì´íŠ¸
                    progress = 30 + ((idx + 1) / sample_size) * 50  # 30-80% êµ¬ê°„
                    await self.update_progress(job_id, progress, {
                        "processed": idx + 1,
                        "total": sample_size,
                        "current_uid": row.get(uid_column, f'ROW_{idx+1}'),
                        "error": str(e)
                    })
            
            await self.update_progress(job_id, 85, {
                "status": "ê²°ê³¼ ìƒì„± ì¤‘",
                "processed": sample_size,
                "total": sample_size
            })
            
            # 7. ë¶„ì„ ê²°ê³¼ ìš”ì•½
            valid_results = [r for r in analysis_results if 'error' not in r]
            if valid_results:
                avg_score = sum(r['score'] for r in valid_results) / len(valid_results)
                grade_distribution = {}
                for r in valid_results:
                    grade = r['grade']
                    grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
            else:
                avg_score = 0
                grade_distribution = {}
            
            # 8. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            results = {
                "job_id": job_id,
                "file_id": file_id,
                "filename": filename,
                "data": analysis_results,
                "analysis_results": analysis_results,  # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±
                "summary": {
                    "total_analyzed": len(analysis_results),
                    "successful": len(valid_results),
                    "failed": len(analysis_results) - len(valid_results),
                    "average_score": round(avg_score, 1),
                    "grade_distribution": grade_distribution,
                    "analysis_mode": job_data.get('analysis_mode', 'hybrid'),
                    "ai_enabled": job_data.get('enable_ai_feedback', False)
                },
                "metadata": {
                    "analyzer_version": "AIRISS v4.0",
                    "analysis_timestamp": datetime.now().isoformat(),
                    "system_status": analyzer.get_system_status()
                }
            }
            
            await self.update_progress(job_id, 95, {"status": "ë¶„ì„ ì™„ë£Œ"})
            
            # 9. ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
            await self.complete_analysis(job_id, results)
            
            # 10. EmployeeResult í…Œì´ë¸”ì— ê° ì§ì› ê²°ê³¼ ì €ì¥
            logger.info(f"ğŸ”¥ EmployeeResult ì €ì¥ í•¨ìˆ˜ í˜¸ì¶œ ì „: job_id={job_id}, ê²°ê³¼ ê°œìˆ˜={len(analysis_results)}")
            try:
                await self._save_employee_results(job_id, analysis_results)
            except Exception as save_error:
                logger.error(f"âŒ EmployeeResult ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {save_error}")
                # ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¶„ì„ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            
            # 11. ë¶„ì„ ì‘ì—… ì •ë³´ëŠ” ì´ë¯¸ complete_analysisì—ì„œ ì €ì¥ë¨
            logger.info(f"âœ… ë¶„ì„ ì²˜ë¦¬ ì™„ë£Œ: job_id={job_id}, ì´ {len(analysis_results)}ëª… ë¶„ì„")
            
        except Exception as e:
            import traceback
            logger.error(f"âŒ ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            await self.fail_analysis(job_id, str(e))
    
    async def _save_employee_results(self, job_id: str, analysis_results: list):
        """ë¶„ì„ ê²°ê³¼ë¥¼ EmployeeResult í…Œì´ë¸”ì— ì €ì¥"""
        try:
            logger.info(f"ğŸ”„ EmployeeResult ì €ì¥ ì‹œì‘: job_id={job_id}, ê²°ê³¼ ê°œìˆ˜={len(analysis_results)}")
            from app.db.database import get_db
            from app.models.employee import EmployeeResult
            import uuid
            
            # ë™ê¸° DB ì„¸ì…˜ ìƒì„±
            def save_results():
                db = next(get_db())
                try:
                    logger.info(f"ğŸ“Š DB ì„¸ì…˜ ìƒì„± ì™„ë£Œ, EmployeeResult í…Œì´ë¸”ì— ì €ì¥ ì‹œì‘")
                    
                    # ê¸°ì¡´ ê²°ê³¼ ì‚­ì œ (job_id ê¸°ì¤€)
                    deleted_count = db.query(EmployeeResult).filter(EmployeeResult.job_id == job_id).delete()
                    logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ê²°ê³¼ {deleted_count}ê°œ ì‚­ì œë¨")
                    
                    # ìƒˆ ê²°ê³¼ ì €ì¥
                    saved_count = 0
                    for i, result in enumerate(analysis_results):
                        if 'error' not in result:
                            logger.info(f"ğŸ’¾ ì €ì¥ ì¤‘ [{i+1}/{len(analysis_results)}]: uid={result.get('uid')}, score={result.get('score')}")
                            employee_result = EmployeeResult(
                                id=str(uuid.uuid4()),
                                job_id=job_id,
                                uid=result['uid'],
                                overall_score=result['score'],
                                grade=result['grade'],
                                text_score=result.get('text_score', 0),
                                quantitative_score=result.get('quantitative_score', 0),
                                confidence=result.get('confidence', 0),
                                dimension_scores=result.get('dimension_scores', {}),
                                ai_feedback={
                                    'strengths': result.get('ai_feedback', {}).get('strengths', []),
                                    'improvements': result.get('ai_feedback', {}).get('improvements', []),
                                    'overall_comment': result.get('ai_feedback', {}).get('overall_comment', '')
                                },
                                employee_metadata={
                                    'name': result.get('name', ''),
                                    'department': result.get('department', ''),
                                    'position': result.get('position', '')
                                }
                            )
                            db.add(employee_result)
                            saved_count += 1
                        else:
                            logger.warning(f"âš ï¸ ì˜¤ë¥˜ê°€ ìˆëŠ” ê²°ê³¼ ê±´ë„ˆëœ€ [{i+1}]: {result.get('error')}")
                    
                    db.commit()
                    logger.info(f"âœ… {saved_count}ê°œì˜ ì§ì› ë¶„ì„ ê²°ê³¼ë¥¼ EmployeeResult í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ")
                    
                except Exception as e:
                    db.rollback()
                    logger.error(f"âŒ EmployeeResult ì €ì¥ ì˜¤ë¥˜: {e}")
                finally:
                    db.close()
            
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ - Python 3.9 í˜¸í™˜
            import asyncio
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, save_results)
            
        except Exception as e:
            logger.error(f"âŒ EmployeeResult ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _extract_strengths(self, ai_feedback_data: dict) -> list:
        """í¼ë“œë°±ì—ì„œ ê°•ì  ì¶”ì¶œ"""
        if not ai_feedback_data:
            return []
        
        # ai_strengths í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if 'ai_strengths' in ai_feedback_data:
            strengths_text = ai_feedback_data.get('ai_strengths', '')
            if strengths_text:
                # ë¬¸ìì—´ì—ì„œ ê°•ì  ì¶”ì¶œ (ì½¤ë§ˆë¡œ êµ¬ë¶„)
                return [s.strip() for s in strengths_text.split(',') if s.strip()][:3]
        
        # ai_feedback í•„ë“œì—ì„œ ê°•ì  ì¶”ì¶œ
        feedback_text = ai_feedback_data.get('ai_feedback', '')
        if 'ê°•ì ' in feedback_text:
            # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
            import re
            strengths_match = re.search(r'ê°•ì [:ì€]([^\nì¹˜]+)', feedback_text)
            if strengths_match:
                strengths = strengths_match.group(1).strip()
                return [s.strip() for s in strengths.split(',') if s.strip()][:3]
        
        # ê¸°ë³¸ê°’
        return ["íŒ€ì›Œí¬ ìš°ìˆ˜", "ì„±ì‹¤í•œ ì—…ë¬´ íƒœë„"]
    
    def _extract_improvements(self, ai_feedback_data: dict) -> list:
        """í”¼ë“œë°±ì—ì„œ ê°œì„ ì  ì¶”ì¶œ"""
        if not ai_feedback_data:
            return []
        
        # ai_weaknesses í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if 'ai_weaknesses' in ai_feedback_data:
            weaknesses_text = ai_feedback_data.get('ai_weaknesses', '')
            if weaknesses_text:
                # ë¬¸ìì—´ì—ì„œ ê°œì„ ì  ì¶”ì¶œ (ì½¤ë§ˆë¡œ êµ¬ë¶„)
                return [w.strip() for w in weaknesses_text.split(',') if w.strip()][:2]
        
        # ai_feedback í•„ë“œì—ì„œ ê°œì„ ì  ì¶”ì¶œ
        feedback_text = ai_feedback_data.get('ai_feedback', '')
        if 'ê°œì„ ' in feedback_text or 'ë³´ì™„' in feedback_text:
            # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
            import re
            improvements_match = re.search(r'(ê°œì„ |ë³´ì™„)[:ì ì´]([^\nê°•]+)', feedback_text)
            if improvements_match:
                improvements = improvements_match.group(2).strip()
                return [i.strip() for i in improvements.split(',') if i.strip()][:2]
        
        # ê¸°ë³¸ê°’
        return ["ì „ë¬¸ì„± í–¥ìƒ í•„ìš”"]
    
    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        # ë¨¼ì € ë©”ëª¨ë¦¬ì—ì„œ í™•ì¸
        job_info = self.active_jobs.get(job_id)
        if job_info:
            return job_info
        
        # ë©”ëª¨ë¦¬ì— ì—†ìœ¼ë©´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™•ì¸
        try:
            from app.db.database import get_db
            from app.models.job import Job
            import json
            
            db = next(get_db())
            try:
                job = db.query(Job).filter(Job.id == job_id).first()
                if job:
                    # ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ë¥¼ ë©”ëª¨ë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    job_info = {
                        'status': job.status,
                        'start_time': job.created_at,
                        'progress': 100 if job.status == 'completed' else 0,
                        'data': json.loads(job.job_data) if job.job_data else {}
                    }
                    
                    # ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                    if job.results_data:
                        results = json.loads(job.results_data) if isinstance(job.results_data, str) else job.results_data
                        job_info['results'] = results
                    
                    logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‘ì—… ì°¾ìŒ: {job_id}, ìƒíƒœ: {job.status}")
                    return job_info
                else:
                    logger.warning(f"âŒ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}")
                    return None
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def list_active_jobs(self) -> Dict[str, Any]:
        """í™œì„± ì‘ì—… ëª©ë¡"""
        return {
            "active_jobs": len(self.active_jobs),
            "jobs": list(self.active_jobs.keys())
        }
    
    async def get_analysis_results(self, job_id: str) -> Optional[pd.DataFrame]:
        """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
        try:
            # 1. ë¨¼ì € ë©”ëª¨ë¦¬ì—ì„œ ì‘ì—… í™•ì¸
            job_info = self.active_jobs.get(job_id)
            if not job_info:
                logger.warning(f"job_id {job_id}ì— ëŒ€í•œ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            # 2. ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
            if job_info['status'] != 'completed':
                logger.warning(f"ì‘ì—… {job_id}ê°€ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {job_info['status']}")
                return None
            
            # 3. ê²°ê³¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            results = job_info.get('results', {})
            data = results.get('data', [])
            
            if not data:
                logger.warning(f"job_id {job_id}ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            # 4. DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame(data)
            logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì„±ê³µ: {len(df)} ê°œì˜ ê²°ê³¼")
            return df
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    async def export_results(self, job_id: str, format: str = "excel") -> Optional[bytes]:
        """ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        try:
            # ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
            df = await self.get_analysis_results(job_id)
            
            if df is None or df.empty:
                logger.warning(f"job_id {job_id}ì— ëŒ€í•œ ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            # ì‘ì—… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            job_info = self.active_jobs.get(job_id, {})
            results = job_info.get('results', {})
            summary = results.get('summary', {})
            metadata = results.get('metadata', {})
            
            # í˜•ì‹ì— ë”°ë¼ ë‚´ë³´ë‚´ê¸°
            if format.lower() == "excel":
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    # ë¶„ì„ ê²°ê³¼ ì‹œíŠ¸
                    df.to_excel(writer, sheet_name='Analysis_Results', index=False)
                    
                    # ìš”ì•½ ì •ë³´ ì‹œíŠ¸
                    summary_df = pd.DataFrame([summary])
                    summary_df.to_excel(writer, sheet_name='Summary', index=False)
                    
                    # ë©”íƒ€ë°ì´í„° ì‹œíŠ¸
                    metadata_df = pd.DataFrame([metadata])
                    metadata_df.to_excel(writer, sheet_name='Metadata', index=False)
                    
                    # ì—‘ì…€ íŒŒì¼ í¬ë§·íŒ…
                    workbook = writer.book
                    for sheet_name in workbook.sheetnames:
                        worksheet = workbook[sheet_name]
                        # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
                        for column in worksheet.columns:
                            max_length = 0
                            column_letter = column[0].column_letter
                            for cell in column:
                                try:
                                    if len(str(cell.value)) > max_length:
                                        max_length = len(str(cell.value))
                                except:
                                    pass
                            adjusted_width = min(max_length + 2, 50)
                            worksheet.column_dimensions[column_letter].width = adjusted_width
                
                output.seek(0)
                return output.getvalue()
                
            elif format.lower() == "csv":
                output = io.StringIO()
                df.to_csv(output, index=False, encoding='utf-8-sig')
                return output.getvalue().encode('utf-8-sig')
                
            elif format.lower() == "json":
                export_data = {
                    "job_id": job_id,
                    "results": df.to_dict(orient='records'),
                    "summary": summary,
                    "metadata": metadata
                }
                return json.dumps(export_data, ensure_ascii=False, indent=2).encode('utf-8')
                
            else:
                logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def _get_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ë°˜í™˜"""
        from app.db.database import get_db
        return next(get_db())