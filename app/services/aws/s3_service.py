# app/services/aws/s3_service.py
"""
AIRISS v4.0 - AWS S3 ë‹¤ìš´ë¡œë“œ ì„œë¹„ìŠ¤
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ S3 í†µí•©
"""

import boto3
import os
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
import json
import pandas as pd
import io
from botocore.exceptions import NoCredentialsError, ClientError
import asyncio
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)

class S3DownloadService:
    """AWS S3ë¥¼ í™œìš©í•œ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ì¸ ë‹¤ìš´ë¡œë“œ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """S3 ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.enabled = os.getenv("S3_DOWNLOAD_ENABLED", "false").lower() == "true"
        
        if not self.enabled:
            logger.info("ğŸ”§ S3 ë‹¤ìš´ë¡œë“œ ì„œë¹„ìŠ¤ ë¹„í™œì„±í™” ìƒíƒœ")
            return
        
        # AWS ì„¤ì •
        self.region = os.getenv("AWS_DEFAULT_REGION", "ap-northeast-2")
        self.bucket_name = os.getenv("AWS_S3_BUCKET_NAME", "airiss-downloads")
        
        # ë‹¤ìš´ë¡œë“œ ì„¤ì •
        self.expiry_hours = int(os.getenv("S3_DOWNLOAD_EXPIRY_HOURS", "24"))
        self.max_file_size_mb = int(os.getenv("S3_MAX_FILE_SIZE_MB", "50"))
        self.url_expiry_minutes = int(os.getenv("S3_URL_EXPIRY_MINUTES", "60"))
        
        # Thread pool for async operations
        self.thread_pool = ThreadPoolExecutor(max_workers=3)
        
        try:
            # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.s3_client = boto3.client(
                's3',
                region_name=self.region,
                aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
                aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY")
            )
            
            # ë²„í‚· ì¡´ì¬ í™•ì¸ (ì—†ìœ¼ë©´ ìƒì„±)
            self._ensure_bucket_exists()
            
            logger.info(f"âœ… S3 ë‹¤ìš´ë¡œë“œ ì„œë¹„ìŠ¤ í™œì„±í™” ì™„ë£Œ")
            logger.info(f"ğŸ“¦ ë²„í‚·: {self.bucket_name}, ë¦¬ì „: {self.region}")
            logger.info(f"â° íŒŒì¼ ë³´ê´€: {self.expiry_hours}ì‹œê°„, URL ìœ íš¨: {self.url_expiry_minutes}ë¶„")
            
        except NoCredentialsError:
            logger.error("âŒ AWS ìê²© ì¦ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            self.enabled = False
        except Exception as e:
            logger.error(f"âŒ S3 ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.enabled = False
    
    def _ensure_bucket_exists(self):
        """S3 ë²„í‚· ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""
        try:
            # ë²„í‚· ì¡´ì¬ í™•ì¸
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            logger.info(f"âœ… S3 ë²„í‚· í™•ì¸ ì™„ë£Œ: {self.bucket_name}")
            
        except ClientError as e:
            error_code = int(e.response['Error']['Code'])
            
            if error_code == 404:
                # ë²„í‚·ì´ ì—†ìœ¼ë©´ ìƒì„±
                logger.info(f"ğŸ“¦ S3 ë²„í‚· ìƒì„± ì¤‘: {self.bucket_name}")
                
                if self.region == 'us-east-1':
                    # us-east-1ì€ LocationConstraint ì—†ì´ ìƒì„±
                    self.s3_client.create_bucket(Bucket=self.bucket_name)
                else:
                    # ë‹¤ë¥¸ ë¦¬ì „ì€ LocationConstraint í•„ìš”
                    self.s3_client.create_bucket(
                        Bucket=self.bucket_name,
                        CreateBucketConfiguration={'LocationConstraint': self.region}
                    )
                
                # ìˆ˜ëª… ì£¼ê¸° ì •ì±… ì„¤ì • (ìë™ ì •ë¦¬)
                lifecycle_policy = {
                    'Rules': [
                        {
                            'ID': 'DeleteTempFiles',
                            'Status': 'Enabled',
                            'Filter': {'Prefix': 'airiss-temp/'},
                            'Expiration': {'Days': 1}  # 1ì¼ í›„ ìë™ ì‚­ì œ
                        }
                    ]
                }
                
                self.s3_client.put_bucket_lifecycle_configuration(
                    Bucket=self.bucket_name,
                    LifecycleConfiguration=lifecycle_policy
                )
                
                logger.info(f"âœ… S3 ë²„í‚· ìƒì„± ë° ì •ì±… ì„¤ì • ì™„ë£Œ: {self.bucket_name}")
                
            else:
                raise e
    
    async def upload_analysis_result_async(self, job_id: str, df: pd.DataFrame, format: str = "excel") -> Optional[Dict[str, Any]]:
        """ë¹„ë™ê¸° ë¶„ì„ ê²°ê³¼ ì—…ë¡œë“œ"""
        if not self.enabled:
            logger.warning("âš ï¸ S3 ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            return None
        
        try:
            # ë¹„ë™ê¸°ë¡œ ì—…ë¡œë“œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.thread_pool,
                self._upload_analysis_result_sync,
                job_id, df, format
            )
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë¹„ë™ê¸° ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _upload_analysis_result_sync(self, job_id: str, df: pd.DataFrame, format: str) -> Dict[str, Any]:
        """ë™ê¸° ë°©ì‹ ë¶„ì„ ê²°ê³¼ ì—…ë¡œë“œ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # S3 í‚¤ ìƒì„±
        s3_key = f"airiss-temp/{job_id}/{timestamp}_result.{format}"
        
        # íŒŒì¼ ë°ì´í„° ì¤€ë¹„
        file_data = self._prepare_file_data(df, format)
        if not file_data:
            raise Exception(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size_mb = len(file_data) / (1024 * 1024)
        if file_size_mb > self.max_file_size_mb:
            raise Exception(f"íŒŒì¼ í¬ê¸° ì´ˆê³¼: {file_size_mb:.1f}MB (ìµœëŒ€: {self.max_file_size_mb}MB)")
        
        # S3 ì—…ë¡œë“œ
        self.s3_client.put_object(
            Bucket=self.bucket_name,
            Key=s3_key,
            Body=file_data,
            ContentType=self._get_content_type(format),
            Metadata={
                'job_id': job_id,
                'created_at': timestamp,
                'format': format,
                'records': str(len(df)),
                'file_size_mb': f"{file_size_mb:.2f}"
            }
        )
        
        logger.info(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_key} ({file_size_mb:.1f}MB)")
        
        return {
            's3_key': s3_key,
            'file_size_mb': round(file_size_mb, 2),
            'records_count': len(df),
            'upload_time': timestamp,
            'bucket': self.bucket_name
        }
    
    def _prepare_file_data(self, df: pd.DataFrame, format: str) -> Optional[bytes]:
        """í˜•ì‹ë³„ íŒŒì¼ ë°ì´í„° ì¤€ë¹„"""
        try:
            if format.lower() == "csv":
                # CSV í˜•ì‹
                output = io.StringIO()
                df.to_csv(output, index=False, encoding='utf-8')
                return output.getvalue().encode('utf-8-sig')
                
            elif format.lower() == "json":
                # JSON í˜•ì‹
                json_data = df.to_json(orient='records', force_ascii=False, indent=2)
                return json_data.encode('utf-8')
                
            elif format.lower() in ["excel", "xlsx"]:
                # Excel í˜•ì‹ (í–¥ìƒëœ ë²„ì „)
                output = io.BytesIO()
                
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    # ìš”ì•½ ì‹œíŠ¸
                    summary_data = {
                        'í•­ëª©': [
                            'ë¶„ì„ì¼ì‹œ', 'ì´ ë¶„ì„ê±´ìˆ˜', 'í‰ê·  ì ìˆ˜', 'ìµœê³  ì ìˆ˜', 'ìµœì € ì ìˆ˜',
                            'Së“±ê¸‰ ìˆ˜', 'Aë“±ê¸‰ ìˆ˜', 'Bë“±ê¸‰ ìˆ˜', 'Cë“±ê¸‰ ìˆ˜', 'Dë“±ê¸‰ ìˆ˜'
                        ]
                    }
                    
                    # ì ìˆ˜ ì»¬ëŸ¼ ì°¾ê¸°
                    score_col = None
                    for col in ['AIRISS_v4_ì¢…í•©ì ìˆ˜', 'ì¢…í•©ì ìˆ˜', 'overall_score']:
                        if col in df.columns:
                            score_col = col
                            break
                    
                    # ë“±ê¸‰ ë¶„í¬ ê³„ì‚°
                    grade_counts = {'OKâ˜…â˜…â˜…': 0, 'OKâ˜…â˜…': 0, 'OKâ˜…': 0, 'OK A': 0, 'OK B+': 0, 'OK B': 0, 'OK C': 0, 'OK D': 0}
                    grade_col = None
                    for col in ['OKë“±ê¸‰', 'ë“±ê¸‰', 'grade']:
                        if col in df.columns:
                            grade_col = col
                            break
                    
                    if grade_col:
                        grade_counts = df[grade_col].value_counts().to_dict()
                    
                    summary_data['ê°’'] = [
                        datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        len(df),
                        round(df[score_col].mean(), 1) if score_col else 'N/A',
                        df[score_col].max() if score_col else 'N/A',
                        df[score_col].min() if score_col else 'N/A',
                        grade_counts.get('OKâ˜…â˜…â˜…', 0),
                        grade_counts.get('OKâ˜…â˜…', 0) + grade_counts.get('OKâ˜…', 0) + grade_counts.get('OK A', 0),
                        grade_counts.get('OK B+', 0) + grade_counts.get('OK B', 0),
                        grade_counts.get('OK C', 0),
                        grade_counts.get('OK D', 0)
                    ]
                    
                    summary_df = pd.DataFrame(summary_data)
                    summary_df.to_excel(writer, sheet_name='ğŸ“Š ìš”ì•½', index=False)
                    
                    # ìƒì„¸ ê²°ê³¼ ì‹œíŠ¸
                    df.to_excel(writer, sheet_name='ğŸ“‹ ìƒì„¸ê²°ê³¼', index=False)
                    
                    # ê³ ë“ì ì ì‹œíŠ¸ (90ì  ì´ìƒ)
                    if score_col and score_col in df.columns:
                        high_performers = df[df[score_col] >= 90]
                        if not high_performers.empty:
                            high_performers.to_excel(writer, sheet_name='ğŸŒŸ ìš°ìˆ˜ì', index=False)
                        
                        # ê°œì„ í•„ìš” ì‹œíŠ¸ (60ì  ë¯¸ë§Œ)
                        low_performers = df[df[score_col] < 60]
                        if not low_performers.empty:
                            low_performers.to_excel(writer, sheet_name='âš¡ ê°œì„ í•„ìš”', index=False)
                
                return output.getvalue()
            
            else:
                logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_content_type(self, format: str) -> str:
        """í˜•ì‹ë³„ Content-Type ë°˜í™˜"""
        content_types = {
            'csv': 'text/csv',
            'json': 'application/json',
            'excel': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        }
        return content_types.get(format.lower(), 'application/octet-stream')
    
    def generate_download_url(self, s3_key: str, filename: str = None) -> Optional[str]:
        """ì•ˆì „í•œ ì„ì‹œ ë‹¤ìš´ë¡œë“œ URL ìƒì„±"""
        if not self.enabled:
            return None
        
        try:
            # íŒŒì¼ëª… ì„¤ì •
            if not filename:
                filename = s3_key.split('/')[-1]
            
            # Presigned URL ìƒì„±
            url = self.s3_client.generate_presigned_url(
                'get_object',
                Params={
                    'Bucket': self.bucket_name,
                    'Key': s3_key,
                    'ResponseContentDisposition': f'attachment; filename="{filename}"'
                },
                ExpiresIn=self.url_expiry_minutes * 60
            )
            
            logger.info(f"ğŸ”— ë‹¤ìš´ë¡œë“œ URL ìƒì„±: {s3_key} (ìœ íš¨: {self.url_expiry_minutes}ë¶„)")
            return url
            
        except Exception as e:
            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ URL ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def check_file_exists(self, s3_key: str) -> bool:
        """S3 íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        if not self.enabled:
            return False
        
        try:
            self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)
            return True
        except ClientError:
            return False
    
    def get_file_info(self, s3_key: str) -> Optional[Dict[str, Any]]:
        """S3 íŒŒì¼ ì •ë³´ ì¡°íšŒ"""
        if not self.enabled:
            return None
        
        try:
            response = self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)
            
            return {
                'size_bytes': response['ContentLength'],
                'size_mb': round(response['ContentLength'] / (1024 * 1024), 2),
                'last_modified': response['LastModified'].isoformat(),
                'content_type': response.get('ContentType', 'unknown'),
                'metadata': response.get('Metadata', {})
            }
            
        except ClientError as e:
            logger.error(f"âŒ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_old_files(self, days_old: int = 1):
        """ì˜¤ë˜ëœ ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        if not self.enabled:
            return
        
        try:
            # ë²„í‚·ì˜ ìˆ˜ëª… ì£¼ê¸° ì •ì±…ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë³„ë„ ì •ë¦¬ ë¶ˆí•„ìš”
            logger.info(f"ğŸ“ S3 ìˆ˜ëª… ì£¼ê¸° ì •ì±…ìœ¼ë¡œ {days_old}ì¼ í›„ ìë™ ì •ë¦¬ë©ë‹ˆë‹¤")
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_service_status(self) -> Dict[str, Any]:
        """S3 ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
        return {
            'enabled': self.enabled,
            'bucket_name': self.bucket_name if self.enabled else None,
            'region': self.region if self.enabled else None,
            'max_file_size_mb': self.max_file_size_mb if self.enabled else None,
            'url_expiry_minutes': self.url_expiry_minutes if self.enabled else None,
            'auto_cleanup_hours': self.expiry_hours if self.enabled else None
        }

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
s3_service = S3DownloadService()
