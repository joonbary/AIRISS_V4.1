import asyncpg
import json
import uuid
import pickle
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List

logger = logging.getLogger(__name__)

class PostgresService:
    """PostgreSQL(Neon) ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, database_url: str):
        # SQLAlchemy URLì„ asyncpg URLë¡œ ë³€í™˜
        if database_url.startswith("postgresql+psycopg2://"):
            self.database_url = database_url.replace("postgresql+psycopg2://", "postgresql://")
        else:
            self.database_url = database_url
        logger.info(f"ğŸ˜ PostgreSQL ì—°ê²° ì¤€ë¹„: {self.database_url.split('@')[1]}")  # ë¹„ë°€ë²ˆí˜¸ ìˆ¨ê¹€
        
    async def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° í…Œì´ë¸” ìƒì„±"""
        try:
            conn = await asyncpg.connect(self.database_url)
            # Files í…Œì´ë¸”
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id TEXT PRIMARY KEY,
                    filename TEXT NOT NULL,
                    upload_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    total_records INTEGER NOT NULL,
                    columns JSONB NOT NULL,
                    uid_columns JSONB NOT NULL,
                    opinion_columns JSONB NOT NULL,
                    quantitative_columns JSONB NOT NULL,
                    file_data BYTEA
                )
            """)
            # Jobs í…Œì´ë¸”
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS jobs (
                    id TEXT PRIMARY KEY,
                    file_id TEXT NOT NULL REFERENCES files(id) ON DELETE CASCADE,
                    status TEXT NOT NULL,
                    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    job_data JSONB NOT NULL
                )
            """)
            # Results í…Œì´ë¸”
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS results (
                    id TEXT PRIMARY KEY,
                    job_id TEXT NOT NULL REFERENCES jobs(id) ON DELETE CASCADE,
                    uid TEXT NOT NULL,
                    result_data JSONB NOT NULL,
                    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
                )
            """)
            # ì¸ë±ìŠ¤ ìƒì„±
            await conn.execute("CREATE INDEX IF NOT EXISTS idx_jobs_file_id ON jobs(file_id)")
            await conn.execute("CREATE INDEX IF NOT EXISTS idx_jobs_status ON jobs(status)")
            await conn.execute("CREATE INDEX IF NOT EXISTS idx_results_job_id ON results(job_id)")
            await conn.close()
            logger.info("âœ… PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            raise

    async def save_file(self, file_data: Dict[str, Any]) -> str:
        """íŒŒì¼ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            file_id = str(uuid.uuid4())
            df = file_data.get('dataframe')
            file_data_blob = pickle.dumps(df) if df is not None else None
            conn = await asyncpg.connect(self.database_url)
            try:
                await conn.execute("""
                    INSERT INTO files (
                        id, filename, upload_time, total_records, 
                        columns, uid_columns, opinion_columns, 
                        quantitative_columns, file_data
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                """, 
                    file_id,
                    file_data['filename'],
                    datetime.now(),
                    file_data['total_records'],
                    json.dumps(file_data['columns']),
                    json.dumps(file_data['uid_columns']),
                    json.dumps(file_data['opinion_columns']),
                    json.dumps(file_data.get('quantitative_columns', [])),
                    file_data_blob
                )
            finally:
                await conn.close()
            logger.info(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_id}")
            return file_id
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            raise

    async def get_file(self, file_id: str) -> Optional[Dict[str, Any]]:
        """íŒŒì¼ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ"""
        try:
            conn = await asyncpg.connect(self.database_url)
            try:
                row = await conn.fetchrow("""
                    SELECT id, filename, upload_time, total_records,
                           columns, uid_columns, opinion_columns,
                           quantitative_columns, file_data
                    FROM files WHERE id = $1
                """, file_id)
                if not row:
                    return None
                df = pickle.loads(row['file_data']) if row['file_data'] is not None else None
                return {
                    'id': row['id'],
                    'filename': row['filename'],
                    'upload_time': row['upload_time'].isoformat(),
                    'total_records': row['total_records'],
                    'columns': json.loads(row['columns']),
                    'uid_columns': json.loads(row['uid_columns']),
                    'opinion_columns': json.loads(row['opinion_columns']),
                    'quantitative_columns': json.loads(row['quantitative_columns']),
                    'dataframe': df
                }
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            raise

    async def list_files(self, limit: int = 100) -> List[Dict[str, Any]]:
        """íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            conn = await asyncpg.connect(self.database_url)
            try:
                rows = await conn.fetch("""
                    SELECT id, filename, upload_time, total_records
                    FROM files 
                    ORDER BY upload_time DESC 
                    LIMIT $1
                """, limit)
                return [
                    {
                        'id': row['id'],
                        'filename': row['filename'],
                        'upload_time': row['upload_time'].isoformat() if row['upload_time'] else None,
                        'total_records': row['total_records']
                    }
                    for row in rows
                ]
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

    async def create_analysis_job(self, job_data: Dict[str, Any]) -> str:
        """ë¶„ì„ ì‘ì—… ìƒì„±"""
        try:
            job_id = job_data.get('job_id', str(uuid.uuid4()))
            status = job_data.get('status', 'created')
            conn = await asyncpg.connect(self.database_url)
            try:
                await conn.execute("""
                    INSERT INTO jobs (
                        id, file_id, status, created_at, updated_at, job_data
                    ) VALUES ($1, $2, $3, $4, $5, $6)
                """,
                    job_id,
                    job_data['file_id'],
                    status,
                    datetime.now(),
                    datetime.now(),
                    json.dumps(job_data)
                )
            finally:
                await conn.close()
            logger.info(f"âœ… ë¶„ì„ ì‘ì—… ìƒì„± ì™„ë£Œ: {job_id}")
            return job_id
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‘ì—… ìƒì„± ì˜¤ë¥˜: {e}")
            raise

    async def get_analysis_job(self, job_id: str) -> Optional[Dict[str, Any]]:
        """ë¶„ì„ ì‘ì—… ì¡°íšŒ"""
        try:
            conn = await asyncpg.connect(self.database_url)
            try:
                row = await conn.fetchrow("""
                    SELECT id, file_id, status, created_at, updated_at, job_data
                    FROM jobs WHERE id = $1
                """, job_id)
                if not row:
                    return None
                job_data = json.loads(row['job_data'])
                job_data.update({
                    'job_id': row['id'],
                    'file_id': row['file_id'],
                    'status': row['status'],
                    'created_at': row['created_at'].isoformat(),
                    'updated_at': row['updated_at'].isoformat()
                })
                return job_data
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‘ì—… ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None

    async def update_analysis_job(self, job_id: str, updates: Dict[str, Any]) -> bool:
        """ë¶„ì„ ì‘ì—… ì—…ë°ì´íŠ¸"""
        try:
            conn = await asyncpg.connect(self.database_url)
            try:
                # ê¸°ì¡´ job_data ì¡°íšŒ
                row = await conn.fetchrow("SELECT job_data FROM jobs WHERE id = $1", job_id)
                if not row:
                    return False
                job_data = json.loads(row['job_data'])
                job_data.update(updates)
                status = job_data.get('status', 'completed')
                await conn.execute("""
                    UPDATE jobs 
                    SET status = $1, updated_at = $2, job_data = $3
                    WHERE id = $4
                """,
                    status,
                    datetime.now(),
                    json.dumps(job_data),
                    job_id
                )
            finally:
                await conn.close()
            logger.debug(f"âœ… ë¶„ì„ ì‘ì—… ì—…ë°ì´íŠ¸ ì™„ë£Œ: {job_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‘ì—… ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return False
    # ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ë„ ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ êµ¬í˜„
    # list_files, delete_file, save_analysis_result, get_analysis_results ë“±
    # SQLite ë²„ì „ê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€, asyncpg ë¬¸ë²•ë§Œ ì‚¬ìš© 