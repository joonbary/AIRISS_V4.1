# app/api/downloads_aws.py
# AIRISS v4.0 AWS S3 ë‹¤ìš´ë¡œë“œ API - Railway ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°

from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import RedirectResponse, FileResponse
import pandas as pd
import logging
from datetime import datetime
import os
import asyncio

# AWS S3 ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì € import
from aws_s3_download_fix import s3_manager, create_s3_download, get_s3_download_status

logger = logging.getLogger(__name__)

# ë¼ìš°í„° ìƒì„±
router = APIRouter(prefix="/downloads", tags=["downloads"])

@router.post("/create/{job_id}")
async def create_download_link(
    job_id: str,
    format: str = "excel",  # excel, csv, json
    background_tasks: BackgroundTasks = None
):
    """
    ğŸš€ AWS S3 ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„± API
    Railway ë©”ëª¨ë¦¬ ì œí•œ ë¬¸ì œ í•´ê²° + ëŒ€ìš©ëŸ‰ íŒŒì¼ ì§€ì›
    """
    try:
        logger.info(f"ğŸ”— ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„± ìš”ì²­: {job_id} - {format}")
        
        # 1. DBì—ì„œ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
        from app.db.sqlite_service import SQLiteService
        db_service = SQLiteService()
        await db_service.init_database()
        
        # ì‘ì—… ì¡´ì¬ í™•ì¸
        job_data = await db_service.get_analysis_job(job_id)
        if not job_data:
            raise HTTPException(status_code=404, detail="ë¶„ì„ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if job_data["status"] != "completed":
            raise HTTPException(status_code=400, detail="ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ê²°ê³¼ ë°ì´í„° ì¡°íšŒ
        results = await db_service.get_analysis_results(job_id)
        if not results:
            raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # 2. DataFrame ìƒì„±
        result_list = []
        for result in results:
            try:
                result_data = result.get("result_data", {})
                if result_data:
                    result_list.append(result_data)
            except Exception as e:
                logger.warning(f"âš ï¸ ê²°ê³¼ ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        if not result_list:
            raise HTTPException(status_code=500, detail="ìœ íš¨í•œ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        df = pd.DataFrame(result_list)
        logger.info(f"ğŸ“Š DataFrame ìƒì„±: {df.shape}")
        
        # 3. S3 ì—…ë¡œë“œ ë° ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±
        upload_result = await create_s3_download(job_id, df, format)
        
        if not upload_result["success"]:
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {upload_result.get('error', 'Unknown error')}")
        
        # 4. ì‘ë‹µ ìƒì„±
        response = {
            "job_id": job_id,
            "download_ready": True,
            "download_method": upload_result["download_method"],
            "download_url": upload_result["download_url"],
            "filename": upload_result["filename"],
            "file_size_mb": upload_result["file_size_mb"],
            "format": format,
            "expires_at": upload_result["expires_at"],
            "record_count": len(result_list),
            "message": "ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
        # S3 ì—…ë¡œë“œì¸ ê²½ìš° ì¶”ê°€ ì •ë³´
        if upload_result["download_method"] == "s3":
            response["s3_key"] = upload_result.get("s3_key")
            response["note"] = "AWS S3ì— ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì–´ ëŒ€ìš©ëŸ‰ íŒŒì¼ë„ ë¹ ë¥´ê²Œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤"
        else:
            response["note"] = "ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"
        
        logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„± ì™„ë£Œ: {upload_result['download_method']}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.get("/status/{job_id}")
async def get_download_status(job_id: str):
    """ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸"""
    try:
        status = await get_s3_download_status(job_id)
        return status
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/file/{job_id}/{format}")
async def download_file_direct(job_id: str, format: str):
    """
    ë¡œì»¬ íŒŒì¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (S3 í´ë°±ìš©)
    """
    try:
        logger.info(f"ğŸ“¥ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ìš”ì²­: {job_id} - {format}")
        
        # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ í™•ì¸
        local_dir = "uploads/downloads"
        files = []
        
        if os.path.exists(local_dir):
            for filename in os.listdir(local_dir):
                if job_id[:8] in filename and filename.endswith(f".{format}"):
                    files.append(filename)
        
        if not files:
            raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
        files.sort(reverse=True)
        filepath = os.path.join(local_dir, files[0])
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(filepath):
            raise HTTPException(status_code=404, detail="íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        # MIME íƒ€ì… ì„¤ì •
        if format.lower() == "csv":
            media_type = "text/csv"
        elif format.lower() == "json":
            media_type = "application/json"
        else:  # excel
            media_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        
        logger.info(f"âœ… ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì œê³µ: {filepath}")
        
        return FileResponse(
            path=filepath,
            media_type=media_type,
            filename=files[0],
            headers={"Content-Disposition": f"attachment; filename={files[0]}"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/cleanup/{job_id}")
async def cleanup_download_files(job_id: str):
    """ë‹¤ìš´ë¡œë“œ íŒŒì¼ ìˆ˜ë™ ì •ë¦¬"""
    try:
        # S3 íŒŒì¼ ì‚­ì œ
        if s3_manager.s3_client:
            prefix = f"downloads/{job_id}/"
            response = s3_manager.s3_client.list_objects_v2(
                Bucket=s3_manager.bucket_name,
                Prefix=prefix
            )
            
            deleted_count = 0
            if 'Contents' in response:
                for obj in response['Contents']:
                    s3_manager.s3_client.delete_object(
                        Bucket=s3_manager.bucket_name,
                        Key=obj['Key']
                    )
                    deleted_count += 1
            
            logger.info(f"ğŸ—‘ï¸ S3 íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ")
        
        # ë¡œì»¬ íŒŒì¼ ì‚­ì œ
        local_dir = "uploads/downloads"
        local_deleted = 0
        
        if os.path.exists(local_dir):
            for filename in os.listdir(local_dir):
                if job_id[:8] in filename:
                    filepath = os.path.join(local_dir, filename)
                    os.remove(filepath)
                    local_deleted += 1
        
        logger.info(f"ğŸ—‘ï¸ ë¡œì»¬ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {local_deleted}ê°œ")
        
        return {
            "job_id": job_id,
            "cleaned_up": True,
            "s3_files_deleted": deleted_count if s3_manager.s3_client else 0,
            "local_files_deleted": local_deleted,
            "message": "ë‹¤ìš´ë¡œë“œ íŒŒì¼ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/health")
async def downloads_health_check():
    """ë‹¤ìš´ë¡œë“œ ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬"""
    try:
        # S3 ì—°ê²° í™•ì¸
        s3_status = "connected" if s3_manager.s3_client else "not_configured"
        
        # ë¡œì»¬ ë””ë ‰í† ë¦¬ í™•ì¸
        local_dir = "uploads/downloads"
        local_status = "available" if os.path.exists(local_dir) else "not_found"
        
        return {
            "status": "healthy",
            "s3_connection": s3_status,
            "local_storage": local_status,
            "aws_region": s3_manager.aws_region,
            "bucket_name": s3_manager.bucket_name,
            "max_file_size_mb": s3_manager.max_file_size_mb,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‘ì—…
async def background_cleanup():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë§Œë£Œëœ íŒŒì¼ ì •ë¦¬"""
    try:
        await s3_manager.cleanup_expired_files()
        logger.info("ğŸ§¹ ë°±ê·¸ë¼ìš´ë“œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì •ê¸°ì  ì •ë¦¬ ì‘ì—… ìŠ¤ì¼€ì¤„ë§ (FastAPI lifespanì—ì„œ í˜¸ì¶œ)
async def schedule_cleanup():
    """ì •ê¸°ì  ì •ë¦¬ ì‘ì—… ìŠ¤ì¼€ì¤„ë§"""
    while True:
        await asyncio.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤
        await background_cleanup()
