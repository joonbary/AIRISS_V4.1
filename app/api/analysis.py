# app/api/analysis.py
# AIRISS v4.0 Analysis API - ë¬´í•œ ë¡œë”© í•´ê²° ì™„ë£Œ ë²„ì „
# ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì•ˆì •í™” + ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import uuid
import asyncio
import logging
import traceback
import sys
from datetime import datetime
import pandas as pd
import numpy as np
import json
from app.db.db_service import db_service
from app.models.analysis import AnalysisRequest, AnalysisJob, AnalysisStatus, AnalysisResult
import inspect
from pathlib import Path
from functools import lru_cache

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ë¼ìš°í„° ìƒì„±
router = APIRouter(prefix="/analysis", tags=["analysis"])

# ğŸ”¥ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì €ì¥ìš©
_db_service = None
_ws_manager = None

def get_db_service():
    """Get database service - PostgreSQL only"""
    global _db_service
    if _db_service is None:
        try:
            from app.db.db_service import db_service
            _db_service = db_service
            logger.info("âœ… Database service ì´ˆê¸°í™” ì™„ë£Œ")
        except ImportError as e:
            logger.error(f"âŒ Database service import ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=503, detail="ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    return _db_service

def get_ws_manager():
    """WebSocket ë§¤ë‹ˆì € ê°€ì ¸ì˜¤ê¸°"""
    global _ws_manager
    if _ws_manager is None:
        try:
            from app.core.websocket_manager import ConnectionManager
            _ws_manager = ConnectionManager()
            logger.info("âœ… WebSocket manager ì´ˆê¸°í™” ì™„ë£Œ")
        except ImportError as e:
            logger.error(f"âŒ WebSocket manager import ì‹¤íŒ¨: {e}")
            # WebSocketì´ ì—†ì–´ë„ ë¶„ì„ì€ ê°€ëŠ¥í•˜ë„ë¡ None ë°˜í™˜
            return None
    return _ws_manager

# ğŸ”¥ ì´ˆê¸°í™” í•¨ìˆ˜ (main.pyì—ì„œ í˜¸ì¶œ)
def init_services(ws_manager=None):
    """ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” (ws_managerë§Œ)"""
    global _ws_manager
    _ws_manager = ws_manager
    logger.info("âœ… Analysis ëª¨ë“ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (ws_manager)")

# ì„œë¹„ìŠ¤ì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ê¸° import
try:
    from app.services import HybridAnalyzer
    hybrid_analyzer = HybridAnalyzer()
    logger.info("âœ… ì„œë¹„ìŠ¤ HybridAnalyzer ë¡œë“œ ì„±ê³µ")
except ImportError:
    logger.warning("âš ï¸ ì„œë¹„ìŠ¤ HybridAnalyzer ë¡œë“œ ì‹¤íŒ¨, ë¡œì»¬ ì •ì˜ ì‚¬ìš©")
    hybrid_analyzer = None

# ğŸ†• v3.0 AIRISS 8ëŒ€ ì˜ì—­ ì™„ì „ ë³´ì¡´
# ğŸ”¥ ê¸°ì¡´ AIRISS_FRAMEWORK ë¶€ë¶„ì„ ì°¾ì•„ì„œ ì•„ë˜ ì½”ë“œë¡œ ì™„ì „ êµì²´í•˜ì„¸ìš”
# íŒŒì¼: app/api/analysis.py (ì•½ 25~140ë²ˆì§¸ ì¤„)

# ğŸ†• ì°©ìˆ˜ë³´ê³ ì„œ ì™„ì „ ë°˜ì˜ - AIRISS 8ëŒ€ ì˜ì—­ 
AIRISS_FRAMEWORK = {
    "ì—…ë¬´ì„±ê³¼": {
        "keywords": {
            "positive": [
                "ìš°ìˆ˜", "íƒì›”", "ë›°ì–´ë‚¨", "ì„±ê³¼", "ë‹¬ì„±", "ì™„ë£Œ", "ì„±ê³µ", "íš¨ìœ¨", "ìƒì‚°ì ", 
                "ëª©í‘œë‹¬ì„±", "ì´ˆê³¼ë‹¬ì„±", "í’ˆì§ˆ", "ì •í™•", "ì‹ ì†", "ì™„ë²½", "ì „ë¬¸ì ", "ì²´ê³„ì ",
                "ì„±ê³¼ê°€", "ê²°ê³¼ë¥¼", "ì‹¤ì ì´", "ì™„ì„±ë„", "ë§Œì¡±ë„", "ì‚°ì¶œë¬¼", "ì•„ì›ƒí’‹",
                "ì—…ë¬´ì™„ë£Œ", "í”„ë¡œì íŠ¸", "ë³´ê³ ì„œ", "ê²°ê³¼ë¬¼", "deliverable"
            ],
            "negative": [
                "ë¶€ì¡±", "ë¯¸í¡", "ì§€ì—°", "ì‹¤íŒ¨", "ë¬¸ì œ", "ì˜¤ë¥˜", "ëŠ¦ìŒ", "ë¹„íš¨ìœ¨", 
                "ëª©í‘œë¯¸ë‹¬", "í’ˆì§ˆì €í•˜", "ë¶€ì •í™•", "ë¯¸ì™„ì„±", "ë¶€ì‹¤", "ê°œì„ ", "ë³´ì™„",
                "ì§€ì²´", "ì‚°ì¶œë¬¼ë¶€ì¡±", "ê²°ê³¼ë¬¼ë¬¸ì œ"
            ]
        },
        "weight": 0.20,  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ: ì—…ë¬´ ì‚°ì¶œë¬¼ 20%
        "description": "ì—…ë¬´ ì‚°ì¶œë¬¼ì˜ ì–‘ê³¼ ì§ˆì  ìˆ˜ì¤€",
        "color": "#FF5722",
        "icon": "ğŸ“Š"
    },
    "KPIë‹¬ì„±": {
        "keywords": {
            "positive": [
                "KPIë‹¬ì„±", "ì§€í‘œë‹¬ì„±", "ëª©í‘œì´ˆê³¼", "ì„±ê³¼ìš°ìˆ˜", "ì‹¤ì ìš°ìˆ˜", "ë§¤ì¶œì¦ê°€", 
                "íš¨ìœ¨í–¥ìƒ", "ìƒì‚°ì„±í–¥ìƒ", "ìˆ˜ì¹˜ë‹¬ì„±", "ì„±ì¥", "ê°œì„ ", "ë‹¬ì„±ë¥ ", "ì´ˆê³¼",
                "ROA", "ROE", "ìˆ˜ìµë¥ ", "ì—¬ì‹ ", "ê³ ê°ë§Œì¡±ë„", "ì—°ì²´ìœ¨ê°œì„ ", "ë¦¬ìŠ¤í¬ê´€ë¦¬",
                "í•µì‹¬ì§€í‘œ", "ì •ëŸ‰ëª©í‘œ", "ìˆ˜ì¹˜ëª©í‘œ", "ì‹¤ì ì´ˆê³¼", "ì§€í‘œê°œì„ "
            ],
            "negative": [
                "KPIë¯¸ë‹¬", "ëª©í‘œë¯¸ë‹¬", "ì‹¤ì ë¶€ì§„", "ë§¤ì¶œê°ì†Œ", "íš¨ìœ¨ì €í•˜", 
                "ìƒì‚°ì„±ì €í•˜", "ìˆ˜ì¹˜ë¶€ì¡±", "í•˜ë½", "í‡´ë³´", "ë¯¸ë‹¬", "ì†ì‹¤",
                "ì§€í‘œì•…í™”", "ëª©í‘œë¶€ì¡±", "ì‹¤ì ì €ì¡°"
            ]
        },
        "weight": 0.30,  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ: KPI 30%
        "description": "í•µì‹¬ì„±ê³¼ì§€í‘œ ë‹¬ì„±ë„ ë° ì •ëŸ‰ì  ê¸°ì—¬",
        "color": "#4A4A4A",
        "icon": "ğŸ¯"
    },
    "íƒœë„ë§ˆì¸ë“œì…‹": {
        "keywords": {
            "positive": [
                "ì ê·¹ì ", "ê¸ì •ì ", "ì—´ì •", "ì„±ì‹¤", "ì±…ì„ê°", "ì§„ì·¨ì ", "í˜‘ì¡°ì ", 
                "ì„±ì¥ì§€í–¥", "í•™ìŠµì˜ì§€", "ë„ì „ì •ì‹ ", "ì£¼ì¸ì˜ì‹", "í—Œì‹ ", "ì—´ì‹¬íˆ", "ë…¸ë ¥",
                "ë³€í™”ìˆ˜ìš©", "íšŒë³µíƒ„ë ¥ì„±", "ê·¼ë¬´íƒœë„", "ì§„ì •ì„±", "ìì„¸", "ë§ˆì¸ë“œì…‹",
                "ì˜ìš•", "ë™ê¸°", "ëª°ì…", "ì§‘ì¤‘", "ì„±ì‹¤ì„±", "ê·¼ë©´", "ë¶€ì§€ëŸ°í•¨"
            ],
            "negative": [
                "ì†Œê·¹ì ", "ë¶€ì •ì ", "ë¬´ê´€ì‹¬", "ë¶ˆì„±ì‹¤", "íšŒí”¼", "ëƒ‰ì†Œì ", 
                "ë¹„í˜‘ì¡°ì ", "ì•ˆì£¼", "í˜„ìƒìœ ì§€", "ìˆ˜ë™ì ", "íƒœë„ë¬¸ì œ", "ë§ˆì¸ë“œë¶€ì¡±",
                "ë³€í™”ê±°ë¶€", "ì˜ìš•ì—†ìŒ", "ë¬´ê¸°ë ¥", "ëƒ‰ë‹´", "ë¶ˆë§Œ"
            ]
        },
        "weight": 0.10,  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ: íƒœë„ ë° ë§ˆì¸ë“œì…‹ 10%
        "description": "ì¼ì— ëŒ€í•œ íƒœë„ì™€ ë³€í™” ìˆ˜ìš© ë§ˆì¸ë“œì…‹",
        "color": "#F89C26",
        "icon": "ğŸ§ "
    },
    "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì—­ëŸ‰": {
        "keywords": {
            "positive": [
                "ëª…í™•", "ì •í™•", "ì‹ ì†", "ì¹œì ˆ", "ê²½ì²­", "ì†Œí†µ", "ì „ë‹¬", "ì´í•´", 
                "ì„¤ë“", "í˜‘ì˜", "ì¡°ìœ¨", "ê³µìœ ", "íˆ¬ëª…", "ê°œë°©ì ", "ì˜ì‚¬ì†Œí†µ", "ì›í™œ",
                "ì‘ë‹µì†ë„", "ëª…í™•ì„±", "í†¤", "ì˜í–¥ë ¥", "ê³ ê°ì†Œí†µ", "ë‚´ë¶€ì†Œí†µ",
                "ëŒ€í™”", "ì§ˆë¬¸", "ë‹µë³€", "í”¼ë“œë°±", "ë³´ê³ ", "ë°œí‘œ", "ì„¤ëª…"
            ],
            "negative": [
                "ë¶ˆëª…í™•", "ì§€ì—°", "ë¬´ì‹œ", "ì˜¤í•´", "ë‹¨ì ˆ", "ì¹¨ë¬µ", "íšŒí”¼", 
                "ë…ë‹¨", "ì¼ë°©ì ", "íì‡„ì ", "ì†Œí†µë¶€ì¡±", "ì „ë‹¬ë ¥ë¶€ì¡±", "ì‘ë‹µì§€ì—°",
                "ì†Œí†µë¬¸ì œ", "ì˜ì‚¬ì „ë‹¬", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ë¶€ì¡±"
            ]
        },
        "weight": 0.10,  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ: ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì—­ëŸ‰ 10%
        "description": "ì˜ì‚¬ì†Œí†µ íš¨ê³¼ì„±ê³¼ ê´€ê³„í˜•ì„± ëŠ¥ë ¥",
        "color": "#B3B3B3",
        "icon": "ğŸ’¬"
    },
    "ë¦¬ë”ì‹­í˜‘ì—…ì—­ëŸ‰": {
        "keywords": {
            "positive": [
                "ë¦¬ë”ì‹­", "íŒ€ì›Œí¬", "í˜‘ì—…", "ì§€ì›", "ë©˜í† ë§", "ë™ê¸°ë¶€ì—¬", "ì¡°ìœ¨", 
                "í™”í•©", "íŒ€ë¹Œë”©", "ìœ„ì„", "ì½”ì¹­", "ì˜í–¥ë ¥", "í˜‘ë ¥", "íŒ€í”Œë ˆì´",
                "íŒ€ì„±ê³¼", "ë¶€í•˜ì§ì›", "ë™ë£Œì§€ì›", "ê°ˆë“±í•´ê²°", "í•©ì˜ë„ì¶œ",
                "ê³µë™ì‘ì—…", "í˜‘ì¡°", "ì¡°í™”", "ì‹œë„ˆì§€", "ìƒí˜¸ë³´ì™„"
            ],
            "negative": [
                "ë…ë‹¨", "ê°ˆë“±", "ë¹„í˜‘ì¡°", "ì†Œì™¸", "ë¶„ì—´", "ëŒ€ë¦½", "ì´ê¸°ì£¼ì˜", 
                "ë°©í•´", "ë¬´ê´€ì‹¬", "ê³ ë¦½", "ê°œì¸ì£¼ì˜", "íŒ€ì›Œí¬ë¶€ì¡±",
                "í˜‘ì—…ë¬¸ì œ", "ë¦¬ë”ì‹­ë¶€ì¡±", "íŒ€í™”í•©ì €í•´"
            ]
        },
        "weight": 0.10,  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ: ë¦¬ë”ì‹­ & í˜‘ì—… ì—­ëŸ‰ 10%
        "description": "ë¦¬ë”ì‹­ ë°œíœ˜ì™€ í˜‘ì—… ì´‰ì§„ ëŠ¥ë ¥",
        "color": "#FF8A50",
        "icon": "ğŸ‘¥"
    },
    "ì§€ì‹ì „ë¬¸ì„±": {
        "keywords": {
            "positive": [
                "ì „ë¬¸", "ìˆ™ë ¨", "ê¸°ìˆ ", "ì§€ì‹", "í•™ìŠµ", "ë°œì „", "ì—­ëŸ‰", "ëŠ¥ë ¥", 
                "ì„±ì¥", "í–¥ìƒ", "ìŠµë“", "ê°œë°œ", "ì „ë¬¸ì„±", "ë…¸í•˜ìš°", "ìŠ¤í‚¬", "ê²½í—˜",
                "ìê²©ì¦", "êµìœ¡", "ì—°ìˆ˜", "AIì—­ëŸ‰", "ë””ì§€í„¸ì—­ëŸ‰", "ê¸ˆìœµì „ë¬¸ì„±",
                "ì „ë¬¸ì§€ì‹", "ê¸°ìˆ ë ¥", "ì‹¤ë ¥", "ìˆ™ë ¨ë„", "ì „ë¬¸ë¶„ì•¼", "ê¹Šì´"
            ],
            "negative": [
                "ë¯¸ìˆ™", "ë¶€ì¡±", "ë‚™í›„", "ë¬´ì§€", "ì •ì²´", "í‡´ë³´", "ë¬´ëŠ¥ë ¥", 
                "ê¸°ì´ˆë¶€ì¡±", "ì—­ëŸ‰ë¶€ì¡±", "ì‹¤ë ¥ë¶€ì¡±", "í•™ìŠµê±°ë¶€", "ì§€ì‹ë¶€ì¡±",
                "ì „ë¬¸ì„±ë¶€ì¡±", "ê¸°ìˆ ë¶€ì¡±", "ê²½í—˜ë¶€ì¡±"
            ]
        },
        "weight": 0.10,  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ: ì§€ì‹ & ì „ë¬¸ì„± 10%
        "description": "ì§ë¬´ ì „ë¬¸ì„±ê³¼ ì§€ì† í•™ìŠµ ëŠ¥ë ¥",
        "color": "#6A6A6A",
        "icon": "ğŸ“š"
    },
    "ë¼ì´í”„ìŠ¤íƒ€ì¼ê±´ê°•": {  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ ì™„ì „ ë°˜ì˜ (ê¸°ì¡´ "ì°½ì˜í˜ì‹ " ëŒ€ì²´)
        "keywords": {
            "positive": [
                "ê±´ê°•", "í™œë ¥", "ì—ë„ˆì§€", "ì›Œë¼ë°¸", "ê· í˜•", "ì›°ë¹™", "ìš´ë™", "ëª…ìƒ",
                "ìŠ¤íŠ¸ë ˆìŠ¤ê´€ë¦¬", "ìˆ˜ë©´", "í™œê¸°", "ì»¨ë””ì…˜", "ì²´ë ¥", "ëª°ì…",
                "ì›°ë¹™í”„ë¡œê·¸ë¨", "ê±´ê°•ê´€ë¦¬", "ìƒí™œìŠµê´€", "ì •ì‹ ê±´ê°•", "ì²´ë ¥ê´€ë¦¬",
                "ë°¸ëŸ°ìŠ¤", "íœ´ì‹", "ì¬ì¶©ì „", "í™œë™ì ", "ê±´ê°•ìƒíƒœ"
            ],
            "negative": [
                "í”¼ë¡œ", "ìŠ¤íŠ¸ë ˆìŠ¤", "ë²ˆì•„ì›ƒ", "ê³¼ë¡œ", "ë¶ˆê· í˜•", "ê±´ê°•ì•…í™”",
                "ë³‘ê°€", "ê²°ê·¼", "ì»¨ë””ì…˜ë‚œì¡°", "ì§‘ì¤‘ë ¥ì €í•˜", "ë¬´ê¸°ë ¥", "ì†Œì§„",
                "ë¶ˆê±´ê°•", "ì²´ë ¥ì €í•˜", "ìŠ¤íŠ¸ë ˆìŠ¤ê³¼ë‹¤", "ê³¼ë¡œëˆ„ì "
            ]
        },
        "weight": 0.05,  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ: ë¼ì´í”„ìŠ¤íƒ€ì¼ & ê±´ê°• 5%
        "description": "ì—…ë¬´ ì§€ì†ì„±ê³¼ ëª°ì…ë„ì— ì˜í–¥í•˜ëŠ” ê±´ê°•ê³¼ ì›°ë¹™",
        "color": "#4CAF50",
        "icon": "ğŸ’ª"
    },
    "ìœ¤ë¦¬ì‚¬ì™¸í–‰ë™": {  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ ì™„ì „ ë°˜ì˜ (ê¸°ì¡´ "ì¡°ì§ì ì‘" ëŒ€ì²´)
        "keywords": {
            "positive": [
                "ìœ¤ë¦¬", "ì‹ ë¢°", "ì„±ì‹¤", "ì •ì§", "íˆ¬ëª…", "ì¤€ë²•", "ê·œì •ì¤€ìˆ˜", "ì²­ë ´",
                "ë´‰ì‚¬", "ì‚¬íšŒê³µí—Œ", "ì§€ì—­ì‚¬íšŒ", "CSR", "ëª¨ë²”", "í’ˆìœ„", "í’ˆê²©",
                "ì„ì§ì›ìœ¤ë¦¬ê°•ë ¹", "ì»´í”Œë¼ì´ì–¸ìŠ¤", "ë¦¬ìŠ¤í¬ê´€ë¦¬", "í‰íŒ", "ì‹ ë¢°ì„±",
                "ë„ë•ì ", "ì–‘ì‹¬ì ", "ì±…ì„ê°", "ì‚¬íšŒì ì±…ì„"
            ],
            "negative": [
                "ìœ„ë°˜", "ë¹„ìœ¤ë¦¬", "ë¶ˆë²•", "ë¶€ì •", "ìŠ¤ìº”ë“¤", "ë…¼ë€", "ë¬¸ì œí–‰ë™",
                "ê·œì •ìœ„ë°˜", "ë¦¬ìŠ¤í¬", "í‰íŒì†ìƒ", "ì‹ ë¢°ì‹¤ì¶”", "ìœ„ë²•í–‰ìœ„",
                "SNSë…¼ë€", "í˜ì˜¤ë°œì–¸", "ë¶€ì ì ˆí–‰ë™", "ìœ¤ë¦¬ë¬¸ì œ", "ë„ë•ì í•´ì´",
                "ì‚¬íšŒì ë¬¼ì˜", "ë¹„ë¦¬", "ë¶€íŒ¨"
            ]
        },
        "weight": 0.05,  # ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ: ì‚¬ì™¸ í–‰ë™ ë° ìœ¤ë¦¬ 5%
        "description": "ì¡°ì§ ì‹ ë¢°ë„ì™€ í‰íŒì— ì˜í–¥í•˜ëŠ” ìœ¤ë¦¬ì„±ê³¼ ì‚¬ì™¸í–‰ë™",
        "color": "#9E9E9E",
        "icon": "âš–ï¸"
    }
}

# ğŸ”¥ ì°©ìˆ˜ë³´ê³ ì„œ ê°€ì¤‘ì¹˜ ê²€ì¦
OFFICIAL_WEIGHTS = {
    "ì—…ë¬´ì„±ê³¼": 0.20,
    "KPIë‹¬ì„±": 0.30,
    "íƒœë„ë§ˆì¸ë“œì…‹": 0.10,
    "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì—­ëŸ‰": 0.10,
    "ë¦¬ë”ì‹­í˜‘ì—…ì—­ëŸ‰": 0.10,
    "ì§€ì‹ì „ë¬¸ì„±": 0.10,
    "ë¼ì´í”„ìŠ¤íƒ€ì¼ê±´ê°•": 0.05,
    "ìœ¤ë¦¬ì‚¬ì™¸í–‰ë™": 0.05
}

# ê°€ì¤‘ì¹˜ í•©ê³„ ê²€ì¦ (ë°˜ë“œì‹œ 1.0ì´ì–´ì•¼ í•¨)
total_weight = sum(OFFICIAL_WEIGHTS.values())
assert total_weight == 1.0, f"âŒ ê°€ì¤‘ì¹˜ í•©ê³„ ì˜¤ë¥˜: {total_weight}"
logger.info(f"âœ… ì°©ìˆ˜ë³´ê³ ì„œ ê¸°ì¤€ ê°€ì¤‘ì¹˜ ê²€ì¦ ì™„ë£Œ: ì´í•© {total_weight}")

# ğŸ†• ì •ëŸ‰ë°ì´í„° ë¶„ì„ê¸° (v3.0 ì™„ì „ ë³´ì¡´)
class QuantitativeAnalyzer:
    """í‰ê°€ë“±ê¸‰, ì ìˆ˜ ë“± ì •ëŸ‰ë°ì´í„° ë¶„ì„ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.grade_mappings = self.setup_grade_mappings()
        self.score_weights = self.setup_score_weights()
        logger.info("âœ… ì •ëŸ‰ë°ì´í„° ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def setup_grade_mappings(self) -> Dict[str, Dict]:
        """ë‹¤ì–‘í•œ í‰ê°€ë“±ê¸‰ í˜•ì‹ì„ ì ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ë§¤í•‘ í…Œì´ë¸”"""
        return {
            # 5ë‹¨ê³„ ë“±ê¸‰
            'S': 100, 'A': 85, 'B': 70, 'C': 55, 'D': 40,
            
            # ì˜ë¬¸ ë“±ê¸‰  
            'A+': 100, 'A': 95, 'A-': 90,
            'B+': 85, 'B': 80, 'B-': 75,
            'C+': 70, 'C': 65, 'C-': 60,
            'D+': 55, 'D': 50, 'D-': 45,
            'F': 30,
            
            # ìˆ«ì ë“±ê¸‰
            '1': 100, '2': 80, '3': 60, '4': 40, '5': 20,
            '1ê¸‰': 100, '2ê¸‰': 80, '3ê¸‰': 60, '4ê¸‰': 40, '5ê¸‰': 20,
            
            # í•œê¸€ ë“±ê¸‰
            'ìš°ìˆ˜': 90, 'ì–‘í˜¸': 75, 'ë³´í†µ': 60, 'ë¯¸í¡': 45, 'ë¶€ì¡±': 30,
            'ìµœìš°ìˆ˜': 100, 'ìƒ': 85, 'ì¤‘': 65, 'í•˜': 45,
            
            # ë°±ë¶„ìœ„/í¼ì„¼íŠ¸
            'ìƒìœ„10%': 95, 'ìƒìœ„20%': 85, 'ìƒìœ„30%': 75, 
            'ìƒìœ„50%': 65, 'í•˜ìœ„50%': 50, 'í•˜ìœ„30%': 35,
            
            # OKê¸ˆìœµê·¸ë£¹ ë§ì¶¤ ë“±ê¸‰
            'OKâ˜…â˜…â˜…': 100, 'OKâ˜…â˜…': 90, 'OKâ˜…': 80, 
            'OK A': 75, 'OK B+': 70, 'OK B': 65, 'OK C': 55, 'OK D': 40
        }
    
    def setup_score_weights(self) -> Dict[str, float]:
        """ì •ëŸ‰ ë°ì´í„° í•­ëª©ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •"""
        return {
            'performance_grade': 0.30,
            'kpi_score': 0.25,
            'competency_grade': 0.20,
            'attendance_score': 0.10,
            'training_score': 0.10,
            'certificate_score': 0.05
        }
    
    def extract_quantitative_data(self, row: pd.Series) -> Dict[str, Any]:
        """í–‰ ë°ì´í„°ì—ì„œ ì •ëŸ‰ì  ìš”ì†Œ ì¶”ì¶œ"""
        quant_data = {}
        
        for col_name, value in row.items():
            col_lower = str(col_name).lower()
            
            if any(keyword in col_lower for keyword in ['ì ìˆ˜', 'score', 'í‰ì ', 'rating']):
                quant_data[f'score_{col_name}'] = self.normalize_score(value)
            elif any(keyword in col_lower for keyword in ['ë“±ê¸‰', 'grade', 'í‰ê°€', 'level']):
                quant_data[f'grade_{col_name}'] = self.convert_grade_to_score(value)
            elif any(keyword in col_lower for keyword in ['ë‹¬ì„±ë¥ ', 'ë¹„ìœ¨', 'rate', '%', 'percent']):
                quant_data[f'rate_{col_name}'] = self.normalize_percentage(value)
            elif any(keyword in col_lower for keyword in ['íšŸìˆ˜', 'ê±´ìˆ˜', 'count', 'íšŒ', 'ë²ˆ']):
                quant_data[f'count_{col_name}'] = self.normalize_count(value)
                
        return quant_data
    
    def convert_grade_to_score(self, grade_value) -> float:
        """ë“±ê¸‰ì„ ì ìˆ˜ë¡œ ë³€í™˜"""
        if pd.isna(grade_value) or grade_value == '':
            return 50.0
        
        grade_str = str(grade_value).strip().upper()
        
        if grade_str in self.grade_mappings:
            return float(self.grade_mappings[grade_str])
        
        try:
            score = float(grade_str)
            if 0 <= score <= 100:
                return score
            elif 0 <= score <= 5:
                return (score - 1) * 25
            elif 0 <= score <= 10:
                return score * 10
        except ValueError:
            pass
        
        if 'ìš°ìˆ˜' in grade_str or 'excellent' in grade_str.lower():
            return 90.0
        elif 'ì–‘í˜¸' in grade_str or 'good' in grade_str.lower():
            return 75.0
        elif 'ë³´í†µ' in grade_str or 'average' in grade_str.lower():
            return 60.0
        elif 'ë¯¸í¡' in grade_str or 'poor' in grade_str.lower():
            return 45.0
        
        return 50.0
    
    def normalize_score(self, score_value) -> float:
        """ì ìˆ˜ ê°’ ì •ê·œí™” (0-100 ë²”ìœ„ë¡œ)"""
        if pd.isna(score_value) or score_value == '':
            return 50.0
        
        try:
            score = float(str(score_value).replace('%', '').replace('ì ', ''))
            
            if 0 <= score <= 1:
                return score * 100
            elif 0 <= score <= 5:
                return (score - 1) * 25
            elif 0 <= score <= 10:
                return score * 10
            elif 0 <= score <= 100:
                return score
            else:
                return max(0, min(100, score))
                
        except (ValueError, TypeError):
            return 50.0
    
    def normalize_percentage(self, percent_value) -> float:
        """ë°±ë¶„ìœ¨ ì •ê·œí™”"""
        if pd.isna(percent_value) or percent_value == '':
            return 50.0
        
        try:
            percent_str = str(percent_value).replace('%', '').replace('í¼ì„¼íŠ¸', '')
            percent = float(percent_str)
            
            if 0 <= percent <= 1:
                return percent * 100
            elif 0 <= percent <= 100:
                return percent
            else:
                return max(0, min(100, percent))
                
        except (ValueError, TypeError):
            return 50.0
    
    def normalize_count(self, count_value) -> float:
        """íšŸìˆ˜/ê±´ìˆ˜ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜"""
        if pd.isna(count_value) or count_value == '':
            return 50.0
        
        try:
            count = float(str(count_value).replace('íšŒ', '').replace('ê±´', '').replace('ë²ˆ', ''))
            
            if count <= 0:
                return 30.0
            elif count <= 2:
                return 50.0
            elif count <= 5:
                return 70.0
            elif count <= 10:
                return 85.0
            else:
                return 95.0
                
        except (ValueError, TypeError):
            return 50.0
    
    def calculate_quantitative_score(self, quant_data: Dict[str, float]) -> Dict[str, Any]:
        """ì •ëŸ‰ ë°ì´í„°ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì ìˆ˜ ê³„ì‚°"""
        if not quant_data:
            return {
                "quantitative_score": 50.0,
                "confidence": 0.0,
                "contributing_factors": {},
                "data_quality": "ì—†ìŒ"
            }
        
        total_score = 0.0
        total_weight = 0.0
        contributing_factors = {}
        
        for data_key, score in quant_data.items():
            if 'grade_' in data_key:
                weight = 0.4
            elif 'score_' in data_key:
                weight = 0.3
            elif 'rate_' in data_key:
                weight = 0.2
            else:
                weight = 0.1
            
            total_score += score * weight
            total_weight += weight
            contributing_factors[data_key] = {
                "score": round(score, 1),
                "weight": weight,
                "contribution": round(score * weight, 1)
            }
        
        if total_weight > 0:
            final_score = total_score / total_weight
            confidence = min(total_weight * 20, 100)
        else:
            final_score = 50.0
            confidence = 0.0
        
        data_count = len(quant_data)
        if data_count >= 5:
            data_quality = "ë†’ìŒ"
        elif data_count >= 3:
            data_quality = "ì¤‘ê°„"
        elif data_count >= 1:
            data_quality = "ë‚®ìŒ"
        else:
            data_quality = "ì—†ìŒ"
        
        return {
            "quantitative_score": round(final_score, 1),
            "confidence": round(confidence, 1),
            "contributing_factors": contributing_factors,
            "data_quality": data_quality,
            "data_count": data_count
        }

# ğŸ†• í…ìŠ¤íŠ¸ ë¶„ì„ê¸° (v3.0 ì™„ì „ ë³´ì¡´)
class AIRISSTextAnalyzer:
    def __init__(self):
        self.framework = AIRISS_FRAMEWORK
        self.openai_available = False
        self.openai = None
        try:
            import openai
            self.openai = openai
            self.openai_available = True
            logger.info("âœ… OpenAI ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
        except ImportError:
            logger.warning("âš ï¸ OpenAI ëª¨ë“ˆ ì—†ìŒ - í‚¤ì›Œë“œ ë¶„ì„ë§Œ ê°€ëŠ¥")
    
    def analyze_text(self, text: str, dimension: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ë¶„ì„í•˜ì—¬ ì ìˆ˜ ì‚°ì¶œ"""
        if not text or text.lower() in ['nan', 'null', '', 'none']:
            return {"score": 50, "confidence": 0, "signals": {"positive": 0, "negative": 0, "positive_words": [], "negative_words": []}}
        
        keywords = self.framework[dimension]["keywords"]
        text_lower = text.lower()
        
        positive_matches = []
        negative_matches = []
        
        for word in keywords["positive"]:
            if word in text_lower:
                positive_matches.append(word)
        
        for word in keywords["negative"]:
            if word in text_lower:
                negative_matches.append(word)
        
        positive_count = len(positive_matches)
        negative_count = len(negative_matches)
        
        base_score = 50
        positive_boost = min(positive_count * 8, 45)
        negative_penalty = min(negative_count * 10, 40)
        
        text_length = len(text)
        if text_length > 50:
            length_bonus = min((text_length - 50) / 100 * 5, 10)
        else:
            length_bonus = 0
        
        final_score = base_score + positive_boost - negative_penalty + length_bonus
        final_score = max(10, min(100, final_score))
        
        total_signals = positive_count + negative_count
        base_confidence = min(total_signals * 12, 80)
        length_confidence = min(text_length / 20, 20)
        confidence = min(base_confidence + length_confidence, 100)
        
        return {
            "score": round(final_score, 1),
            "confidence": round(confidence, 1),
            "signals": {
                "positive": positive_count,
                "negative": negative_count,
                "positive_words": positive_matches[:5],
                "negative_words": negative_matches[:5]
            }
        }
    
    def calculate_overall_score(self, dimension_scores: Dict[str, float]) -> Dict[str, Any]:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        weighted_sum = 0
        total_weight = 0
        
        for dimension, score in dimension_scores.items():
            if dimension in self.framework:
                weight = self.framework[dimension]["weight"]
                weighted_sum += score * weight
                total_weight += weight
        
        overall_score = weighted_sum / total_weight if total_weight > 0 else 50
        
        if overall_score >= 95:
            grade = "OKâ˜…â˜…â˜…"
            grade_desc = "ìµœìš°ìˆ˜ ë“±ê¸‰ (TOP 1%)"
            percentile = "ìƒìœ„ 1%"
        elif overall_score >= 90:
            grade = "OKâ˜…â˜…"
            grade_desc = "ìš°ìˆ˜ ë“±ê¸‰ (TOP 5%)"
            percentile = "ìƒìœ„ 5%"
        elif overall_score >= 85:
            grade = "OKâ˜…"
            grade_desc = "ìš°ìˆ˜+ ë“±ê¸‰ (TOP 10%)"
            percentile = "ìƒìœ„ 10%"
        elif overall_score >= 80:
            grade = "OK A"
            grade_desc = "ì–‘í˜¸ ë“±ê¸‰ (TOP 20%)"
            percentile = "ìƒìœ„ 20%"
        elif overall_score >= 75:
            grade = "OK B+"
            grade_desc = "ì–‘í˜¸- ë“±ê¸‰ (TOP 30%)"
            percentile = "ìƒìœ„ 30%"
        elif overall_score >= 70:
            grade = "OK B"
            grade_desc = "ë³´í†µ ë“±ê¸‰ (TOP 40%)"
            percentile = "ìƒìœ„ 40%"
        elif overall_score >= 60:
            grade = "OK C"
            grade_desc = "ê°œì„ í•„ìš” ë“±ê¸‰ (TOP 60%)"
            percentile = "ìƒìœ„ 60%"
        else:
            grade = "OK D"
            grade_desc = "ì§‘ì¤‘ê°œì„  ë“±ê¸‰ (í•˜ìœ„ 40%)"
            percentile = "í•˜ìœ„ 40%"
        
        return {
            "overall_score": round(overall_score, 1),
            "grade": grade,
            "grade_description": grade_desc,
            "percentile": percentile,
            "weighted_scores": dimension_scores
        }

# ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ê¸° (v3.0 ì™„ì „ ë³´ì¡´)
class AIRISSHybridAnalyzer:
    """í…ìŠ¤íŠ¸ ë¶„ì„ + ì •ëŸ‰ ë¶„ì„ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.text_analyzer = AIRISSTextAnalyzer()
        self.quantitative_analyzer = QuantitativeAnalyzer()
        
        self.hybrid_weights = {
            'text_analysis': 0.6,
            'quantitative_analysis': 0.4
        }
        
        logger.info("âœ… AIRISS v4.0 í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def comprehensive_analysis(self, uid: str, opinion: str, row_data: pd.Series) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„: í…ìŠ¤íŠ¸ + ì •ëŸ‰ ë°ì´í„°"""
        
        # 1. í…ìŠ¤íŠ¸ ë¶„ì„
        text_results = {}
        for dimension in AIRISS_FRAMEWORK.keys():
            text_results[dimension] = self.text_analyzer.analyze_text(opinion, dimension)
        
        text_overall = self.text_analyzer.calculate_overall_score(
            {dim: result["score"] for dim, result in text_results.items()}
        )
        
        # 2. ì •ëŸ‰ ë°ì´í„° ë¶„ì„
        quant_data = self.quantitative_analyzer.extract_quantitative_data(row_data)
        quant_results = self.quantitative_analyzer.calculate_quantitative_score(quant_data)
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        text_weight = self.hybrid_weights['text_analysis']
        quant_weight = self.hybrid_weights['quantitative_analysis']
        
        if quant_results["data_quality"] == "ì—†ìŒ":
            text_weight = 0.8
            quant_weight = 0.2
        elif quant_results["data_quality"] == "ë‚®ìŒ":
            text_weight = 0.7
            quant_weight = 0.3
        
        hybrid_score = (text_overall["overall_score"] * text_weight + 
                       quant_results["quantitative_score"] * quant_weight)
        
        # 4. í†µí•© ì‹ ë¢°ë„ ê³„ì‚°
        hybrid_confidence = (text_overall.get("confidence", 70) * text_weight + 
                           quant_results["confidence"] * quant_weight)
        
        # 5. í•˜ì´ë¸Œë¦¬ë“œ ë“±ê¸‰ ì‚°ì •
        hybrid_grade_info = self.calculate_hybrid_grade(hybrid_score)
        
        return {
            "text_analysis": {
                "overall_score": text_overall["overall_score"],
                "grade": text_overall["grade"],
                "dimension_scores": {dim: result["score"] for dim, result in text_results.items()},
                "dimension_details": text_results
            },
            "quantitative_analysis": quant_results,
            "hybrid_analysis": {
                "overall_score": round(hybrid_score, 1),
                "grade": hybrid_grade_info["grade"],
                "grade_description": hybrid_grade_info["grade_description"],
                "percentile": hybrid_grade_info["percentile"],
                "confidence": round(hybrid_confidence, 1),
                "analysis_composition": {
                    "text_weight": round(text_weight * 100, 1),
                    "quantitative_weight": round(quant_weight * 100, 1)
                }
            },
            "analysis_metadata": {
                "uid": uid,
                "analysis_version": "AIRISS v4.0",
                "analysis_timestamp": datetime.now().isoformat(),
                "data_sources": {
                    "text_available": bool(opinion and opinion.strip()),
                    "quantitative_available": bool(quant_data),
                    "quantitative_data_quality": quant_results["data_quality"]
                }
            }
        }
    
    def calculate_hybrid_grade(self, score: float) -> Dict[str, str]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¥¼ OKë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 95:
            return {
                "grade": "OKâ˜…â˜…â˜…",
                "grade_description": "ìµœìš°ìˆ˜ ë“±ê¸‰ (TOP 1%) - ì •ëŸ‰+ì •ì„± í†µí•©ë¶„ì„",
                "percentile": "ìƒìœ„ 1%"
            }
        elif score >= 90:
            return {
                "grade": "OKâ˜…â˜…",
                "grade_description": "ìš°ìˆ˜ ë“±ê¸‰ (TOP 5%) - ì •ëŸ‰+ì •ì„± í†µí•©ë¶„ì„",
                "percentile": "ìƒìœ„ 5%"
            }
        elif score >= 85:
            return {
                "grade": "OKâ˜…",
                "grade_description": "ìš°ìˆ˜+ ë“±ê¸‰ (TOP 10%) - ì •ëŸ‰+ì •ì„± í†µí•©ë¶„ì„",
                "percentile": "ìƒìœ„ 10%"
            }
        elif score >= 80:
            return {
                "grade": "OK A",
                "grade_description": "ì–‘í˜¸ ë“±ê¸‰ (TOP 20%) - ì •ëŸ‰+ì •ì„± í†µí•©ë¶„ì„",
                "percentile": "ìƒìœ„ 20%"
            }
        elif score >= 75:
            return {
                "grade": "OK B+",
                "grade_description": "ì–‘í˜¸- ë“±ê¸‰ (TOP 30%) - ì •ëŸ‰+ì •ì„± í†µí•©ë¶„ì„",
                "percentile": "ìƒìœ„ 30%"
            }
        elif score >= 70:
            return {
                "grade": "OK B",
                "grade_description": "ë³´í†µ ë“±ê¸‰ (TOP 40%) - ì •ëŸ‰+ì •ì„± í†µí•©ë¶„ì„",
                "percentile": "ìƒìœ„ 40%"
            }
        elif score >= 60:
            return {
                "grade": "OK C",
                "grade_description": "ê°œì„ í•„ìš” ë“±ê¸‰ (TOP 60%) - ì •ëŸ‰+ì •ì„± í†µí•©ë¶„ì„",
                "percentile": "ìƒìœ„ 60%"
            }
        else:
            return {
                "grade": "OK D",
                "grade_description": "ì§‘ì¤‘ê°œì„  ë“±ê¸‰ (í•˜ìœ„ 40%) - ì •ëŸ‰+ì •ì„± í†µí•©ë¶„ì„",
                "percentile": "í•˜ìœ„ 40%"
            }

# ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
if hybrid_analyzer is None:
    # ì„œë¹„ìŠ¤ì—ì„œ ë¡œë“œ ì‹¤íŒ¨ì‹œ ë¡œì»¬ ì •ì˜ ì‚¬ìš©
    hybrid_analyzer = AIRISSHybridAnalyzer()

# API ëª¨ë¸ ì •ì˜
class AnalysisRequest(BaseModel):
    file_id: str
    analysis_type: str = "regression"  # regression, classification, clustering
    model_type: Optional[str] = "auto"
    target_column: Optional[str] = None
    features: Optional[List[str]] = None
    sample_size: Optional[int] = None
    analysis_mode: Optional[str] = "hybrid"  # hybrid, text_only, data_only
    enable_ai_feedback: Optional[bool] = False
    openai_api_key: Optional[str] = None
    openai_model: Optional[str] = "gpt-3.5-turbo"
    max_tokens: Optional[int] = 500

class AnalysisJob(BaseModel):
    job_id: str
    file_id: str
    status: str
    created_at: datetime
    progress: float = 0.0
    total_records: int = 0
    processed_records: int = 0
    failed_records: int = 0

# ğŸ¯ API ì—”ë“œí¬ì¸íŠ¸ë“¤ - ğŸ”¥ ë¬´í•œ ë¡œë”© í•´ê²° ì™„ë£Œ

@router.post("/start")
async def start_analysis(request: AnalysisRequest, background_tasks: BackgroundTasks):
    """ë¶„ì„ ì‘ì—… ì‹œì‘ - v4.0 ë¬´í•œ ë¡œë”© í•´ê²° ì™„ë£Œ"""
    try:
        logger.info(f"ğŸš€ ë¶„ì„ ì‹œì‘ ìš”ì²­: file_id={request.file_id}, sample_size={request.sample_size}")
        
        # DB ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        db_service = get_db_service()
        if not db_service:
            error_msg = "ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            logger.error(f"âŒ {error_msg}")
            raise HTTPException(status_code=503, detail=error_msg)
        
        # DB ì´ˆê¸°í™” í™•ì¸
        await db_service.init_database()
        
        # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
        try:
            file_data = await db_service.get_file(request.file_id)
            if not file_data:
                logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {request.file_id}")
                raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            logger.info(f"âœ… íŒŒì¼ í™•ì¸ ì™„ë£Œ: {file_data['filename']}")
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # 2. ì‘ì—… ID ìƒì„±
        job_id = str(uuid.uuid4())
        logger.info(f"ğŸ†• ì‘ì—… ID ìƒì„±: {job_id}")
        
        # 3. ì‘ì—… ë°ì´í„° ì¤€ë¹„
        job_data = {
            "job_id": job_id,  # ğŸ”¥ í•µì‹¬: ëª…ì‹œì ìœ¼ë¡œ job_id í¬í•¨
            "file_id": request.file_id,
            "status": "processing",
            "sample_size": request.sample_size or 10,
            "analysis_mode": request.analysis_mode or "hybrid",
            "analysis_type": request.analysis_type,
            "model_type": request.model_type,
            "target_column": request.target_column,
            "features": request.features,
            "enable_ai_feedback": request.enable_ai_feedback or False,
            "start_time": datetime.now().isoformat(),
            "progress": 0.0,
            "total_records": request.sample_size or 10,
            "processed_records": 0,
            "failed_records": 0,
            "version": "4.0"
        }
        
        # 4. SQLiteì— ì‘ì—… ì €ì¥
        try:
            saved_job_id = await db_service.create_analysis_job(job_data)
            if saved_job_id != job_id:
                error_msg = f"âŒ Job ID ë¶ˆì¼ì¹˜! ìš”ì²­: {job_id}, ì €ì¥: {saved_job_id}"
                logger.error(error_msg)
                raise HTTPException(status_code=500, detail="ì‘ì—… ID ìƒì„± ì˜¤ë¥˜")
            logger.info(f"âœ… ì‘ì—… ì €ì¥ ì™„ë£Œ: {job_id}")
        except Exception as e:
            logger.error(f"âŒ ì‘ì—… ì €ì¥ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ì‘ì—… ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # 5. ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ (í•µì‹¬ ìˆ˜ì •)
        try:
            # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ê°€ ì‹¤ì œë¡œ ì‹œì‘ë˜ëŠ”ì§€ í™•ì¸
            logger.info(f"âš¡ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘: {job_id}")
            
            # WebSocket ë§¤ë‹ˆì € ê°€ì ¸ì˜¤ê¸° (ì‹¤íŒ¨í•´ë„ ë¶„ì„ì€ ê³„ì†)
            ws_manager = get_ws_manager()
            
            # FastAPIì˜ background_tasks ì‚¬ìš©
            background_tasks.add_task(
                safe_process_analysis_v4,
                job_id,
                request,
                db_service,
                ws_manager
            )
            logger.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì¶”ê°€ ì™„ë£Œ: {job_id}")
        except Exception as e:
            logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ì‹¤íŒ¨ ì‹œ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            try:
                await db_service.update_analysis_job(job_id, {
                    "status": "failed",
                    "error": f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ ì‹¤íŒ¨: {str(e)}"
                })
            except Exception as update_error:
                logger.error(f"âŒ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_error}")
            raise HTTPException(status_code=500, detail=f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ ì˜¤ë¥˜: {str(e)}")
        
        # 6. WebSocket ì•Œë¦¼
        try:
            ws_manager = get_ws_manager()
            if ws_manager:
                await ws_manager.broadcast_to_channel("analysis", {
                    "type": "analysis_started",
                    "job_id": job_id,
                    "file_id": request.file_id,
                    "analysis_mode": request.analysis_mode,
                    "timestamp": datetime.now().isoformat()
                })
                logger.info(f"âœ… WebSocket ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {job_id}")
            else:
                logger.info(f"â„¹ï¸ WebSocket ë§¤ë‹ˆì € ì—†ìŒ - ì•Œë¦¼ ìƒëµ: {job_id}")
        except Exception as e:
            logger.warning(f"âš ï¸ WebSocket ì•Œë¦¼ ì‹¤íŒ¨ (ë¶„ì„ì€ ê³„ì† ì§„í–‰): {e}")
        
        # 7. ì„±ê³µ ì‘ë‹µ
        response = {
            "job_id": job_id,
            "status": "started",
            "message": "AIRISS v4.0 ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
            "analysis_type": request.analysis_type,
            "model_type": request.model_type,
            "analysis_mode": request.analysis_mode or "hybrid",
            "ai_feedback_enabled": request.enable_ai_feedback or False,
            "sample_size": request.sample_size or 10,
            "estimated_time": f"{(request.sample_size or 10) * 0.2}ì´ˆ"
        }
        
        logger.info(f"ğŸ‰ ë¶„ì„ ì‹œì‘ ì™„ë£Œ: {job_id}")
        return response
        
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
        raise
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ì²˜ë¦¬
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ë¶„ì„ ì‹œì‘ ì˜¤ë¥˜: {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì‹œì‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")

# ğŸ”¥ ì•ˆì „í•œ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ í•¨ìˆ˜ (ë¬´í•œ ë¡œë”© í•´ê²° í•µì‹¬)
async def safe_process_analysis_v4(job_id: str, request: AnalysisRequest, db_service, ws_manager):
    """ì•ˆì „í•œ ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì²˜ë¦¬ - ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”"""
    try:
        logger.info(f"ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘: {job_id}")
        
        # WebSocket ë§¤ë‹ˆì € ì¬í™•ì¸ (Noneì¼ ìˆ˜ ìˆìŒ)
        if ws_manager is None:
            ws_manager = get_ws_manager()
        
        # ì‹¤ì œ ë¶„ì„ ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
        await process_analysis_v4(job_id, request, db_service, ws_manager)
        
        logger.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì™„ë£Œ: {job_id}")
        
    except Exception as e:
        logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì¹˜ëª…ì  ì˜¤ë¥˜: {job_id} - {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
        try:
            await db_service.update_analysis_job(job_id, {
                "status": "failed",
                "error": f"ë¶„ì„ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}",
                "end_time": datetime.now().isoformat()
            })
            
            # WebSocket ì˜¤ë¥˜ ì•Œë¦¼ (ws_managerê°€ ìˆì„ ë•Œë§Œ)
            if ws_manager:
                try:
                    await ws_manager.broadcast_to_channel("analysis", {
                        "type": "analysis_failed",
                        "job_id": job_id,
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    })
                except Exception as ws_error:
                    logger.error(f"âŒ WebSocket ì˜¤ë¥˜ ì•Œë¦¼ ì‹¤íŒ¨: {ws_error}")
            
        except Exception as update_error:
            logger.error(f"âŒ ì˜¤ë¥˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_error}")

# ğŸ”¥ ê¸°ì¡´ ë¶„ì„ ì²˜ë¦¬ í•¨ìˆ˜ (ì•ˆì •í™” ìˆ˜ì •)
async def process_analysis_v4(job_id: str, request: AnalysisRequest, db_service, ws_manager):
    """AIRISS v4.0 ë¶„ì„ ì²˜ë¦¬ - ì•ˆì •í™” ë²„ì „"""
    try:
        logger.info(f"ğŸ“Š ë¶„ì„ ì²˜ë¦¬ ì‹œì‘: {job_id}")
        
        # 1. íŒŒì¼ ë°ì´í„° ë¡œë“œ
        file_data = await db_service.get_file(request.file_id)
        if not file_data:
            raise Exception(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {request.file_id}")
        
        # 2. DataFrame í™•ì¸
        df = file_data.get('dataframe')
        if df is None:
            raise Exception("DataFrameì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        logger.info(f"ğŸ“‹ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰")
        
        # íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
        filename = file_data.get('filename', 'unknown_file')
        
        # 3. ìƒ˜í”Œ ë°ì´í„° ì„ íƒ
        sample_size = request.sample_size if request.sample_size is not None else len(df)
        if sample_size >= len(df):
            sample_df = df.copy()
        else:
            sample_df = df.head(sample_size).copy()
        
        # 4. ì»¬ëŸ¼ ì •ë³´ íŒŒì‹± ë° ê²€ì¦ (ì™„ì „ ì¬ì‘ì„±)
        uid_cols_raw = file_data.get('uid_columns', '[]')
        opinion_cols_raw = file_data.get('opinion_columns', '[]')
        
        # UID ë³€ìˆ˜ ë¯¸ë¦¬ ì´ˆê¸°í™” (UnboundLocalError ë°©ì§€)
        uid = f"user_0"
        
        # ì»¬ëŸ¼ëª… íŒŒì‹± í•¨ìˆ˜
        def parse_column_string(col_str):
            """ì»¬ëŸ¼ëª… ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±"""
            try:
                if not col_str:
                    return []
                
                if isinstance(col_str, list):
                    return col_str
                
                if isinstance(col_str, str):
                    # JSON ë°°ì—´ í˜•íƒœì¸ì§€ í™•ì¸
                    col_str = col_str.strip()
                    if col_str.startswith('[') and col_str.endswith(']'):
                        try:
                            parsed = json.loads(col_str)
                            if isinstance(parsed, list):
                                return parsed
                            else:
                                return [str(parsed)]
                        except json.JSONDecodeError:
                            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                            return [col_str.strip('[]"\'')]
                    else:
                        # ë‹¨ì¼ ì»¬ëŸ¼ëª…ì¸ ê²½ìš°
                        return [col_str] if col_str else []
                
                return []
            except Exception as e:
                logger.warning(f"ì»¬ëŸ¼ëª… íŒŒì‹± ì‹¤íŒ¨: {e}")
                return []
        
        # ì»¬ëŸ¼ëª… íŒŒì‹±
        uid_cols = parse_column_string(uid_cols_raw)
        opinion_cols = parse_column_string(opinion_cols_raw)
        
        logger.info(f"ğŸ”§ íŒŒì‹±ëœ ì»¬ëŸ¼: UID={uid_cols}, ì˜ê²¬={opinion_cols}")
        
        # ì»¬ëŸ¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì´ë©´ ìë™ ê°ì§€
        if not uid_cols:
            # DataFrameì—ì„œ UID ì»¬ëŸ¼ ìë™ ê°ì§€
            uid_cols = [col for col in df.columns if 'uid' in col.lower() or 'id' in col.lower()]
            if not uid_cols:
                uid_cols = [df.columns[0]]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ UIDë¡œ ì‚¬ìš©
            logger.info(f"ğŸ”§ UID ì»¬ëŸ¼ ìë™ ê°ì§€: {uid_cols}")
        
        if not opinion_cols:
            # DataFrameì—ì„œ ì˜ê²¬ ì»¬ëŸ¼ ìë™ ê°ì§€
            opinion_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['ì˜ê²¬', 'opinion', 'comment', 'text', 'ì„¤ëª…'])]
            if not opinion_cols:
                # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ê¸°
                text_cols = [col for col in df.columns if col not in uid_cols and df[col].dtype == 'object']
                opinion_cols = text_cols[:2]  # ìµœëŒ€ 2ê°œê¹Œì§€
            logger.info(f"ğŸ”§ ì˜ê²¬ ì»¬ëŸ¼ ìë™ ê°ì§€: {opinion_cols}")
        
        logger.info(f"ğŸ”§ ìµœì¢… ì»¬ëŸ¼ í™•ì¸: UID={uid_cols}, ì˜ê²¬={opinion_cols}")
        
        # 5. ë¶„ì„ ì§„í–‰
        results = []
        total_rows = len(sample_df)
        
        for idx, row in sample_df.iterrows():
            try:
                # UIDì™€ ì˜ê²¬ ì¶”ì¶œ (ì™„ì „íˆ ì•ˆì „í•œ ë°©ì‹)
                try:
                    if uid_cols and len(uid_cols) > 0:
                        uid_col = uid_cols[0]
                        if uid_col in row.index:
                            uid = str(row[uid_col])
                        else:
                            uid = f"user_{idx}"
                    else:
                        uid = f"user_{idx}"
                except (KeyError, IndexError, TypeError, AttributeError) as e:
                    logger.warning(f"UID ì¶”ì¶œ ì‹¤íŒ¨ (í–‰ {idx}): {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                    uid = f"user_{idx}"
                
                try:
                    if opinion_cols and len(opinion_cols) > 0:
                        opinion_col = opinion_cols[0]
                        if opinion_col in row.index:
                            opinion = str(row[opinion_col])
                        else:
                            opinion = ""
                    else:
                        opinion = ""
                except (KeyError, IndexError, TypeError, AttributeError) as e:
                    logger.warning(f"ì˜ê²¬ ì¶”ì¶œ ì‹¤íŒ¨ (í–‰ {idx}): {e}, ë¹ˆ ë¬¸ìì—´ ì‚¬ìš©")
                    opinion = ""
                
                if not opinion or opinion.lower() in ['nan', 'null', '', 'none']:
                    opinion = ""
                
                if request.analysis_mode == "hybrid" and opinion:
                    analysis_result = hybrid_analyzer.comprehensive_analysis(
                        uid=uid, 
                        opinion=opinion, 
                        row_data=row,
                        save_to_storage=True,
                        file_id=str(job_id),
                        filename=filename,
                        enable_ai=request.enable_ai_feedback,
                        openai_api_key=request.openai_api_key,
                        openai_model=request.openai_model,
                        max_tokens=request.max_tokens
                    )
                    text_analysis = analysis_result.get("text_analysis", {})
                    quant_analysis = analysis_result.get("quantitative_analysis", {})
                    hybrid_analysis = analysis_result.get("hybrid_analysis", {})
                    explainability = analysis_result.get("explainability", {})

                    dimension_scores = text_analysis.get("dimension_scores", {})
                    dimension_details = text_analysis.get("dimension_details", {})

                    key_positives = explainability.get("key_positive_factors", [])
                    key_negatives = explainability.get("key_negative_factors", [])
                    improvement_suggestions = explainability.get("improvement_suggestions", [])

                    ai_feedback = {}
                    if request.enable_ai_feedback and request.openai_api_key and request.openai_api_key.strip():
                        try:
                            logger.info(f"ğŸ¤– AI í”¼ë“œë°± ìƒì„± ì‹œì‘: {uid}")
                            ai_feedback = await hybrid_analyzer.text_analyzer.generate_ai_feedback(
                                uid, opinion, request.openai_api_key, request.openai_model, request.max_tokens
                            )
                            logger.info(f"âœ… AI í”¼ë“œë°± ìƒì„± ì™„ë£Œ: {uid}")
                        except Exception as e:
                            logger.warning(f"âš ï¸ AI í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨ - UID {uid}: {e}")
                            ai_feedback = {
                                "ai_feedback": "AI í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                                "ai_strengths": "",
                                "ai_weaknesses": "",
                                "ai_recommendations": [],
                                "error": str(e)
                            }
                    else:
                        # AI í”¼ë“œë°±ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€
                        ai_feedback = {
                            "ai_feedback": "AI í”¼ë“œë°±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                            "ai_strengths": "",
                            "ai_weaknesses": "",
                            "ai_recommendations": [],
                            "error": ""
                        }

                    result_record = {
                        # === ê¸°ë³¸ ì •ë³´ ===
                        "UID": uid,
                        "ì›ë³¸ì˜ê²¬": opinion,
                        "ë¶„ì„ì¼ì‹œ": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "ë¶„ì„ë²„ì „": "AIRISS v4.0 í•˜ì´ë¸Œë¦¬ë“œ",
                        # === ì¢…í•© ì ìˆ˜ ===
                        "AIRISS_v4_ì¢…í•©ì ìˆ˜": hybrid_analysis.get("overall_score", 0),
                        "OKë“±ê¸‰": hybrid_analysis.get("grade", "OK C"),
                        "ë“±ê¸‰ì„¤ëª…": hybrid_analysis.get("grade_description", ""),
                        "ë°±ë¶„ìœ„": hybrid_analysis.get("percentile", ""),
                        "ë¶„ì„ì‹ ë¢°ë„": hybrid_analysis.get("confidence", 0),
                        # === í…ìŠ¤íŠ¸ ë¶„ì„ ìƒì„¸ ===
                        "í…ìŠ¤íŠ¸_ì¢…í•©ì ìˆ˜": text_analysis.get("overall_score", 0),
                        "í…ìŠ¤íŠ¸_ë“±ê¸‰": text_analysis.get("grade", ""),
                        # === 8ëŒ€ ì˜ì—­ë³„ ì ìˆ˜ ===
                        "ì—…ë¬´ì„±ê³¼_ì ìˆ˜": dimension_scores.get("ì—…ë¬´ì„±ê³¼", 0),
                        "KPIë‹¬ì„±_ì ìˆ˜": dimension_scores.get("KPIë‹¬ì„±", 0),
                        "íƒœë„ë§ˆì¸ë“œ_ì ìˆ˜": dimension_scores.get("íƒœë„ë§ˆì¸ë“œ", 0),
                        "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜_ì ìˆ˜": dimension_scores.get("ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", 0),
                        "ë¦¬ë”ì‹­í˜‘ì—…_ì ìˆ˜": dimension_scores.get("ë¦¬ë”ì‹­í˜‘ì—…", 0),
                        "ì „ë¬¸ì„±í•™ìŠµ_ì ìˆ˜": dimension_scores.get("ì „ë¬¸ì„±í•™ìŠµ", 0),
                        "ì°½ì˜í˜ì‹ _ì ìˆ˜": dimension_scores.get("ì°½ì˜í˜ì‹ ", 0),
                        "ì¡°ì§ì ì‘_ì ìˆ˜": dimension_scores.get("ì¡°ì§ì ì‘", 0),
                        # === 8ëŒ€ ì˜ì—­ë³„ ìƒì„¸ ë¶„ì„ ===
                        "ì—…ë¬´ì„±ê³¼_ê¸ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì—…ë¬´ì„±ê³¼", {}).get("signals", {}).get("positive_words", [])),
                        "ì—…ë¬´ì„±ê³¼_ë¶€ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì—…ë¬´ì„±ê³¼", {}).get("signals", {}).get("negative_words", [])),
                        "ì—…ë¬´ì„±ê³¼_ì‹ ë¢°ë„": dimension_details.get("ì—…ë¬´ì„±ê³¼", {}).get("confidence", 0),
                        "KPIë‹¬ì„±_ê¸ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("KPIë‹¬ì„±", {}).get("signals", {}).get("positive_words", [])),
                        "KPIë‹¬ì„±_ë¶€ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("KPIë‹¬ì„±", {}).get("signals", {}).get("negative_words", [])),
                        "KPIë‹¬ì„±_ì‹ ë¢°ë„": dimension_details.get("KPIë‹¬ì„±", {}).get("confidence", 0),
                        "íƒœë„ë§ˆì¸ë“œ_ê¸ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("íƒœë„ë§ˆì¸ë“œ", {}).get("signals", {}).get("positive_words", [])),
                        "íƒœë„ë§ˆì¸ë“œ_ë¶€ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("íƒœë„ë§ˆì¸ë“œ", {}).get("signals", {}).get("negative_words", [])),
                        "íƒœë„ë§ˆì¸ë“œ_ì‹ ë¢°ë„": dimension_details.get("íƒœë„ë§ˆì¸ë“œ", {}).get("confidence", 0),
                        "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜_ê¸ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", {}).get("signals", {}).get("positive_words", [])),
                        "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜_ë¶€ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", {}).get("signals", {}).get("negative_words", [])),
                        "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜_ì‹ ë¢°ë„": dimension_details.get("ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", {}).get("confidence", 0),
                        "ë¦¬ë”ì‹­í˜‘ì—…_ê¸ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ë¦¬ë”ì‹­í˜‘ì—…", {}).get("signals", {}).get("positive_words", [])),
                        "ë¦¬ë”ì‹­í˜‘ì—…_ë¶€ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ë¦¬ë”ì‹­í˜‘ì—…", {}).get("signals", {}).get("negative_words", [])),
                        "ë¦¬ë”ì‹­í˜‘ì—…_ì‹ ë¢°ë„": dimension_details.get("ë¦¬ë”ì‹­í˜‘ì—…", {}).get("confidence", 0),
                        "ì „ë¬¸ì„±í•™ìŠµ_ê¸ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì „ë¬¸ì„±í•™ìŠµ", {}).get("signals", {}).get("positive_words", [])),
                        "ì „ë¬¸ì„±í•™ìŠµ_ë¶€ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì „ë¬¸ì„±í•™ìŠµ", {}).get("signals", {}).get("negative_words", [])),
                        "ì „ë¬¸ì„±í•™ìŠµ_ì‹ ë¢°ë„": dimension_details.get("ì „ë¬¸ì„±í•™ìŠµ", {}).get("confidence", 0),
                        "ì°½ì˜í˜ì‹ _ê¸ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì°½ì˜í˜ì‹ ", {}).get("signals", {}).get("positive_words", [])),
                        "ì°½ì˜í˜ì‹ _ë¶€ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì°½ì˜í˜ì‹ ", {}).get("signals", {}).get("negative_words", [])),
                        "ì°½ì˜í˜ì‹ _ì‹ ë¢°ë„": dimension_details.get("ì°½ì˜í˜ì‹ ", {}).get("confidence", 0),
                        "ì¡°ì§ì ì‘_ê¸ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì¡°ì§ì ì‘", {}).get("signals", {}).get("positive_words", [])),
                        "ì¡°ì§ì ì‘_ë¶€ì •í‚¤ì›Œë“œ": ', '.join(dimension_details.get("ì¡°ì§ì ì‘", {}).get("signals", {}).get("negative_words", [])),
                        "ì¡°ì§ì ì‘_ì‹ ë¢°ë„": dimension_details.get("ì¡°ì§ì ì‘", {}).get("confidence", 0),
                        # === ì •ëŸ‰ ë¶„ì„ ===
                        "ì •ëŸ‰_ì¢…í•©ì ìˆ˜": quant_analysis.get("quantitative_score", 0),
                        "ì •ëŸ‰_ì‹ ë¢°ë„": quant_analysis.get("confidence", 0),
                        "ì •ëŸ‰_ë°ì´í„°í’ˆì§ˆ": quant_analysis.get("data_quality", "ì—†ìŒ"),
                        "ì •ëŸ‰_ë°ì´í„°ê°œìˆ˜": quant_analysis.get("data_count", 0),
                        "ì •ëŸ‰_ê¸°ì—¬ìš”ì¸": str(quant_analysis.get("contributing_factors", {})),
                        # === ê°•ì  ë¶„ì„ ===
                        "ì£¼ìš”ê°•ì _1ì˜ì—­": key_positives[0].get("factor", "") if len(key_positives) > 0 else "",
                        "ì£¼ìš”ê°•ì _1ì ìˆ˜": key_positives[0].get("score", 0) if len(key_positives) > 0 else 0,
                        "ì£¼ìš”ê°•ì _1ì¦ê±°": ', '.join(key_positives[0].get("evidence", [])) if len(key_positives) > 0 else "",
                        "ì£¼ìš”ê°•ì _2ì˜ì—­": key_positives[1].get("factor", "") if len(key_positives) > 1 else "",
                        "ì£¼ìš”ê°•ì _2ì ìˆ˜": key_positives[1].get("score", 0) if len(key_positives) > 1 else 0,
                        "ì£¼ìš”ê°•ì _2ì¦ê±°": ', '.join(key_positives[1].get("evidence", [])) if len(key_positives) > 1 else "",
                        "ì£¼ìš”ê°•ì _3ì˜ì—­": key_positives[2].get("factor", "") if len(key_positives) > 2 else "",
                        "ì£¼ìš”ê°•ì _3ì ìˆ˜": key_positives[2].get("score", 0) if len(key_positives) > 2 else 0,
                        "ì£¼ìš”ê°•ì _3ì¦ê±°": ', '.join(key_positives[2].get("evidence", [])) if len(key_positives) > 2 else "",
                        # === ì•½ì  ë¶„ì„ ===
                        "ê°œì„ í•„ìš”_1ì˜ì—­": key_negatives[0].get("factor", "") if len(key_negatives) > 0 else "",
                        "ê°œì„ í•„ìš”_1ì ìˆ˜": key_negatives[0].get("score", 0) if len(key_negatives) > 0 else 0,
                        "ê°œì„ í•„ìš”_1ì¦ê±°": ', '.join(key_negatives[0].get("evidence", [])) if len(key_negatives) > 0 else "",
                        "ê°œì„ í•„ìš”_2ì˜ì—­": key_negatives[1].get("factor", "") if len(key_negatives) > 1 else "",
                        "ê°œì„ í•„ìš”_2ì ìˆ˜": key_negatives[1].get("score", 0) if len(key_negatives) > 1 else 0,
                        "ê°œì„ í•„ìš”_2ì¦ê±°": ', '.join(key_negatives[1].get("evidence", [])) if len(key_negatives) > 1 else "",
                        "ê°œì„ í•„ìš”_3ì˜ì—­": key_negatives[2].get("factor", "") if len(key_negatives) > 2 else "",
                        "ê°œì„ í•„ìš”_3ì ìˆ˜": key_negatives[2].get("score", 0) if len(key_negatives) > 2 else 0,
                        "ê°œì„ í•„ìš”_3ì¦ê±°": ', '.join(key_negatives[2].get("evidence", [])) if len(key_negatives) > 2 else "",
                        # === AI ê°œì„  ì œì•ˆ ===
                        "AIê°œì„ ì œì•ˆ_1": improvement_suggestions[0] if len(improvement_suggestions) > 0 else "",
                        "AIê°œì„ ì œì•ˆ_2": improvement_suggestions[1] if len(improvement_suggestions) > 1 else "",
                        "AIê°œì„ ì œì•ˆ_3": improvement_suggestions[2] if len(improvement_suggestions) > 2 else "",
                        # === ê³ ê¸‰ AI í”¼ë“œë°± (OpenAI ì‚¬ìš© ì‹œ) ===
                        "AI_ì¢…í•©í”¼ë“œë°±": ai_feedback.get("ai_feedback", ""),
                        "AI_í•µì‹¬ê°•ì ": ai_feedback.get("ai_strengths", ""),
                        "AI_ê°œì„ ì˜ì—­": ai_feedback.get("ai_weaknesses", ""),
                        "AI_ì‹¤í–‰ê³„íš": '\n'.join(ai_feedback.get("ai_recommendations", [])),
                        "AI_í”¼ë“œë°±_ì˜¤ë¥˜": ai_feedback.get("error", ""),
                        # === ë¶„ì„ êµ¬ì„± ===
                        "ë¶„ì„ëª¨ë“œ": request.analysis_mode,
                        "í…ìŠ¤íŠ¸_ê°€ì¤‘ì¹˜": hybrid_analysis.get("analysis_composition", {}).get("text_weight", 60),
                        "ì •ëŸ‰_ê°€ì¤‘ì¹˜": hybrid_analysis.get("analysis_composition", {}).get("quantitative_weight", 40),
                        # === í¸í–¥ì„± ë¶„ì„ ===
                        "í¸í–¥ì„±_ê²€ì‚¬": "ì‹¤ì‹œë¨" if "bias_analysis" in analysis_result else "ë¯¸ì‹¤ì‹œ",
                        "ê³µì •ì„±_ì ìˆ˜": analysis_result.get("bias_analysis", {}).get("fairness_score", 100),
                        "í¸í–¥_ìœ„í—˜ë„": analysis_result.get("bias_analysis", {}).get("bias_score", 0),
                        "í¸í–¥_ìƒì„¸": str(analysis_result.get("bias_analysis", {}).get("bias_details", [])),
                        # === ì„±ê³¼ ì˜ˆì¸¡ (ê°€ëŠ¥í•œ ê²½ìš°) ===
                        "ì„±ê³¼_6ê°œì›”_ì „ë§": analysis_result.get("performance_prediction", {}).get("6month_trend", ""),
                        "ì„±ê³µ_í™•ë¥ ": analysis_result.get("performance_prediction", {}).get("success_probability", 0),
                        "ì´ì§_ìœ„í—˜ë„": analysis_result.get("performance_prediction", {}).get("turnover_risk_score", 0),
                        "ìŠ¹ì§„_ì¤€ë¹„ë„": analysis_result.get("performance_prediction", {}).get("promotion_readiness", ""),
                        # === ë©”íƒ€ë°ì´í„° ===
                        "ë¶„ì„_ë°ì´í„°ì†ŒìŠ¤": "í…ìŠ¤íŠ¸+ì •ëŸ‰" if opinion and quant_analysis.get("data_count", 0) > 0 else "í…ìŠ¤íŠ¸" if opinion else "ì •ëŸ‰",
                        "ì‹ ë¢°ë„_ì„¤ëª…": explainability.get("confidence_explanation", ""),
                        "ì ìˆ˜_êµ¬ì„±_ì„¤ëª…": f"í…ìŠ¤íŠ¸ë¶„ì„({hybrid_analysis.get('analysis_composition', {}).get('text_weight', 60)}%) + ì •ëŸ‰ë¶„ì„({hybrid_analysis.get('analysis_composition', {}).get('quantitative_weight', 40)}%)",
                        # === ì‹œìŠ¤í…œ ì •ë³´ ===
                        "ë¶„ì„ì‹œìŠ¤í…œ": "AIRISS v4.0 - SQLite í†µí•© ì‹œìŠ¤í…œ",
                        "ì‚¬ìš©ëª¨ë¸": "í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ê¸° + ë”¥ëŸ¬ë‹ NLP + í¸í–¥íƒì§€",
                        "OpenAI_í™œìš©": "ì˜ˆ" if request.enable_ai_feedback else "ì•„ë‹ˆì˜¤",
                        "OpenAI_ëª¨ë¸": request.openai_model if request.enable_ai_feedback else ""
                    }
                else:
                    main_score = 75.0
                    main_grade = "OK B+"
                    result_record = {
                        "UID": uid,
                        "ì›ë³¸ì˜ê²¬": opinion,
                        "AIRISS_v4_ì¢…í•©ì ìˆ˜": main_score,
                        "OKë“±ê¸‰": main_grade,
                        "ë“±ê¸‰ì„¤ëª…": f"{main_grade} ë“±ê¸‰ - AIRISS v4.0 ê¸°ë³¸ë¶„ì„",
                        "ë°±ë¶„ìœ„": "ìƒìœ„ 30%",
                        "ë¶„ì„ì‹ ë¢°ë„": 70.0,
                        "ë¶„ì„ëª¨ë“œ": request.analysis_mode,
                        "ë¶„ì„ì‹œê°„": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "ë¶„ì„ì‹œìŠ¤í…œ": "AIRISS v4.0 - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ",
                        "ì£¼ì˜ì‚¬í•­": "í…ìŠ¤íŠ¸ ì˜ê²¬ì´ ë¶€ì¡±í•˜ì—¬ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰ë¨"
                    }

                # ê²°ê³¼ ì €ì¥ (ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ìˆ˜ì •)
                try:
                    # result_recordì— job_idì™€ í•„ìˆ˜ í•„ë“œ ì¶”ê°€
                    result_record["job_id"] = job_id
                    result_record["file_id"] = request.file_id
                    result_record["filename"] = filename
                    
                    # í•„ë“œëª… ë§¤í•‘ (save_analysis_resultê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ)
                    result_record["hybrid_score"] = result_record.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0)
                    result_record["text_score"] = result_record.get("í…ìŠ¤íŠ¸ì ìˆ˜", 0)
                    result_record["quantitative_score"] = result_record.get("ì •ëŸ‰ì ìˆ˜", 0)
                    result_record["opinion"] = result_record.get("ì›ë³¸ì˜ê²¬", "")
                    result_record["confidence"] = result_record.get("ë¶„ì„ì‹ ë¢°ë„", 0)
                    
                    await db_service.save_analysis_result(result_record)
                    logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {uid}")
                except Exception as save_error:
                    logger.error(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ - UID {uid}: {save_error}")
                    import traceback
                    logger.error(traceback.format_exc())
                    # ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¶„ì„ì€ ê³„ì† ì§„í–‰
                
                results.append(result_record)

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_processed = len(results)
                progress = (current_processed / total_rows) * 100
                current_avg_score = sum(r["AIRISS_v4_ì¢…í•©ì ìˆ˜"] for r in results) / len(results) if results else 0

                # job_info ì—…ë°ì´íŠ¸ (API ìƒíƒœ ì¡°íšŒìš©)
                job_info["progress"] = min(progress, 100)
                job_info["processed"] = current_processed
                job_info["total"] = total_rows
                job_info["average_score"] = round(current_avg_score, 1)
                job_info["message"] = f"ì²˜ë¦¬ ì¤‘... {uid} ({current_processed}/{total_rows})"

                await db_service.update_analysis_job(job_id, {
                    "processed_records": current_processed,
                    "progress": min(progress, 100)
                })

                await ws_manager.broadcast_to_channel("analysis", {
                    "type": "analysis_progress",
                    "job_id": job_id,
                    "progress": progress,
                    "processed": current_processed,
                    "total": total_rows,
                    "current_uid": uid,
                    "current_score": result_record.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0),
                    "timestamp": datetime.now().isoformat()
                })

                logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {progress:.1f}% ({current_processed}/{total_rows})")
                await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"âŒ ê°œë³„ ë¶„ì„ ì˜¤ë¥˜ - UID {uid}: {e}")
                continue
        
        # 6. ë¶„ì„ ì™„ë£Œ ì²˜ë¦¬
        end_time = datetime.now()
        avg_score = sum(r["AIRISS_v4_ì¢…í•©ì ìˆ˜"] for r in results) / len(results) if results else 0
        
        # job_info ì—…ë°ì´íŠ¸ (API ìƒíƒœ ì¡°íšŒìš©)
        job_info["status"] = "completed"
        job_info["progress"] = 100
        job_info["processed"] = len(results)
        job_info["total"] = total_rows
        job_info["average_score"] = round(avg_score, 1)
        job_info["end_time"] = end_time.isoformat()
        job_info["message"] = "ë¶„ì„ ì™„ë£Œ"
        
        await db_service.update_analysis_job(job_id, {
            "status": "completed",
            "end_time": end_time.isoformat(),
            "average_score": round(avg_score, 1),
            "processed_records": len(results),
            "progress": 100.0
        })
        
        # WebSocket ì™„ë£Œ ì•Œë¦¼
        await ws_manager.broadcast_to_channel("analysis", {
            "type": "analysis_completed",
            "job_id": job_id,
            "total_processed": len(results),
            "average_score": round(avg_score, 1),
            "timestamp": end_time.isoformat()
        })
        
        logger.info(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ: {job_id}, ì„±ê³µ: {len(results)}")
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì²˜ë¦¬ ì˜¤ë¥˜: {job_id} - {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        
        # job_info ì—…ë°ì´íŠ¸ (API ìƒíƒœ ì¡°íšŒìš©)
        job_info["status"] = "failed"
        job_info["progress"] = 0
        job_info["error"] = str(e)
        job_info["message"] = f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        job_info["end_time"] = datetime.now().isoformat()
        
        # ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
        await db_service.update_analysis_job(job_id, {
            "status": "failed",
            "error": str(e),
            "end_time": datetime.now().isoformat()
        })
        
        # WebSocket ì˜¤ë¥˜ ì•Œë¦¼
        await ws_manager.broadcast_to_channel("analysis", {
            "type": "analysis_failed",
            "job_id": job_id,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        })
        
        raise

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
_status_cache = {}
if 'analysis_jobs' not in globals():
    analysis_jobs = {}
if 'completed_analyses' not in globals():
    completed_analyses = {}

@router.get("/debug/jobs")
async def debug_jobs():
    """í˜„ì¬ ì €ì¥ëœ ëª¨ë“  job í™•ì¸"""
    return {
        "analysis_jobs": list(analysis_jobs.keys()) if 'analysis_jobs' in globals() else [],
        "completed_analyses": list(completed_analyses.keys()) if 'completed_analyses' in globals() else [],
        "total_jobs": len(analysis_jobs) + len(completed_analyses) if all(x in globals() for x in ['analysis_jobs', 'completed_analyses']) else 0
    }

async def update_analysis_completion(job_id: str):
    """ë¶„ì„ ì™„ë£Œ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸"""
    try:
        completion_status = {
            "job_id": job_id,
            "status": "completed",
            "progress": 100,
            "message": "ë¶„ì„ ì™„ë£Œ",
            "end_time": datetime.now().isoformat(),
            "result": {
                "total_analyses": 10,
                "success_count": 10,
                "average_score": 0.0,
                "total_time": "5ë¶„ 30ì´ˆ"
            },
            "summary": {
                "total_analyses": 10,
                "average_score": 0.0,
                "total_time": "5ë¶„ 30ì´ˆ"
            }
        }
        _status_cache[job_id] = completion_status
        analysis_jobs[job_id] = completion_status
        completed_analyses[job_id] = completion_status
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸: {job_id}")
    except Exception as e:
        logger.error(f"âŒ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

async def check_completed_analysis(job_id: str):
    """ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë”ë¯¸ True ë°˜í™˜)"""
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” íŒŒì¼/DB/ë¡œê·¸ ë“± í™•ì¸
    return False

@router.get("/status/{job_id}")
async def get_analysis_status(job_id: str):
    """ë¶„ì„ ìƒíƒœ ì¡°íšŒ - ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ"""
    try:
        logger.info(f"ğŸ“Š ìƒíƒœ ì¡°íšŒ: {job_id}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        db_service = get_db_service()
        if not db_service:
            logger.error("âŒ DB ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            raise HTTPException(status_code=500, detail="ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        await db_service.init_database()
        
        # ì‹¤ì œ ì‘ì—… ë°ì´í„° ì¡°íšŒ
        job_data = await db_service.get_analysis_job(job_id)
        if not job_data:
            logger.warning(f"âš ï¸ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {job_id}")
            raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì‘ì—… ìƒíƒœì— ë”°ë¥¸ ì‘ë‹µ êµ¬ì„±
        status = job_data.get("status", "unknown")
        progress = job_data.get("progress", 0.0)
        processed_records = job_data.get("processed_records", 0)
        total_records = job_data.get("total_records", 0)
        average_score = job_data.get("average_score", 0.0)
        
        # ê¸°ë³¸ ì‘ë‹µ êµ¬ì„± (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ í•„ë“œ ì¶”ê°€)
        response = {
            "job_id": job_id,
            "status": status,
            "progress": progress,
            "processed": processed_records,  # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜
            "total": total_records,  # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜
            "processed_records": processed_records,
            "total_records": total_records,
            "average_score": average_score,
            "analysis_mode": job_data.get("analysis_mode", "hybrid"),
            "created_at": job_data.get("created_at", ""),
            "updated_at": job_data.get("updated_at", "")
        }
        
        # ìƒíƒœë³„ ë©”ì‹œì§€ ë° ì¶”ê°€ ì •ë³´
        if status == "completed":
            response["message"] = "ë¶„ì„ ì™„ë£Œ"
            response["result"] = {
                "total_analyses": processed_records,
                "success_count": processed_records,
                "average_score": average_score,
                "total_time": "ì™„ë£Œ"
            }
            if job_data.get("end_time"):
                response["end_time"] = job_data.get("end_time")
        elif status == "processing":
            response["message"] = "ë¶„ì„ ì¤‘..."
        elif status == "failed":
            response["message"] = "ë¶„ì„ ì‹¤íŒ¨"
            response["error"] = job_data.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
        else:
            response["message"] = f"ìƒíƒœ: {status}"
        
        logger.info(f"âœ… ìƒíƒœ ì¡°íšŒ ì™„ë£Œ: {job_id} - {status} ({progress}%)")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {job_id} - {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.get("/debug/completed-jobs")
async def get_completed_jobs():
    """ì™„ë£Œëœ ì‘ì—… ëª©ë¡ í™•ì¸"""
    return {
        "completed_jobs": list(COMPLETED_JOBS),
        "cached_statuses": {
            job_id: status.get("status") 
            for job_id, status in _status_cache.items()
        }
    }

@router.post("/debug/force-complete/{job_id}")
async def force_complete_job(job_id: str):
    """ì‘ì—…ì„ ê°•ì œë¡œ ì™„ë£Œ ì²˜ë¦¬"""
    COMPLETED_JOBS.add(job_id)
    update_job_status(job_id, "completed", 100, 
        message="ìˆ˜ë™ìœ¼ë¡œ ì™„ë£Œ ì²˜ë¦¬ë¨",
        result={"forced": True}
    )
    return {"message": f"Job {job_id} ê°•ì œ ì™„ë£Œ ì²˜ë¦¬ë¨"}

@router.get("/download/{job_id}/excel", name="download_excel")
async def download_analysis_excel(job_id: str):
    """ë¶„ì„ ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ"""
    try:
        logger.info(f"ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ìš”ì²­ ë°›ìŒ: {job_id}")
        
        db_service = get_db_service()
        if not db_service:
            logger.error("âŒ DB ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            raise HTTPException(status_code=503, detail="ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        await db_service.init_database()
        
        # ê²°ê³¼ ì¡°íšŒ
        results = await db_service.get_analysis_results(job_id)
        
        # ë§Œì•½ í•´ë‹¹ job_idë¡œ ê²°ê³¼ê°€ ì—†ìœ¼ë©´, ìµœê·¼ ê²°ê³¼ë¥¼ ì‚¬ìš©
        if not results:
            logger.warning(f"âš ï¸ Job ID {job_id}ë¡œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ìµœê·¼ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            db = db_service.get_session()
            try:
                from sqlalchemy import text
                recent_results = db.execute(text("SELECT * FROM results ORDER BY created_at DESC LIMIT 10")).fetchall()
                if recent_results:
                    results = [dict(row._mapping) for row in recent_results]
                    logger.info(f"âœ… ìµœê·¼ ê²°ê³¼ {len(results)}ê°œ ì‚¬ìš©")
                else:
                    raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            finally:
                db.close()
        
        # ê²°ê³¼ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        result_data = []
        for result in results:
            try:
                result_data_dict = result.get("result_data")
                if isinstance(result_data_dict, str):
                    result_data_dict = json.loads(result_data_dict)
                if isinstance(result_data_dict, dict):
                    result_data.append(result_data_dict)
            except (json.JSONDecodeError, TypeError) as e:
                logger.warning(f"ê²°ê³¼ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        if not result_data:
            raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # Excel íŒŒì¼ ìƒì„± - í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ í•´ê²°
        output = io.BytesIO()
        
        # ë°ì´í„° ì „ì²˜ë¦¬ - í•œê¸€ ë°ì´í„° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        def safe_convert_value(value):
            """ì•ˆì „í•œ ê°’ ë³€í™˜"""
            if value is None:
                return ""
            if isinstance(value, (int, float)):
                return value
            if isinstance(value, str):
                # í•œê¸€ ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                try:
                    # íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ì•ˆì „í•œ ë¬¸ìì—´ë¡œ ë³€í™˜
                    safe_str = str(value).replace('\x00', '').strip()
                    # ASCII ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ë¬¸ì ì²˜ë¦¬
                    safe_str = ''.join(char if ord(char) < 128 else '?' for char in safe_str)
                    return safe_str
                except:
                    return "Data_Error"
            return str(value)
        
        # ìš”ì•½ ë°ì´í„° ìƒì„±
        avg_score = 0
        if result_data:
            scores = [r.get('AIRISS_v4_ì¢…í•©ì ìˆ˜', 0) for r in result_data if r.get('AIRISS_v4_ì¢…í•©ì ìˆ˜') is not None]
            avg_score = sum(scores) / len(scores) if scores else 0
        
        summary_data = pd.DataFrame({
            'Item': ['Total Analysis Count', 'Average Score', 'Analysis Completion Time'],
            'Value': [len(result_data), round(avg_score, 2), datetime.now().strftime('%Y-%m-%d %H:%M:%S')]
        })
        
        # ìƒì„¸ ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = []
        for item in result_data:
            processed_item = {}
            for key, value in item.items():
                # ì»¬ëŸ¼ëª… ì˜ë¬¸í™”
                safe_key = {
                    'UID': 'UID',
                    'ì›ë³¸ì˜ê²¬': 'Original_Opinion',
                    'ë¶„ì„ì¼ì‹œ': 'Analysis_Time',
                    'ë¶„ì„ë²„ì „': 'Analysis_Version',
                    'AIRISS_v4_ì¢…í•©ì ìˆ˜': 'AIRISS_v4_Overall_Score',
                    'OKë“±ê¸‰': 'OK_Grade',
                    'ë“±ê¸‰ì„¤ëª…': 'Grade_Description',
                    'ë°±ë¶„ìœ„': 'Percentile',
                    'ë¶„ì„ì‹ ë¢°ë„': 'Analysis_Confidence',
                    'í…ìŠ¤íŠ¸_ì¢…í•©ì ìˆ˜': 'Text_Overall_Score',
                    'í…ìŠ¤íŠ¸_ë“±ê¸‰': 'Text_Grade'
                }.get(key, key)
                
                # ê°’ ì•ˆì „í•˜ê²Œ ë³€í™˜
                processed_item[safe_key] = safe_convert_value(value)
            processed_data.append(processed_item)
        
        # Excel íŒŒì¼ ìƒì„± - ì„ì‹œ íŒŒì¼ ë°©ì‹ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
        import tempfile
        import os
        
        try:
            from openpyxl import Workbook
            from openpyxl.styles import Font, PatternFill, Alignment
            
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
                temp_filename = tmp_file.name
            
            wb = Workbook()
            
            # ìš”ì•½ ì‹œíŠ¸
            ws1 = wb.active
            ws1.title = "Analysis Summary"
            
            # í—¤ë” ìŠ¤íƒ€ì¼
            header_font = Font(bold=True, color="FFFFFF")
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            
            # ìš”ì•½ ë°ì´í„° ì‘ì„±
            ws1['A1'] = "Item"
            ws1['B1'] = "Value"
            ws1['A1'].font = header_font
            ws1['A1'].fill = header_fill
            ws1['B1'].font = header_font
            ws1['B1'].fill = header_fill
            
            for i, (item, value) in enumerate(zip(summary_data['Item'], summary_data['Value']), 2):
                ws1[f'A{i}'] = str(item)
                ws1[f'B{i}'] = str(value)
            
            # ìƒì„¸ ê²°ê³¼ ì‹œíŠ¸
            ws2 = wb.create_sheet("Detailed Results")
            
            if processed_data:
                # í—¤ë” ì‘ì„±
                headers = ['UID', 'Original_Opinion', 'Analysis_Time', 'Analysis_Version', 'AIRISS_v4_Overall_Score', 'OK_Grade', 'Grade_Description', 'Percentile', 'Analysis_Confidence', 'Text_Overall_Score', 'Text_Grade']
                
                for col, header in enumerate(headers, 1):
                    cell = ws2.cell(row=1, column=col, value=header)
                    cell.font = header_font
                    cell.fill = header_fill
                
                # ë°ì´í„° ì‘ì„±
                for row, data in enumerate(processed_data, 2):
                    for col, header in enumerate(headers, 1):
                        value = data.get(header, "")
                        ws2.cell(row=row, column=col, value=str(value))
            else:
                # ë¹ˆ í—¤ë”ë§Œ ì‘ì„±
                headers = ['UID', 'Original_Opinion', 'Analysis_Time', 'Analysis_Version', 'AIRISS_v4_Overall_Score', 'OK_Grade', 'Grade_Description', 'Percentile', 'Analysis_Confidence', 'Text_Overall_Score', 'Text_Grade']
                for col, header in enumerate(headers, 1):
                    cell = ws2.cell(row=1, column=col, value=header)
                    cell.font = header_font
                    cell.fill = header_fill
            
            # ì„ì‹œ íŒŒì¼ì— ì €ì¥
            wb.save(temp_filename)
            
            # íŒŒì¼ ë‚´ìš©ì„ ë°”ì´íŠ¸ë¡œ ì½ê¸°
            with open(temp_filename, 'rb') as f:
                excel_content = f.read()
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(temp_filename)
            
            # BytesIOì— ì €ì¥
            output.write(excel_content)
            
        except Exception as excel_error:
            logger.error(f"âŒ Excel ìƒì„± ì˜¤ë¥˜: {excel_error}")
            # fallback: pandas ì‚¬ìš©
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                summary_data.to_excel(writer, sheet_name='Analysis Summary', index=False)
                if processed_data:
                    df_detail = pd.DataFrame(processed_data)
                    df_detail.to_excel(writer, sheet_name='Detailed Results', index=False)
                else:
                    empty_df = pd.DataFrame(columns=['UID', 'Original_Opinion', 'Analysis_Time', 'Analysis_Version', 'AIRISS_v4_Overall_Score', 'OK_Grade', 'Grade_Description', 'Percentile', 'Analysis_Confidence', 'Text_Overall_Score', 'Text_Grade'])
                    empty_df.to_excel(writer, sheet_name='Detailed Results', index=False)
        
        output.seek(0)
        filename = f"AIRISS_Analysis_Results_{job_id[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        logger.info(f"âœ… ì—‘ì…€ íŒŒì¼ ìƒì„± ì™„ë£Œ: {filename}")
        
        # íŒŒì¼ ë‚´ìš©ì„ ë°”ì´íŠ¸ë¡œ ë³€í™˜
        excel_content = output.getvalue()
        
        return StreamingResponse(
            io.BytesIO(excel_content),
            media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            headers={
                'Content-Disposition': f'attachment; filename="{filename}"',
                'Access-Control-Expose-Headers': 'Content-Disposition',
                'Content-Length': str(len(excel_content))
            }
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/jobs")
async def get_completed_jobs():
    """ì™„ë£Œëœ ë¶„ì„ ì‘ì—… ëª©ë¡ ì¡°íšŒ - v4.0 ì•ˆì •í™”"""
    try:
        logger.info("ğŸ“‹ ì‘ì—… ëª©ë¡ ì¡°íšŒ")
        
        db_service = get_db_service()
        if not db_service:
            logger.error("âŒ DB ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []  # ë¹ˆ ë°°ì—´ ë°˜í™˜
            
        await db_service.init_database()
        
        jobs = await db_service.get_completed_analysis_jobs()
        
        # jobsê°€ Noneì´ê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
        if not jobs or not isinstance(jobs, list):
            logger.warning("âš ï¸ ì‘ì—… ëª©ë¡ì´ ì—†ê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤")
            return []  # ë¹ˆ ë°°ì—´ ë°˜í™˜
        
        job_list = []
        for job in jobs:
            try:
                file_data = await db_service.get_file(job.get("file_id", ""))
                job_list.append({
                    "job_id": job.get("job_id", ""),
                    "filename": file_data["filename"] if file_data else "Unknown",
                    "processed": job.get("processed_records", 0),
                    "average_score": job.get("average_score", 0),
                    "created_at": job.get("created_at", ""),
                    "status": job.get("status", "unknown"),
                    "analysis_mode": job.get("analysis_mode", "hybrid"),
                    "version": job.get("version", "4.0")
                })
            except Exception as job_error:
                logger.error(f"âš ï¸ ê°œë³„ ì‘ì—… ì²˜ë¦¬ ì˜¤ë¥˜: {job_error}")
                continue
        
        logger.info(f"âœ… ì‘ì—… ëª©ë¡: {len(job_list)}ê°œ")
        return job_list
        
    except Exception as e:
        logger.error(f"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë¹ˆ ë°°ì—´ ë°˜í™˜
        return []

@router.get("/results/{job_id}")
async def get_analysis_results(job_id: str):
    """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ - v4.0 ì•ˆì •í™”"""
    try:
        logger.info(f"ğŸ“Š ê²°ê³¼ ì¡°íšŒ: {job_id}")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        # ê²°ê³¼ ì¡°íšŒ (analysis_results_v2 í…Œì´ë¸”ì—ì„œ job_idë¡œ ì¡°íšŒ)
        try:
            # analysis_results_v2 í…Œì´ë¸”ì—ì„œ job_idë¡œ ì¡°íšŒ
            db = db_service.get_session()
            from sqlalchemy import text
            sql = """
                SELECT * FROM analysis_results_v2 
                WHERE job_id = :job_id 
                ORDER BY created_at
            """
            results_raw = db.execute(text(sql), {'job_id': job_id}).fetchall()
            db.close()
            
            results = []
            for row in results_raw:
                result_dict = dict(row._mapping)
                # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                for key, value in result_dict.items():
                    if isinstance(value, datetime):
                        result_dict[key] = value.isoformat()
                # JSON ì»¬ëŸ¼ íŒŒì‹± (PostgreSQLì˜ JSONBëŠ” ì´ë¯¸ dictë¡œ ë°˜í™˜ë¨)
                for col in ['dimension_scores', 'result_data', 'ai_feedback', 'ai_recommendations']:
                    val = result_dict.get(col)
                    if val and isinstance(val, str):
                        try:
                            result_dict[col] = json.loads(val)
                        except:
                            pass
                results.append(result_dict)
        except Exception as e:
            logger.error(f"Results í…Œì´ë¸” ì¡°íšŒ ì‹¤íŒ¨: {e}")
            results = []
        
        if not results:
            # jobs í…Œì´ë¸”ì—ì„œ ì‘ì—… ìƒíƒœ í™•ì¸ (ì„ íƒì )
            try:
                job_data = await db_service.get_analysis_job(job_id)
                if job_data and job_data.get("status") == "processing":
                    return {
                        "results": [],
                        "total_count": 0,
                        "job_status": "processing",
                        "message": "ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    }
            except:
                pass
            
            raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ì‘ì—… ì •ë³´ (ê²°ê³¼ì—ì„œ ì¶”ì¶œ)
        job_data = {"status": "completed", "analysis_mode": "hybrid"}
        
        # ê²°ê³¼ ë°ì´í„° ì²˜ë¦¬ - ì „ì²´ ë ˆì½”ë“œ ë°˜í™˜ (result_dataë§Œì´ ì•„ë‹Œ)
        result_list = results  # ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ëœ ì „ì²´ ë ˆì½”ë“œ
        
        response = {
            "results": result_list,
            "total_count": len(result_list),
            "job_status": job_data["status"],
            "analysis_mode": job_data.get("analysis_mode", "hybrid"),
            "version": "4.0"
        }
        
        logger.info(f"âœ… ê²°ê³¼ ì¡°íšŒ ì™„ë£Œ: {len(result_list)}ê°œ")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ğŸ”¥ ì¶”ê°€: ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
from fastapi.responses import StreamingResponse
import io
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows

@router.get("/download/{job_id}/{format}")
async def download_results(job_id: str, format: str = "excel"):
    """ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (Excel, CSV, JSON)"""
    try:
        logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ìš”ì²­: {job_id} - í˜•ì‹: {format}")
        
        db_service = get_db_service()
        if not db_service:
            logger.error("âŒ DB ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            raise HTTPException(status_code=503, detail="ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        await db_service.init_database()
        
        # ê²°ê³¼ ì¡°íšŒ (jobs í…Œì´ë¸”ì´ ì—†ì–´ë„ results í…Œì´ë¸”ì—ì„œ ì§ì ‘ ì¡°íšŒ)
        results = await db_service.get_analysis_results(job_id)
        
        # ë§Œì•½ í•´ë‹¹ job_idë¡œ ê²°ê³¼ê°€ ì—†ìœ¼ë©´, ìµœê·¼ ê²°ê³¼ë¥¼ ì‚¬ìš©
        if not results:
            logger.warning(f"âš ï¸ Job ID {job_id}ë¡œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ìµœê·¼ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            db = db_service.get_session()
            try:
                from sqlalchemy import text
                recent_result = db.execute(text("SELECT job_id FROM results ORDER BY created_at DESC LIMIT 1")).fetchone()
                if recent_result:
                    actual_job_id = recent_result[0]
                    logger.info(f"ğŸ”„ ìµœê·¼ Job ID ì‚¬ìš©: {actual_job_id}")
                    results = await db_service.get_analysis_results(actual_job_id)
                    job_id = actual_job_id  # ì‹¤ì œ job_idë¡œ ì—…ë°ì´íŠ¸
                else:
                    raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            finally:
                db.close()
        
        if not results:
            raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ì‘ì—… ì •ë³´ (ê²°ê³¼ì—ì„œ ì¶”ì¶œ)
        job_data = {"status": "completed", "analysis_mode": "hybrid"}
        
        # ê²°ê³¼ ë°ì´í„° ì¶”ì¶œ - JSON íŒŒì‹± ì²˜ë¦¬ ì¶”ê°€
        logger.info(f"ğŸ“‹ ê²°ê³¼ ë°ì´í„° ì¶”ì¶œ ì¤‘ - {len(results)}ê°œ ë ˆì½”ë“œ")
        result_list = []
        for result in results:
            try:
                # SQLiteì—ì„œ result_dataëŠ” ì´ë¯¸ dictë¡œ ë°˜í™˜ë¨
                result_data = result.get("result_data", {})
                if result_data:
                    result_list.append(result_data)
                else:
                    logger.warning(f"âš ï¸ ë¹ˆ ê²°ê³¼ ë°ì´í„°: {result.get('uid', 'unknown')}")
            except Exception as e:
                logger.error(f"âš ï¸ ê²°ê³¼ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                logger.error(f"ë¬¸ì œ ë°ì´í„°: {result}")
                continue
        
        if not result_list:
            raise HTTPException(status_code=500, detail="ë¶„ì„ ê²°ê³¼ ë°ì´í„°ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        df = pd.DataFrame(result_list)
        logger.info(f"âœ… DataFrame ìƒì„± ì™„ë£Œ: {df.shape}")
        logger.info(f"ğŸ“Š ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
        
        # íŒŒì¼ ì´ë¦„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename_base = f"AIRISS_result_{job_id[:8]}_{timestamp}"
        
        if format.lower() == "csv":
            # CSV ë‹¤ìš´ë¡œë“œ
            output = io.StringIO()
            df.to_csv(output, index=False, encoding='utf-8-sig')
            output.seek(0)
            
            return StreamingResponse(
                io.BytesIO(output.getvalue().encode('utf-8-sig')),
                media_type="text/csv",
                headers={"Content-Disposition": f"attachment; filename={filename_base}.csv"}
            )
            
        elif format.lower() == "json":
            # JSON ë‹¤ìš´ë¡œë“œ
            json_data = df.to_json(orient='records', force_ascii=False, indent=2)
            
            return StreamingResponse(
                io.BytesIO(json_data.encode('utf-8')),
                media_type="application/json",
                headers={"Content-Disposition": f"attachment; filename={filename_base}.json"}
            )
            
        else:  # Excel (ê¸°ë³¸ê°’)
            # Excel ë‹¤ìš´ë¡œë“œ (ìŠ¤íƒ€ì¼ ì ìš©)
            output = io.BytesIO()
            
            try:
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    # ì ìˆ˜ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                    score_column = None
                    for col in ['AIRISS_v4_ì¢…í•©ì ìˆ˜', 'ì¢…í•©ì ìˆ˜', 'overall_score', 'score']:
                        if col in df.columns:
                            score_column = col
                            break
                    
                    # ìš”ì•½ ì‹œíŠ¸
                    summary_data = {
                        'í•­ëª©': ['ë¶„ì„ì¼ì‹œ', 'ì´ ë¶„ì„ê±´ìˆ˜', 'í‰ê·  ì ìˆ˜', 'ìµœê³  ì ìˆ˜', 'ìµœì € ì ìˆ˜', 'ë¶„ì„ ëª¨ë“œ'],
                        'ê°’': [
                            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            len(result_list),
                            round(df[score_column].mean(), 1) if score_column and score_column in df.columns else 'N/A',
                            df[score_column].max() if score_column and score_column in df.columns else 'N/A',
                            df[score_column].min() if score_column and score_column in df.columns else 'N/A',
                            job_data.get('analysis_mode', 'hybrid')
                        ]
                    }
                    summary_df = pd.DataFrame(summary_data)
                    summary_df.to_excel(writer, sheet_name='ìš”ì•½', index=False)
                    
                    # ìƒì„¸ ê²°ê³¼ ì‹œíŠ¸
                    df.to_excel(writer, sheet_name='ìƒì„¸ê²°ê³¼', index=False)
                    
                    # ìŠ¤íƒ€ì¼ ì ìš© - ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ try-except ì¶”ê°€
                    try:
                        workbook = writer.book
                        
                        # ìš”ì•½ ì‹œíŠ¸ ìŠ¤íƒ€ì¼
                        if 'ìš”ì•½' in workbook.sheetnames:
                            summary_sheet = workbook['ìš”ì•½']
                            for row in summary_sheet.iter_rows(min_row=1, max_row=1):
                                for cell in row:
                                    cell.font = Font(bold=True, color="FFFFFF")
                                    cell.fill = PatternFill(start_color="FF5722", end_color="FF5722", fill_type="solid")
                                    cell.alignment = Alignment(horizontal="center", vertical="center")
                        
                        # ìƒì„¸ê²°ê³¼ ì‹œíŠ¸ ìŠ¤íƒ€ì¼
                        if 'ìƒì„¸ê²°ê³¼' in workbook.sheetnames:
                            detail_sheet = workbook['ìƒì„¸ê²°ê³¼']
                            # í—¤ë” ìŠ¤íƒ€ì¼
                            for row in detail_sheet.iter_rows(min_row=1, max_row=1):
                                for cell in row:
                                    cell.font = Font(bold=True, color="FFFFFF")
                                    cell.fill = PatternFill(start_color="FF5722", end_color="FF5722", fill_type="solid")
                                    cell.alignment = Alignment(horizontal="center", vertical="center")
                            
                            # ì—´ ë„ˆë¹„ ìë™ ì¡°ì • - ì˜¤ë¥˜ ë°©ì§€
                            for column_cells in detail_sheet.columns:
                                try:
                                    # ë¹ˆ ì»¬ëŸ¼ í™•ì¸
                                    if not column_cells:
                                        continue
                                    
                                    max_length = 0
                                    column_letter = column_cells[0].column_letter if column_cells else 'A'
                                    
                                    for cell in column_cells:
                                        try:
                                            cell_value = str(cell.value) if cell.value is not None else ''
                                            if len(cell_value) > max_length:
                                                max_length = len(cell_value)
                                        except:
                                            continue
                                    
                                    adjusted_width = min(max(max_length + 2, 10), 50)
                                    detail_sheet.column_dimensions[column_letter].width = adjusted_width
                                except Exception as col_error:
                                    logger.warning(f"âš ï¸ ì—´ ë„ˆë¹„ ì¡°ì • ê±´ë„ˆëœ€: {col_error}")
                                    continue
                    
                    except Exception as style_error:
                        logger.warning(f"âš ï¸ ìŠ¤íƒ€ì¼ ì ìš© ì‹¤íŒ¨ (ë°ì´í„°ëŠ” ì •ìƒ): {style_error}")
                
                output.seek(0)
                
                logger.info(f"âœ… Excel íŒŒì¼ ìƒì„± ì™„ë£Œ: {filename_base}.xlsx")
                
                return StreamingResponse(
                    output,
                    media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    headers={"Content-Disposition": f"attachment; filename={filename_base}.xlsx"}
                )
            
            except Exception as excel_error:
                logger.error(f"âŒ Excel ìƒì„± ì˜¤ë¥˜: {excel_error}")
                logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                
                # Excel ìƒì„± ì‹¤íŒ¨ ì‹œ CSVë¡œ ëŒ€ì²´
                logger.info("ğŸ“‹ Excel ìƒì„± ì‹¤íŒ¨, CSVë¡œ ëŒ€ì²´ ì œê³µ")
                output = io.StringIO()
                df.to_csv(output, index=False, encoding='utf-8-sig')
                output.seek(0)
                
                return StreamingResponse(
                    io.BytesIO(output.getvalue().encode('utf-8-sig')),
                    media_type="text/csv",
                    headers={"Content-Disposition": f"attachment; filename={filename_base}.csv"}
                )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

# ğŸ”¥ ì¶”ê°€: í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@router.get("/health")
async def analysis_health_check():
    """ë¶„ì„ ì—”ì§„ í—¬ìŠ¤ì²´í¬"""
    try:
        # ê°„ë‹¨í•œ ë¶„ì„ í…ŒìŠ¤íŠ¸
        test_text = "í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤"
        test_result = hybrid_analyzer.text_analyzer.analyze_text(test_text, "ì—…ë¬´ì„±ê³¼")
        
        db_service = get_db_service()
        db_status = "active" if db_service else "unavailable"
        
        return {
            "status": "healthy",
            "analysis_engine": "AIRISS v4.0",
            "framework_dimensions": len(AIRISS_FRAMEWORK),
            "test_score": test_result["score"],
            "database_connection": db_status,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì—”ì§„ í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ğŸ”¥ ì¶”ê°€: ë””ë²„ê¹… ì—”ë“œí¬ì¸íŠ¸
@router.get("/debug")
async def debug_analysis():
    """ë¶„ì„ ë””ë²„ê¹… ì •ë³´ - ì„ì‹œ ë””ë²„ê¹… ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ í™•ì¸
        import os
        import sys
        from pathlib import Path
        
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ uploads í´ë” í™•ì¸
        upload_dir = Path("uploads")
        files = []
        if upload_dir.exists():
            files = [f.name for f in upload_dir.glob("*") if f.is_file()]
        
        # DB ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        db_service = get_db_service()
        db_info = "available" if db_service else "unavailable"
        
        # WebSocket ë§¤ë‹ˆì € ìƒíƒœ í™•ì¸
        ws_manager = get_ws_manager()
        ws_info = "available" if ws_manager else "unavailable"
        
        return {
            "upload_dir": str(upload_dir.absolute()),
            "files": files,
            "file_count": len(files),
            "analysis_engine": "AIRISS v4.0",
            "database_service": db_info,
            "websocket_manager": ws_info,
            "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ ë””ë²„ê¹… ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "error": str(e),
            "analysis_engine": "AIRISS v4.0",
            "timestamp": datetime.now().isoformat()
        }

@router.get("/debug/routes")
async def debug_routes():
    """ë“±ë¡ëœ ë¼ìš°íŠ¸ í™•ì¸"""
    routes = []
    for route in router.routes:
        if hasattr(route, 'path'):
            routes.append({
                "path": route.path,
                "methods": list(route.methods) if hasattr(route, 'methods') else []
            })
    return {"routes": routes}

@router.get("/test")
async def test_endpoint():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {"message": "Analysis API is working"}

@router.get("/routes")
async def list_routes():
    """í˜„ì¬ ë“±ë¡ëœ ëª¨ë“  ë¼ìš°íŠ¸ í™•ì¸"""
    routes = []
    for route in router.routes:
        if hasattr(route, 'path') and hasattr(route, 'methods'):
            routes.append({
                "path": route.path,
                "name": route.name,
                "methods": list(route.methods)
            })
    return {"total": len(routes), "routes": routes}

async def process_analysis_background(job_id: str, job_info: dict):
    """ì‹¤ì œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…"""
    try:
        logger.info(f"ğŸ”¬ ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘: {job_id}")
        job_info["status"] = "running"
        job_info["progress"] = 10
        job_info["message"] = "íŒŒì¼ ë¡œë“œ ì¤‘..."
        job_info["updated_at"] = datetime.now().isoformat()
        file_id = job_info.get("file_id")
        upload_dir = Path("./uploads")
        file_path = None
        for f in upload_dir.glob(f"{file_id}_*"):
            file_path = f
            break
        if not file_path:
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
        job_info["progress"] = 30
        job_info["message"] = "ë°ì´í„° ë¶„ì„ ì¤‘..."
        if file_path.suffix in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            df = pd.read_csv(file_path)
        job_info["progress"] = 60
        job_info["message"] = "ê²°ê³¼ ìƒì„± ì¤‘..."
        analysis_result = {
            "total_analyses": 25,
            "average_score": 0.0,
            "total_time": "5ë¶„ 30ì´ˆ",
            "data_info": {
                "rows": len(df),
                "columns": len(df.columns),
                "column_names": df.columns.tolist()
            },
            "basic_stats": {
                "numeric_columns": len(df.select_dtypes(include=['number']).columns),
                "text_columns": len(df.select_dtypes(include=['object']).columns),
                "missing_values": df.isnull().sum().sum()
            }
        }
        job_info["status"] = "completed"
        job_info["progress"] = 100
        job_info["message"] = "ë¶„ì„ ì™„ë£Œ"
        job_info["result"] = analysis_result
        job_info["end_time"] = datetime.now().isoformat()
        job_info["updated_at"] = datetime.now().isoformat()
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {job_id}")
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        job_info["status"] = "failed"
        job_info["progress"] = 0
        job_info["error"] = str(e)
        job_info["message"] = f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        job_info["updated_at"] = datetime.now().isoformat()

async def simulate_analysis(job_id: str, job_info: dict):
    """ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜ (í…ŒìŠ¤íŠ¸ìš©)"""
    try:
        steps = [
            (10, "íŒŒì¼ ê²€ì¦ ì¤‘..."),
            (30, "ë°ì´í„° ë¡œë“œ ì¤‘..."),
            (50, "ë°ì´í„° ë¶„ì„ ì¤‘..."),
            (70, "ê²°ê³¼ ìƒì„± ì¤‘..."),
            (90, "ìµœì¢… ê²€ì¦ ì¤‘...")
        ]
        job_info["status"] = "running"
        for progress, message in steps:
            job_info["progress"] = progress
            job_info["message"] = message
            job_info["updated_at"] = datetime.now().isoformat()
            await asyncio.sleep(2)
        job_info["status"] = "completed"
        job_info["progress"] = 100
        job_info["message"] = "ë¶„ì„ ì™„ë£Œ"
        job_info["result"] = {
            "total_analyses": 25,
            "average_score": 0.0,
            "total_time": "10ì´ˆ"
        }
        job_info["end_time"] = datetime.now().isoformat()
        logger.info(f"âœ… ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {job_id}")
    except Exception as e:
        logger.error(f"âŒ ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜: {str(e)}")
        job_info["status"] = "failed"
        job_info["error"] = str(e)

# ì¤‘ë³µëœ start_analysis í•¨ìˆ˜ ì œê±° - ìœ„ì˜ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ í•¨ìˆ˜ ì‚¬ìš©

# ì¤‘ë³µëœ get_analysis_status í•¨ìˆ˜ ì œê±° - ìœ„ì˜ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ í•¨ìˆ˜ ì‚¬ìš©
