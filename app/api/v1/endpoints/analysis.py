# app/api/v1/endpoints/analysis.py
"""
AIRISS v4.0 ë¶„ì„ API ì—”ë“œí¬ì¸íŠ¸
ì‹¤ì‹œê°„ WebSocket í†µí•©
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional
import io
import logging

from app.core.websocket_manager import ConnectionManager, manager
from app.services.analysis_service import AnalysisService
import json
from app.exceptions import (
    AnalysisError,
    FileProcessingError,
    ValidationError,
    ResourceNotFoundError,
    InternalServiceError
)

router = APIRouter()

# ë¶„ì„ ìš”ì²­ ëª¨ë¸
class AnalysisRequest(BaseModel):
    sample_size: int = 10
    analysis_mode: str = "hybrid"
    enable_ai_feedback: bool = False
    openai_api_key: Optional[str] = None
    openai_model: str = "gpt-3.5-turbo"
    max_tokens: int = 1200

# ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ í•¨ìˆ˜
def get_ws_manager():
    """WebSocket ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return manager

# ì‹±ê¸€í†¤ ë¶„ì„ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_analysis_service = None

def get_analysis_service(ws_manager: ConnectionManager = Depends(get_ws_manager)):
    """ë¶„ì„ ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _analysis_service
    if _analysis_service is None:
        _analysis_service = AnalysisService(ws_manager)
    return _analysis_service

@router.post("/upload")
async def upload_file(
    file: UploadFile = File(...),
    service: AnalysisService = Depends(get_analysis_service)
):
    """íŒŒì¼ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸"""
    logger = logging.getLogger(__name__)
    
    try:
        contents = await file.read()
        result = await service.upload_file(contents, file.filename)
        return result
        
    except ValidationError as e:
        logger.error(f"ValidationError in upload: {e.message}")
        raise HTTPException(status_code=e.status_code, detail=e.message)
    except FileProcessingError as e:
        logger.error(f"FileProcessingError in upload: {e.message}")
        raise HTTPException(status_code=e.status_code, detail=e.message)
    except ResourceNotFoundError as e:
        logger.error(f"ResourceNotFoundError in upload: {e.message}")
        raise HTTPException(status_code=e.status_code, detail=e.message)
    except AnalysisError as e:
        logger.error(f"AnalysisError in upload: {e.message}")
        raise HTTPException(status_code=e.status_code, detail=e.message)
    except Exception as e:
        logger.error(f"Unexpected error in upload: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.post("/analyze/{file_id}")
async def start_analysis(
    file_id: str,
    request: AnalysisRequest,
    service: AnalysisService = Depends(get_analysis_service)
):
    """ë¶„ì„ ì‹œì‘ ì—”ë“œí¬ì¸íŠ¸ - JSON bodyë¡œ íŒŒë¼ë¯¸í„° ë°›ê¸°"""
    logger = logging.getLogger(__name__)
    
    try:
        # [DEBUG] ë¶„ì„ íŠ¸ë¦¬ê±° API ê²€ì¦
        logger.info("=" * 60)
        logger.info("[API] ë¶„ì„ ìš”ì²­ ìˆ˜ì‹ ")
        logger.info(f"[API] URL path file_id: {file_id}")
        logger.info(f"[API] Request body: {request.dict()}")
        
        # file_idê°€ request bodyì—ë„ ìˆë‹¤ë©´ 1:1 ì¼ì¹˜ í™•ì¸
        if hasattr(request, 'file_id') and request.file_id:
            if file_id != request.file_id:
                logger.error(f"[ERROR] file_id ë¶ˆì¼ì¹˜! URL: {file_id}, Body: {request.file_id}")
                raise HTTPException(status_code=400, detail="file_idê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        logger.info(f"[API] ë¶„ì„ ì„œë¹„ìŠ¤ë¡œ ì „ë‹¬í•  file_id: {file_id}")
        logger.info("=" * 60)
        
        job_id = await service.start_analysis(
            file_id=file_id,
            sample_size=request.sample_size,
            analysis_mode=request.analysis_mode,
            enable_ai_feedback=request.enable_ai_feedback,
            openai_api_key=request.openai_api_key,
            openai_model=request.openai_model,
            max_tokens=request.max_tokens
        )
        return {"job_id": job_id, "status": "started", "message": "ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤"}
        
    except ResourceNotFoundError as e:
        logger.error(f"ResourceNotFoundError in analyze: {e.message}")
        raise HTTPException(status_code=e.status_code, detail=e.message)
    except ValidationError as e:
        logger.error(f"ValidationError in analyze: {e.message}")
        raise HTTPException(status_code=e.status_code, detail=e.message)
    except AnalysisError as e:
        logger.error(f"AnalysisError in analyze: {e.message}")
        raise HTTPException(status_code=e.status_code, detail=e.message)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error in analyze: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/status/{job_id}")
async def get_job_status(
    job_id: str,
    service: AnalysisService = Depends(get_analysis_service)
):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    try:
        status = service.get_job_status(job_id)
        if status is None:
            raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return status
    except HTTPException:
        raise
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"Unexpected error in get_job_status: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/jobs")
async def get_analysis_jobs(
    service: AnalysisService = Depends(get_analysis_service)
):
    """ëª¨ë“  ë¶„ì„ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    logger = logging.getLogger(__name__)
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‘ì—… ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        db = service._get_db()
        try:
            from app.models.job import Job
            jobs_db = db.query(Job).order_by(Job.created_at.desc()).all()
            
            jobs = []
            for job in jobs_db:
                # ê²°ê³¼ ë°ì´í„° íŒŒì‹±
                result = None
                if job.results_data:
                    try:
                        result = json.loads(job.results_data) if isinstance(job.results_data, str) else job.results_data
                    except:
                        result = None
                
                job_info = {
                    "id": job.id,
                    "status": job.status,
                    "created_at": job.created_at.isoformat() if job.created_at else "",
                    "filename": job.filename or "",
                    "file_id": job.file_id or "",
                    "result": result
                }
                jobs.append(job_info)
            
            return jobs
        finally:
            db.close()
    except Exception as e:
        logger.error(f"Error getting jobs list: {str(e)}")
        return []

@router.get("/results/{job_id}")
async def get_results(
    job_id: str,
    service: AnalysisService = Depends(get_analysis_service)
):
    """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ“Š Results requested for job: {job_id}")
    
    # Get job from database to return full result structure
    db = service._get_db()
    try:
        from app.models.job import Job
        job = db.query(Job).filter(Job.id == job_id).first()
        
        if not job:
            logger.error(f"âŒ Job not found: {job_id}")
            raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if job.status != 'completed':
            logger.info(f"â³ Job not completed: {job_id}, status: {job.status}")
            # ì™„ë£Œë˜ì§€ ì•Šì€ ì‘ì—…ë„ í˜„ì¬ ìƒíƒœë¥¼ ë°˜í™˜
            return {
                "status": job.status,
                "message": f"ì‘ì—…ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤: {job.status}",
                "analysis_results": [],
                "data": [],
                "summary": {}
            }
        
        if job.results_data:
            # Return the full results structure
            results = json.loads(job.results_data)
            # ë¶„ì„ ê²°ê³¼ëŠ” 'data' í•„ë“œì— ì €ì¥ë¨
            analysis_results = results.get('data', [])
            logger.info(f"âœ… Returning results for job {job_id}: {len(analysis_results)} results")
            
            # ì²« ë²ˆì§¸ ê²°ê³¼ ë¡œê·¸ë¡œ í™•ì¸
            if analysis_results and len(analysis_results) > 0:
                logger.info(f"ğŸ“‹ ì²« ë²ˆì§¸ ë¶„ì„ ê²°ê³¼ ìƒ˜í”Œ:")
                logger.info(f"  - UID: {analysis_results[0].get('uid')}")
                logger.info(f"  - Name: {analysis_results[0].get('name')}")
                logger.info(f"  - Score: {analysis_results[0].get('score')}")
                logger.info(f"  - Grade: {analysis_results[0].get('grade')}")
            
            # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ analysis_results í•„ë“œë„ ì¶”ê°€
            if 'analysis_results' not in results and analysis_results:
                results['analysis_results'] = analysis_results
            
            return results
        else:
            logger.warning(f"âš ï¸ No results data for job: {job_id}")
            return {"analysis_results": [], "data": [], "summary": {}}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error getting results: {e}")
        raise HTTPException(status_code=500, detail="ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
    finally:
        db.close()

@router.get("/check-results/{job_id}")
async def check_results_availability(
    job_id: str,
    service: AnalysisService = Depends(get_analysis_service)
):
    """ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    try:
        # ì‘ì—… ìƒíƒœ í™•ì¸
        job_status = service.get_job_status(job_id)
        if not job_status:
            return {"available": False, "reason": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        if job_status.get('status') != 'completed':
            return {"available": False, "reason": "ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        
        # ê²°ê³¼ ë°ì´í„° í™•ì¸
        df = await service.get_analysis_results(job_id)
        if df is None or df.empty:
            return {"available": False, "reason": "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ê° í˜•ì‹ë³„ íŒŒì¼ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        formats_available = {}
        for format_type in ["excel", "csv", "json"]:
            try:
                test_data = await service.export_results(job_id, format_type)
                formats_available[format_type] = test_data is not None
            except:
                formats_available[format_type] = False
        
        return {
            "available": True,
            "job_id": job_id,
            "status": job_status.get('status'),
            "formats": formats_available,
            "record_count": len(df)
        }
    except Exception as e:
        return {"available": False, "reason": str(e)}

@router.get("/download/{job_id}/{format}")
async def download_results(
    job_id: str,
    format: str,
    service: AnalysisService = Depends(get_analysis_service)
):
    """ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"""
    data = await service.export_results(job_id, format)
    if data is None:
        raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    if format.lower() == "excel":
        return StreamingResponse(
            io.BytesIO(data),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f"attachment; filename=AIRISS_v4_results_{job_id}.xlsx"
            }
        )
    elif format.lower() == "csv":
        return StreamingResponse(
            io.BytesIO(data),
            media_type="text/csv; charset=utf-8",
            headers={
                "Content-Disposition": f"attachment; filename=AIRISS_v4_results_{job_id}.csv"
            }
        )
    elif format.lower() == "json":
        return StreamingResponse(
            io.BytesIO(data),
            media_type="application/json; charset=utf-8",
            headers={
                "Content-Disposition": f"attachment; filename=AIRISS_v4_results_{job_id}.json"
            }
        )
    else:
        raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")

@router.get("/pdf/{job_id}/{employee_id}")
async def download_employee_pdf(
    job_id: str,
    employee_id: str,
    service: AnalysisService = Depends(get_analysis_service)
):
    """ê°œì¸ë³„ ë¶„ì„ ê²°ê³¼ PDF ë‹¤ìš´ë¡œë“œ"""
    logger = logging.getLogger(__name__)
    logger.info(f"PDF ë‹¤ìš´ë¡œë“œ ìš”ì²­: job_id={job_id}, employee_id='{employee_id}', type={type(employee_id)}")
    logger.info(f"Employee ID ë¬¸ìì—´ ê¸¸ì´: {len(employee_id)}")
    logger.info(f"Employee IDì— 'dtype' í¬í•¨ ì—¬ë¶€: {'dtype' in employee_id}")
    
    # pandas Series í˜•íƒœ ê°ì§€
    if 'dtype: object' in str(employee_id):
        logger.error(f"âš ï¸ Pandas Series ê°ì²´ ê°ì§€! ì›ë³¸: {employee_id}")
        # Seriesì—ì„œ ì‹¤ì œ ê°’ ì¶”ì¶œ ì‹œë„
        try:
            import re
            # EMP001ê³¼ ê°™ì€ íŒ¨í„´ ì¶”ì¶œ
            match = re.search(r'EMP\d+', str(employee_id))
            if match:
                extracted_id = match.group()
                logger.info(f"âœ… Seriesì—ì„œ ID ì¶”ì¶œ ì„±ê³µ: {extracted_id}")
                employee_id = extracted_id
            else:
                logger.error("âŒ Seriesì—ì„œ ID ì¶”ì¶œ ì‹¤íŒ¨")
                raise HTTPException(status_code=400, detail="ì˜ëª»ëœ employee_id í˜•ì‹ì…ë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"âŒ Series ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=400, detail="employee_id ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
    
    try:
        # ì‘ì—… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        db = service._get_db()
        try:
            from app.models.job import Job
            job = db.query(Job).filter(Job.id == job_id).first()
            
            if not job:
                raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
            if job.status != 'completed':
                raise HTTPException(status_code=400, detail="ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
            # ë¶„ì„ ê²°ê³¼ì—ì„œ í•´ë‹¹ ì§ì› ë°ì´í„° ì°¾ê¸°
            if job.results_data:
                results = json.loads(job.results_data)
                analysis_results = results.get('analysis_results', results.get('data', []))
                
                # ì§ì› ë°ì´í„° ì°¾ê¸°
                logger.info(f"ğŸ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ê°œìˆ˜: {len(analysis_results)}")
                logger.info(f"ğŸ” ì²« ë²ˆì§¸ ê²°ê³¼ ìƒ˜í”Œ: {analysis_results[0] if analysis_results else 'None'}")
                
                # pandas Seriesì—ì„œ ì‹¤ì œ ê°’ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
                def extract_from_pandas_series(value):
                    if not isinstance(value, str):
                        return str(value)
                    
                    # pandas Series í˜•íƒœ ê°ì§€
                    if 'dtype: object' in value:
                        import re
                        # EMP001ê³¼ ê°™ì€ íŒ¨í„´ ì¶”ì¶œ
                        emp_id_match = re.search(r'EMP\d+', value)
                        if emp_id_match:
                            return emp_id_match.group()
                    
                    return str(value)
                
                employee_data = None
                for i, result in enumerate(analysis_results):
                    # ì›ë³¸ ê°’
                    raw_emp_id = result.get('employee_id', '')
                    raw_uid = result.get('uid', '')
                    
                    # pandas Seriesì—ì„œ ì‹¤ì œ ê°’ ì¶”ì¶œ
                    result_emp_id = extract_from_pandas_series(raw_emp_id)
                    result_uid = extract_from_pandas_series(raw_uid)
                    
                    logger.info(f"ğŸ” ê²°ê³¼ [{i}]: ì›ë³¸_emp_id='{raw_emp_id[:50]}...', ì¶”ì¶œëœ_emp_id='{result_emp_id}', ì¶”ì¶œëœ_uid='{result_uid}', ë¹„êµëŒ€ìƒ='{employee_id}'")
                    logger.info(f"ğŸ” ë§¤ì¹˜ ì—¬ë¶€: emp_id={result_emp_id == employee_id}, uid={result_uid == employee_id}")
                    
                    if result_emp_id == employee_id or result_uid == employee_id:
                        logger.info(f"âœ… ë§¤ì¹˜ ë°œê²¬! ì¸ë±ìŠ¤: {i}")
                        employee_data = result
                        break
                        
                if not employee_data:
                    raise HTTPException(status_code=404, detail=f"ì§ì› ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {employee_id}")
                    
                # PDF ìƒì„±
                from app.services.pdf_service import PDFReportGenerator
                pdf_generator = PDFReportGenerator()
                pdf_bytes = pdf_generator.generate_employee_report(employee_data, job_id)
                
                # íŒŒì¼ëª… ìƒì„± (URL ì¸ì½”ë”©ìœ¼ë¡œ í•œê¸€ ì²˜ë¦¬)
                from urllib.parse import quote
                employee_name = employee_data.get('name', 'unknown')
                safe_filename = f"AIRISS_v4_{employee_id}_{employee_name}_ë¦¬í¬íŠ¸.pdf"
                encoded_filename = quote(safe_filename, safe='')
                
                return StreamingResponse(
                    io.BytesIO(pdf_bytes),
                    media_type="application/pdf",
                    headers={
                        "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
                    }
                )
            else:
                raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                
        finally:
            db.close()
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"PDF ìƒì„± ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"PDF ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")