# app/api/analysis_storage.py
# ë¶„ì„ ê²°ê³¼ ì €ì¥ ë° ì¡°íšŒ API ì—”ë“œí¬ì¸íŠ¸ - Excel ë‚´ë³´ë‚´ê¸° ì™„ì „ ê°œì„ 

from fastapi import APIRouter, HTTPException, Query, Depends
from fastapi.responses import StreamingResponse
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
import pandas as pd
from io import BytesIO
import json

from app.services.analysis_storage_service import storage_service

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/api/analysis-storage",
    tags=["analysis-storage"]
)

def create_excel_file(results: List[Dict[str, Any]], filename: str = "AIRISS_Report") -> BytesIO:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ì‹¤ì œ Excel íŒŒì¼ë¡œ ìƒì„±
    
    Args:
        results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        filename: íŒŒì¼ëª…
        
    Returns:
        BytesIO: Excel íŒŒì¼ ë°”ì´ë„ˆë¦¬ ìŠ¤íŠ¸ë¦¼
    """
    try:
        # BytesIO ìŠ¤íŠ¸ë¦¼ ìƒì„±
        excel_buffer = BytesIO()
        
        # Excel Writer ìƒì„± (openpyxl ì—”ì§„ ì‚¬ìš©)
        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
            
            # 1. ë©”ì¸ ë¶„ì„ ê²°ê³¼ ì‹œíŠ¸
            if results:
                # ë¶„ì„ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_main = pd.DataFrame(results)
                
                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ ë° ì •ë¦¬
                display_columns = []
                if 'created_at' in df_main.columns:
                    display_columns.append('created_at')
                if 'file_name' in df_main.columns:
                    display_columns.append('file_name')
                if 'uid' in df_main.columns:
                    display_columns.append('uid')
                if 'opinion' in df_main.columns:
                    display_columns.append('opinion')
                if 'hybrid_score' in df_main.columns:
                    display_columns.append('hybrid_score')
                if 'bias_score' in df_main.columns:
                    display_columns.append('bias_score')
                if 'creativity_score' in df_main.columns:
                    display_columns.append('creativity_score')
                if 'problem_solving_score' in df_main.columns:
                    display_columns.append('problem_solving_score')
                
                # ì„ íƒëœ ì»¬ëŸ¼ìœ¼ë¡œ DataFrame ìƒì„±
                if display_columns:
                    df_display = df_main[display_columns].copy()
                else:
                    df_display = df_main.copy()
                
                # ì»¬ëŸ¼ëª… í•œê¸€í™”
                column_mapping = {
                    'created_at': 'ë¶„ì„ ì¼ì‹œ',
                    'file_name': 'íŒŒì¼ëª…',
                    'uid': 'ì‚¬ìš©ì ID',
                    'opinion': 'ì˜ê²¬/í”¼ë“œë°±',
                    'hybrid_score': 'ì¢…í•© ì ìˆ˜',
                    'bias_score': 'í¸í–¥ ì ìˆ˜',
                    'creativity_score': 'ì°½ì˜ì„± ì ìˆ˜',
                    'problem_solving_score': 'ë¬¸ì œí•´ê²° ì ìˆ˜'
                }
                
                df_display.rename(columns=column_mapping, inplace=True)
                
                # ë©”ì¸ ê²°ê³¼ ì‹œíŠ¸ì— ì €ì¥
                df_display.to_excel(writer, sheet_name='ë¶„ì„ ê²°ê³¼', index=False)
                
                # ì›Œí¬ì‹œíŠ¸ ìŠ¤íƒ€ì¼ë§
                worksheet = writer.sheets['ë¶„ì„ ê²°ê³¼']
                
                # í—¤ë” ìŠ¤íƒ€ì¼ë§
                for cell in worksheet[1]:
                    cell.font = cell.font.copy(bold=True)
                    cell.fill = cell.fill.copy(fgColor="366092")
                
                # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì •
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
                
                # 2. í†µê³„ ìš”ì•½ ì‹œíŠ¸
                summary_data = {
                    'í•­ëª©': ['ì´ ë¶„ì„ ê±´ìˆ˜', 'í‰ê·  ì¢…í•© ì ìˆ˜', 'ìµœê³  ì ìˆ˜', 'ìµœì € ì ìˆ˜', 'ë¶„ì„ ê¸°ê°„'],
                    'ê°’': [
                        len(results),
                        round(df_main['hybrid_score'].mean() if 'hybrid_score' in df_main.columns else 0, 2),
                        round(df_main['hybrid_score'].max() if 'hybrid_score' in df_main.columns else 0, 2),
                        round(df_main['hybrid_score'].min() if 'hybrid_score' in df_main.columns else 0, 2),
                        f"{df_main['created_at'].min()} ~ {df_main['created_at'].max()}" if 'created_at' in df_main.columns else "N/A"
                    ]
                }
                
                df_summary = pd.DataFrame(summary_data)
                df_summary.to_excel(writer, sheet_name='í†µê³„ ìš”ì•½', index=False)
                
                # 3. ì ìˆ˜ ë¶„í¬ ì‹œíŠ¸ (ìˆëŠ” ê²½ìš°)
                if 'hybrid_score' in df_main.columns:
                    score_distribution = df_main['hybrid_score'].value_counts().sort_index()
                    df_scores = pd.DataFrame({
                        'ì ìˆ˜': score_distribution.index,
                        'ë¹ˆë„': score_distribution.values
                    })
                    df_scores.to_excel(writer, sheet_name='ì ìˆ˜ ë¶„í¬', index=False)
            
            else:
                # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ì‹œíŠ¸ ìƒì„±
                empty_df = pd.DataFrame({'ë©”ì‹œì§€': ['ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.']})
                empty_df.to_excel(writer, sheet_name='ë¶„ì„ ê²°ê³¼', index=False)
        
        # ë²„í¼ í¬ì¸í„°ë¥¼ ì‹œì‘ìœ¼ë¡œ ì´ë™
        excel_buffer.seek(0)
        
        logger.info(f"âœ… Excel íŒŒì¼ ìƒì„± ì™„ë£Œ: {filename}.xlsx")
        return excel_buffer
        
    except Exception as e:
        logger.error(f"âŒ Excel íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Excel íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")

def create_csv_file(results: List[Dict[str, Any]]) -> str:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ BOM í¬í•¨ CSVë¡œ ìƒì„± (Excel í•œê¸€ í˜¸í™˜)
    
    Args:
        results: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        str: BOM í¬í•¨ CSV ë¬¸ìì—´
    """
    try:
        if not results:
            return '\ufeffë©”ì‹œì§€\në¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n'
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(results)
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        display_columns = []
        column_mapping = {
            'created_at': 'ë¶„ì„ ì¼ì‹œ',
            'file_name': 'íŒŒì¼ëª…',
            'uid': 'ì‚¬ìš©ì ID',
            'opinion': 'ì˜ê²¬/í”¼ë“œë°±',
            'hybrid_score': 'ì¢…í•© ì ìˆ˜',
            'bias_score': 'í¸í–¥ ì ìˆ˜',
            'creativity_score': 'ì°½ì˜ì„± ì ìˆ˜',
            'problem_solving_score': 'ë¬¸ì œí•´ê²° ì ìˆ˜'
        }
        
        for col in ['created_at', 'file_name', 'uid', 'opinion', 'hybrid_score', 'bias_score', 'creativity_score', 'problem_solving_score']:
            if col in df.columns:
                display_columns.append(col)
        
        if display_columns:
            df_display = df[display_columns].copy()
            df_display.rename(columns=column_mapping, inplace=True)
        else:
            df_display = df.copy()
        
        # BOM í¬í•¨ CSV ìƒì„± (Excel í•œê¸€ í˜¸í™˜)
        csv_data = '\ufeff' + df_display.to_csv(index=False, encoding='utf-8')
        
        logger.info("âœ… CSV íŒŒì¼ ìƒì„± ì™„ë£Œ (BOM í¬í•¨)")
        return csv_data
        
    except Exception as e:
        logger.error(f"âŒ CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.post("/save")
async def save_analysis_result(analysis_data: Dict[str, Any]):
    """
    ë¶„ì„ ê²°ê³¼ ì €ì¥
    
    Args:
        analysis_data: ë¶„ì„ ê²°ê³¼ ë°ì´í„°
        
    Returns:
        ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ID
    """
    try:
        analysis_id = storage_service.save_analysis_result(analysis_data)
        return {
            "success": True,
            "analysis_id": analysis_id,
            "message": "ë¶„ì„ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/results")
async def get_analysis_results(
    file_id: Optional[str] = Query(None, description="íŒŒì¼ IDë¡œ í•„í„°ë§"),
    uid: Optional[str] = Query(None, description="ì‚¬ìš©ì IDë¡œ í•„í„°ë§"),
    limit: int = Query(100, description="ê²°ê³¼ ìˆ˜ ì œí•œ"),
    offset: int = Query(0, description="í˜ì´ì§€ë„¤ì´ì…˜ ì˜¤í”„ì…‹")
):
    """
    ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
    
    Args:
        file_id: íŒŒì¼ ID í•„í„°
        uid: ì‚¬ìš©ì ID í•„í„°
        limit: ê²°ê³¼ ìˆ˜ ì œí•œ
        offset: í˜ì´ì§€ë„¤ì´ì…˜ ì˜¤í”„ì…‹
        
    Returns:
        ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        results = storage_service.get_analysis_results(
            file_id=file_id,
            uid=uid,
            limit=limit,
            offset=offset
        )
        
        return {
            "success": True,
            "results": results,
            "count": len(results),
            "pagination": {
                "limit": limit,
                "offset": offset
            }
        }
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/results/{analysis_id}")
async def get_analysis_by_id(analysis_id: str):
    """íŠ¹ì • ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        result = storage_service.get_analysis_by_id(analysis_id)
        
        if not result:
            raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return {
            "success": True,
            "result": result
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/jobs")
async def get_analysis_jobs(
    status: Optional[str] = Query(None, description="ì‘ì—… ìƒíƒœë¡œ í•„í„°ë§"),
    limit: int = Query(50, description="ê²°ê³¼ ìˆ˜ ì œí•œ")
):
    """ë¶„ì„ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    try:
        jobs = storage_service.get_analysis_jobs(status=status, limit=limit)
        
        return {
            "success": True,
            "jobs": jobs,
            "count": len(jobs)
        }
    except Exception as e:
        logger.error(f"ë¶„ì„ ì‘ì—… ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/statistics")
async def get_analysis_statistics(
    days: int = Query(30, description="í†µê³„ ê¸°ê°„ (ì¼)")
):
    """ë¶„ì„ í†µê³„ ì¡°íšŒ"""
    try:
        stats = storage_service.get_analysis_statistics(days=days)
        
        return {
            "success": True,
            "statistics": stats
        }
    except Exception as e:
        logger.error(f"ë¶„ì„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/search")
async def search_analysis_results(
    search_term: str = Query(..., description="ê²€ìƒ‰ì–´"),
    search_type: str = Query("opinion", description="ê²€ìƒ‰ íƒ€ì… (opinion, uid, filename)"),
    limit: int = Query(50, description="ê²°ê³¼ ìˆ˜ ì œí•œ")
):
    """ë¶„ì„ ê²°ê³¼ ê²€ìƒ‰"""
    try:
        results = storage_service.search_analysis_results(
            search_term=search_term,
            search_type=search_type,
            limit=limit
        )
        
        return {
            "success": True,
            "results": results,
            "count": len(results),
            "search_info": {
                "term": search_term,
                "type": search_type
            }
        }
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/score-distribution")
async def get_score_distribution(
    score_type: str = Query("hybrid_score", description="ì ìˆ˜ íƒ€ì…")
):
    """ì ìˆ˜ ë¶„í¬ ì¡°íšŒ"""
    try:
        distribution = storage_service.get_score_distribution(score_type=score_type)
        
        return {
            "success": True,
            "distribution": distribution
        }
    except Exception as e:
        logger.error(f"ì ìˆ˜ ë¶„í¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/cleanup")
async def cleanup_old_results(
    retention_days: int = Query(365, description="ë³´ê´€ ê¸°ê°„ (ì¼)")
):
    """ì˜¤ë˜ëœ ë¶„ì„ ê²°ê³¼ ì •ë¦¬"""
    try:
        storage_service.cleanup_old_results(retention_days=retention_days)
        
        return {
            "success": True,
            "message": f"{retention_days}ì¼ ì´ì „ ë¶„ì„ ê²°ê³¼ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== ğŸš¨ ìƒˆë¡œìš´ Excel/CSV ë‚´ë³´ë‚´ê¸° ì—”ë“œí¬ì¸íŠ¸ ====================

@router.get("/export-excel")
async def export_analysis_results_excel(
    file_id: Optional[str] = Query(None, description="íŒŒì¼ IDë¡œ í•„í„°ë§"),
    uid: Optional[str] = Query(None, description="ì‚¬ìš©ì IDë¡œ í•„í„°ë§"),
    start_date: Optional[str] = Query(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)"),
    filename: str = Query("AIRISS_Report", description="ë‹¤ìš´ë¡œë“œ íŒŒì¼ëª…")
):
    """
    ğŸ†• ì‹¤ì œ Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ë¸Œë¼ìš°ì €ì—ì„œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ)
    
    ì‚¬ìš©ë²•: ë¸Œë¼ìš°ì €ì—ì„œ URL ì§ì ‘ ì ‘ì†í•˜ë©´ Excel íŒŒì¼ ìë™ ë‹¤ìš´ë¡œë“œ
    ì˜ˆì‹œ: https://airiss.railway.app/api/analysis-storage/export-excel
    """
    try:
        logger.info("ğŸ“Š Excel íŒŒì¼ ë‚´ë³´ë‚´ê¸° ìš”ì²­ ì‹œì‘")
        
        # ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
        results = storage_service.get_analysis_results(
            file_id=file_id,
            uid=uid,
            limit=10000  # ëŒ€ëŸ‰ ë‚´ë³´ë‚´ê¸°
        )
        
        # ë‚ ì§œ í•„í„°ë§ (í•„ìš”í•œ ê²½ìš°)
        if start_date or end_date:
            # ë‚ ì§œ í•„í„°ë§ ë¡œì§ êµ¬í˜„ (ì¶”í›„ í™•ì¥ ê°€ëŠ¥)
            pass
        
        # ì‹¤ì œ Excel íŒŒì¼ ìƒì„±
        excel_file = create_excel_file(results, filename)
        
        # íŒŒì¼ëª… ì„¤ì • (í˜„ì¬ ë‚ ì§œ í¬í•¨)
        current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
        download_filename = f"{filename}_{current_date}.xlsx"
        
        logger.info(f"âœ… Excel íŒŒì¼ ìƒì„± ì™„ë£Œ: {download_filename} ({len(results)}ê±´)")
        
        # ì‹¤ì œ Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‘ë‹µ
        return StreamingResponse(
            io=excel_file,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename={download_filename}"}
        )
        
    except Exception as e:
        logger.error(f"âŒ Excel íŒŒì¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Excel íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.get("/export-csv")
async def export_analysis_results_csv(
    file_id: Optional[str] = Query(None, description="íŒŒì¼ IDë¡œ í•„í„°ë§"),
    uid: Optional[str] = Query(None, description="ì‚¬ìš©ì IDë¡œ í•„í„°ë§"),
    start_date: Optional[str] = Query(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)"),
    filename: str = Query("AIRISS_Report", description="ë‹¤ìš´ë¡œë“œ íŒŒì¼ëª…")
):
    """
    ğŸ†• Excel í˜¸í™˜ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ (BOM í¬í•¨, í•œê¸€ ê¹¨ì§ ë°©ì§€)
    
    ì‚¬ìš©ë²•: ë¸Œë¼ìš°ì €ì—ì„œ URL ì§ì ‘ ì ‘ì†í•˜ë©´ CSV íŒŒì¼ ìë™ ë‹¤ìš´ë¡œë“œ
    ì˜ˆì‹œ: https://airiss.railway.app/api/analysis-storage/export-csv
    """
    try:
        logger.info("ğŸ“Š CSV íŒŒì¼ ë‚´ë³´ë‚´ê¸° ìš”ì²­ ì‹œì‘")
        
        # ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
        results = storage_service.get_analysis_results(
            file_id=file_id,
            uid=uid,
            limit=10000  # ëŒ€ëŸ‰ ë‚´ë³´ë‚´ê¸°
        )
        
        # ë‚ ì§œ í•„í„°ë§ (í•„ìš”í•œ ê²½ìš°)
        if start_date or end_date:
            # ë‚ ì§œ í•„í„°ë§ ë¡œì§ êµ¬í˜„ (ì¶”í›„ í™•ì¥ ê°€ëŠ¥)
            pass
        
        # BOM í¬í•¨ CSV ìƒì„±
        csv_data = create_csv_file(results)
        
        # íŒŒì¼ëª… ì„¤ì • (í˜„ì¬ ë‚ ì§œ í¬í•¨)
        current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
        download_filename = f"{filename}_{current_date}.csv"
        
        logger.info(f"âœ… CSV íŒŒì¼ ìƒì„± ì™„ë£Œ: {download_filename} ({len(results)}ê±´)")
        
        # CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‘ë‹µ
        return StreamingResponse(
            io=BytesIO(csv_data.encode('utf-8-sig')),
            media_type="text/csv; charset=utf-8",
            headers={"Content-Disposition": f"attachment; filename={download_filename}"}
        )
        
    except Exception as e:
        logger.error(f"âŒ CSV íŒŒì¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")

# ==================== ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€) ====================

@router.get("/export")
async def export_analysis_results(
    file_id: Optional[str] = Query(None, description="íŒŒì¼ IDë¡œ í•„í„°ë§"),
    start_date: Optional[str] = Query(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)"),
    format: str = Query("json", description="ë‚´ë³´ë‚´ê¸° í˜•ì‹ (json, csv)")
):
    """
    âš ï¸ ê¸°ì¡´ ë‚´ë³´ë‚´ê¸° (í˜¸í™˜ì„± ìœ ì§€ìš©) - í…ìŠ¤íŠ¸ ë°˜í™˜
    
    âœ… ìƒˆë¡œìš´ ê¸°ëŠ¥:
    - /export-excel : ì‹¤ì œ Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    - /export-csv : Excel í˜¸í™˜ CSV ë‹¤ìš´ë¡œë“œ
    """
    try:
        # ê¸°ë³¸ ì¡°íšŒ
        results = storage_service.get_analysis_results(
            file_id=file_id,
            limit=10000  # ëŒ€ëŸ‰ ë‚´ë³´ë‚´ê¸°ë¥¼ ìœ„í•´ ë†’ì€ ì œí•œ
        )
        
        # ë‚ ì§œ í•„í„°ë§ (í•„ìš”í•œ ê²½ìš°)
        if start_date or end_date:
            # ë‚ ì§œ í•„í„°ë§ ë¡œì§ êµ¬í˜„
            pass
        
        if format == "csv":
            # CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            import pandas as pd
            df = pd.DataFrame(results)
            csv_data = df.to_csv(index=False)
            
            return {
                "success": True,
                "format": "csv",
                "data": csv_data,
                "count": len(results),
                "notice": "ğŸ’¡ ì‹¤ì œ Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œëŠ” /export-excel ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!"
            }
        else:
            # JSON í˜•ì‹
            return {
                "success": True,
                "format": "json",
                "data": results,
                "count": len(results),
                "notice": "ğŸ’¡ ì‹¤ì œ Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œëŠ” /export-excel ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!"
            }
            
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
