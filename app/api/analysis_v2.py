"""
AIRISS v4 ë¶„ì„ API v2
í†µí•© ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸
ê¸°ì¡´ APIì™€ì˜ í˜¸í™˜ì„± ìœ ì§€
"""

from fastapi import APIRouter, HTTPException
from app.db.db_service_v2 import db_service_v2
import logging

logger = logging.getLogger(__name__)

# ìƒˆë¡œìš´ ë¼ìš°í„° (ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ìš©)
router_v2 = APIRouter(prefix="/analysis/v2", tags=["analysis-v2"])


@router_v2.get("/results/{job_id}")
async def get_analysis_results_v2(job_id: str):
    """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ - í†µí•© ìŠ¤í‚¤ë§ˆ ì‚¬ìš©"""
    try:
        logger.info(f"ğŸ“Š V2 ê²°ê³¼ ì¡°íšŒ: {job_id}")
        
        # í†µí•© í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
        results = await db_service_v2.get_analysis_results(job_id=job_id)
        
        if not results:
            # ì‘ì—…ì´ ì•„ì§ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸
            from app.db import db_service
            db = db_service.get_session()
            try:
                from sqlalchemy import text
                job_data = db.execute(
                    text("SELECT status FROM jobs WHERE id = :job_id"),
                    {'job_id': job_id}
                ).fetchone()
                
                if job_data and job_data.status == "processing":
                    return {
                        "results": [],
                        "total_count": 0,
                        "job_status": "processing",
                        "message": "ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    }
            finally:
                db.close()
            
            raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ì‘ë‹µ êµ¬ì„±
        response = {
            "results": results,
            "total_count": len(results),
            "job_status": "completed",
            "analysis_mode": results[0].get('analysis_mode', 'hybrid') if results else 'hybrid',
            "version": "4.0-v2",
            "schema_version": "unified"
        }
        
        logger.info(f"âœ… V2 ê²°ê³¼ ì¡°íšŒ ì„±ê³µ: {len(results)}ê°œ")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ V2 ê²°ê³¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@router_v2.get("/download/{job_id}/{format}")
async def download_results_v2(job_id: str, format: str = "excel"):
    """ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ - í†µí•© ìŠ¤í‚¤ë§ˆ ì‚¬ìš©"""
    try:
        logger.info(f"ğŸ“¥ V2 ë‹¤ìš´ë¡œë“œ ìš”ì²­: {job_id} - í˜•ì‹: {format}")
        
        # ë‹¤ìš´ë¡œë“œìš© ë°ì´í„° ì¡°íšŒ
        results = await db_service_v2.get_results_for_download(job_id)
        
        if not results:
            raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        import pandas as pd
        df = pd.DataFrame(results)
        
        # íŒŒì¼ ìƒì„±
        import io
        from datetime import datetime
        from fastapi.responses import StreamingResponse
        
        if format.lower() == "excel":
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                # ìš”ì•½ ì‹œíŠ¸
                summary_data = {
                    'í•­ëª©': ['ë¶„ì„ì¼ì‹œ', 'ì´ ë¶„ì„ê±´ìˆ˜', 'í‰ê·  ì ìˆ˜', 'AI ë¶„ì„ í¬í•¨'],
                    'ê°’': [
                        datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        len(results),
                        round(df['AIRISS_v4_ì¢…í•©ì ìˆ˜'].mean(), 1) if 'AIRISS_v4_ì¢…í•©ì ìˆ˜' in df.columns else 'N/A',
                        'ì˜ˆ' if any(df.get('AI_ì¢…í•©í”¼ë“œë°±', '').notna()) else 'ì•„ë‹ˆì˜¤'
                    ]
                }
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='ìš”ì•½', index=False)
                
                # ìƒì„¸ ê²°ê³¼
                df.to_excel(writer, sheet_name='ìƒì„¸ê²°ê³¼', index=False)
                
                # ìŠ¤íƒ€ì¼ ì ìš©
                workbook = writer.book
                for sheet_name in workbook.sheetnames:
                    worksheet = workbook[sheet_name]
                    # í—¤ë” ìŠ¤íƒ€ì¼
                    for cell in worksheet[1]:
                        cell.font = writer.book.create_font(bold=True)
            
            output.seek(0)
            
            return StreamingResponse(
                io.BytesIO(output.getvalue()),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={
                    "Content-Disposition": f"attachment; filename=AIRISS_v2_results_{job_id[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                }
            )
            
        elif format.lower() == "csv":
            output = io.StringIO()
            df.to_csv(output, index=False, encoding='utf-8-sig')
            
            return StreamingResponse(
                io.BytesIO(output.getvalue().encode('utf-8-sig')),
                media_type="text/csv",
                headers={
                    "Content-Disposition": f"attachment; filename=AIRISS_v2_results_{job_id[:8]}.csv"
                }
            )
            
        else:
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ V2 ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


# ê¸°ì¡´ API íŒ¨ì¹˜ í•¨ìˆ˜ (ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜)
def patch_existing_analysis_api():
    """ê¸°ì¡´ analysis.pyì˜ í•¨ìˆ˜ë“¤ì„ ìƒˆ ì„œë¹„ìŠ¤ë¡œ êµì²´"""
    try:
        import app.api.analysis as analysis_module
        
        # get_analysis_results í•¨ìˆ˜ êµì²´
        async def patched_get_results(job_id: str):
            return await get_analysis_results_v2(job_id)
        
        # íŒ¨ì¹˜ ì ìš©
        analysis_module.get_analysis_results = patched_get_results
        
        logger.info("âœ… ê¸°ì¡´ API íŒ¨ì¹˜ ì™„ë£Œ - í†µí•© ìŠ¤í‚¤ë§ˆ ì‚¬ìš©")
        
    except Exception as e:
        logger.error(f"âŒ API íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")


# ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ í™•ì¸
@router_v2.get("/migration/status")
async def check_migration_status():
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ í™•ì¸"""
    try:
        db = db_service_v2.get_session()
        from sqlalchemy import text
        
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        tables = {}
        for table in ['results', 'analysis_results', 'analysis_results_v2']:
            try:
                count = db.execute(text(f"SELECT COUNT(*) FROM {table}")).scalar()
                tables[table] = {"exists": True, "count": count}
            except:
                tables[table] = {"exists": False, "count": 0}
        
        # ë·° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        views = {}
        for view in ['results_view', 'analysis_results_view']:
            try:
                count = db.execute(text(f"SELECT COUNT(*) FROM {view}")).scalar()
                views[view] = {"exists": True, "count": count}
            except:
                views[view] = {"exists": False, "count": 0}
        
        db.close()
        
        return {
            "migration_ready": tables.get('analysis_results_v2', {}).get('exists', False),
            "tables": tables,
            "views": views,
            "recommendation": "ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤€ë¹„ ì™„ë£Œ" if tables.get('analysis_results_v2', {}).get('exists', False) else "ë§ˆì´ê·¸ë ˆì´ì…˜ í•„ìš”"
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "migration_ready": False
        }